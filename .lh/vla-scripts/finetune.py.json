{
    "sourceFile": "vla-scripts/finetune.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 30,
            "patches": [
                {
                    "date": 1765092846921,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1765095308987,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -773,9 +773,9 @@\n     # Load processor and VLA\n     AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n     processor = AutoProcessor.from_pretrained(cfg.config_file_path, trust_remote_code=True)\n \n-    if cfg.use_minivlm:\n+    if cfg.use_minivlm:#使用旧的vla权重加载进来，改成新的名字，并迁移\n         hf_token = ''\n         if 'prism-qwen25-extra-dinosiglip-224px-0_5b' in cfg.vlm_path:\n             \n             vlm = load(cfg.vlm_path, hf_token=hf_token, load_for_training=True)\n"
                },
                {
                    "date": 1765095687844,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -824,9 +824,9 @@\n             trust_remote_code=False,\n             ).to(device_id)\n \n     # Set number of images in VLA input\n-    vla.vision_backbone.set_num_images_in_input(cfg.num_images_in_input)\n+    vla.vision_backbone.set_num_images_in_input(cfg.num_images_in_input)#1\n \n     # vla.set_version(cfg.version)\n \n     if cfg.use_lora:\n"
                },
                {
                    "date": 1765096550532,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -879,9 +879,9 @@\n             to_bf16=True,\n         )\n \n     # If applicable, instantiate continuous action head for L1 regression\n-    if cfg.use_l1_regression:\n+    if cfg.use_l1_regression: #构建L1动作头\n         action_head = init_module(\n         L1RegressionActionHead,\n         \"action_head\",\n         cfg,\n"
                },
                {
                    "date": 1765096821980,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -879,9 +879,9 @@\n             to_bf16=True,\n         )\n \n     # If applicable, instantiate continuous action head for L1 regression\n-    if cfg.use_l1_regression: #构建L1动作头\n+    if cfg.use_l1_regression: #构建\n         action_head = init_module(\n         L1RegressionActionHead,\n         \"action_head\",\n         cfg,\n"
                },
                {
                    "date": 1765097619934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,9 +66,9 @@\n @dataclass\n class FinetuneConfig:\n     # fmt: off\n     config_file_path: str = \"openvla/openvla-7b\"     # Path to necessary config files of LA-Adapter\n-    vlm_path: str = \"openvla/openvla-7b\"             # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n+    vlm_path: str = \"pretrained_models/prism-qwen25-extra-dinosiglip-224px-0_5b\"             # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n     use_minivlm: bool = False                        # \n     resum_vla_path: str = \"openvla/openvla-7b\"       # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n \n     # Dataset\n"
                },
                {
                    "date": 1765097629599,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,9 +65,9 @@\n \n @dataclass\n class FinetuneConfig:\n     # fmt: off\n-    config_file_path: str = \"openvla/openvla-7b\"     # Path to necessary config files of LA-Adapter\n+    config_file_path: str = \"pretrained_models/configs\"     # Path to necessary config files of LA-Adapter\n     vlm_path: str = \"pretrained_models/prism-qwen25-extra-dinosiglip-224px-0_5b\"             # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n     use_minivlm: bool = False                        # \n     resum_vla_path: str = \"openvla/openvla-7b\"       # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n \n"
                },
                {
                    "date": 1765097663932,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,9 +71,10 @@\n     use_minivlm: bool = False                        # \n     resum_vla_path: str = \"openvla/openvla-7b\"       # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n \n     # Dataset\n-    data_root_dir: Path = Path(\"datasets/rlds\")      # Directory containing RLDS datasets\n+    # data_root_dir: Path = Path(\"datasets/rlds\")      # Directory containing RLDS datasets\n+    data_root_dir: Path(\"data/libero\")\n     dataset_name: str = \"aloha_scoop_x_into_bowl\"    # Name of fine-tuning dataset (e.g., `aloha_scoop_x_into_bowl`)\n     run_root_dir: Path = Path(\"runs\")                # Path to directory to store logs & checkpoints\n     shuffle_buffer_size: int = 100_000               # Dataloader shuffle buffer size (can reduce if OOM errors occur)\n \n"
                },
                {
                    "date": 1765097671419,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,9 +72,9 @@\n     resum_vla_path: str = \"openvla/openvla-7b\"       # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n \n     # Dataset\n     # data_root_dir: Path = Path(\"datasets/rlds\")      # Directory containing RLDS datasets\n-    data_root_dir: Path(\"data/libero\")\n+    data_root_dir: Path = PPath(\"data/libero\")\n     dataset_name: str = \"aloha_scoop_x_into_bowl\"    # Name of fine-tuning dataset (e.g., `aloha_scoop_x_into_bowl`)\n     run_root_dir: Path = Path(\"runs\")                # Path to directory to store logs & checkpoints\n     shuffle_buffer_size: int = 100_000               # Dataloader shuffle buffer size (can reduce if OOM errors occur)\n \n"
                },
                {
                    "date": 1765097716226,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,11 +72,11 @@\n     resum_vla_path: str = \"openvla/openvla-7b\"       # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n \n     # Dataset\n     # data_root_dir: Path = Path(\"datasets/rlds\")      # Directory containing RLDS datasets\n-    data_root_dir: Path = PPath(\"data/libero\")\n+    data_root_dir: Path = Path(\"data/libero\")\n     dataset_name: str = \"aloha_scoop_x_into_bowl\"    # Name of fine-tuning dataset (e.g., `aloha_scoop_x_into_bowl`)\n-    run_root_dir: Path = Path(\"runs\")                # Path to directory to store logs & checkpoints\n+    run_root_dir: Path = Path(\"outputs\")                # Path to directory to store logs & checkpoints\n     shuffle_buffer_size: int = 100_000               # Dataloader shuffle buffer size (can reduce if OOM errors occur)\n \n     # Algorithm and architecture\n     use_l1_regression: bool = True                   # If True, trains continuous action head with L1 regression objective\n"
                },
                {
                    "date": 1765097728542,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -82,9 +82,9 @@\n     use_l1_regression: bool = True                   # If True, trains continuous action head with L1 regression objective\n     use_diffusion: bool = False                      # If True, trains continuous action head with diffusion modeling objective (DDIM)\n     num_diffusion_steps: int = 50                    # (When `diffusion==True`) Number of diffusion steps for training \n     use_film: bool = False                           # If True, uses FiLM to infuse language inputs into visual features\n-    num_images_in_input: int = 1                     # Number of images in the VLA input (default: 1)\n+    num_images_in_input: int = 2                     # Number of images in the VLA input (default: 1)\n     use_proprio: bool = False                        # If True, includes robot proprioceptive state in input\n     phase1_path: str = \"None\"\n \n     # Training configuration\n"
                },
                {
                    "date": 1765097740002,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -83,9 +83,9 @@\n     use_diffusion: bool = False                      # If True, trains continuous action head with diffusion modeling objective (DDIM)\n     num_diffusion_steps: int = 50                    # (When `diffusion==True`) Number of diffusion steps for training \n     use_film: bool = False                           # If True, uses FiLM to infuse language inputs into visual features\n     num_images_in_input: int = 2                     # Number of images in the VLA input (default: 1)\n-    use_proprio: bool = False                        # If True, includes robot proprioceptive state in input\n+    use_proprio: bool = True                        # If True, includes robot proprioceptive state in input\n     phase1_path: str = \"None\"\n \n     # Training configuration\n     batch_size: int = 8                              # Batch size per device (total batch size = batch_size * num GPUs)\n"
                },
                {
                    "date": 1765097750630,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -105,9 +105,9 @@\n     image_aug: bool = True                           # If True, trains with image augmentations (HIGHLY RECOMMENDED)\n     diffusion_sample_freq: int = 50                  # (When `use_diffusion==True`) Frequency for sampling in steps\n \n     # LoRA\n-    use_lora: bool = False                           # If True, uses LoRA fine-tuning\n+    use_lora: bool = True                           # If True, uses LoRA fine-tuning\n     lora_rank: int = 32                              # Rank of LoRA weight matrix\n     lora_dropout: float = 0.0                        # Dropout applied to LoRA weights\n     merge_lora_during_training: bool = False         # If True, merges LoRA weights and saves result during training\n                                                      #   Note: Merging can be very slow on some machines. If so, set to\n"
                },
                {
                    "date": 1765097782761,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -67,9 +67,9 @@\n class FinetuneConfig:\n     # fmt: off\n     config_file_path: str = \"pretrained_models/configs\"     # Path to necessary config files of LA-Adapter\n     vlm_path: str = \"pretrained_models/prism-qwen25-extra-dinosiglip-224px-0_5b\"             # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n-    use_minivlm: bool = False                        # \n+    use_minivlm: bool = True                        # \n     resum_vla_path: str = \"openvla/openvla-7b\"       # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n \n     # Dataset\n     # data_root_dir: Path = Path(\"datasets/rlds\")      # Directory containing RLDS datasets\n"
                },
                {
                    "date": 1765097813419,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -90,9 +90,9 @@\n     # Training configuration\n     batch_size: int = 8                              # Batch size per device (total batch size = batch_size * num GPUs)\n     learning_rate: float = 5e-4                      # Learning rate\n     lr_warmup_steps: int = 0.1                       # Number of steps to warm up learning rate (from 10% to 100%)\n-    num_steps_before_decay: int = 100000             # Number of steps before LR decays by 10x\n+    num_steps_before_decay: int = 200000             # Number of steps before LR decays by 10x\n     grad_accumulation_steps: int = 1                 # Number of gradient accumulation steps\n     max_steps: int = 200000                          # Max number of training steps\n     use_val_set: bool = False                        # If True, uses validation set and log validation metrics\n     val_freq: int = 10_000                           # (When `use_val_set==True`) Validation set logging frequency in steps\n"
                },
                {
                    "date": 1765097822381,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -92,13 +92,13 @@\n     learning_rate: float = 5e-4                      # Learning rate\n     lr_warmup_steps: int = 0.1                       # Number of steps to warm up learning rate (from 10% to 100%)\n     num_steps_before_decay: int = 200000             # Number of steps before LR decays by 10x\n     grad_accumulation_steps: int = 1                 # Number of gradient accumulation steps\n-    max_steps: int = 200000                          # Max number of training steps\n+    max_steps: int = 200005                          # Max number of training steps\n     use_val_set: bool = False                        # If True, uses validation set and log validation metrics\n     val_freq: int = 10_000                           # (When `use_val_set==True`) Validation set logging frequency in steps\n     val_time_limit: int = 180                        # (When `use_val_set==True`) Time limit for computing validation metrics\n-    save_freq: int = 10_000                          # Checkpoint saving frequency in steps\n+    save_freq: int = 5000                          # Checkpoint saving frequency in steps\n     save_latest_checkpoint_only: bool = False        # If True, saves only 1 checkpoint, overwriting latest checkpoint\n                                                      #   (If False, saves all checkpoints)\n     resume: bool = False                             # If True, resumes from checkpoint\n     resume_step: Optional[int] = None                # (When `resume==True`) Step number that we are resuming from\n"
                },
                {
                    "date": 1765097841812,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,9 +108,9 @@\n     # LoRA\n     use_lora: bool = True                           # If True, uses LoRA fine-tuning\n     lora_rank: int = 32                              # Rank of LoRA weight matrix\n     lora_dropout: float = 0.0                        # Dropout applied to LoRA weights\n-    merge_lora_during_training: bool = False         # If True, merges LoRA weights and saves result during training\n+    merge_lora_during_training: bool = True         # If True, merges LoRA weights and saves result during training\n                                                      #   Note: Merging can be very slow on some machines. If so, set to\n                                                      #         False and merge final checkpoint offline!\n \n     # Full Finetune\n"
                },
                {
                    "date": 1765097857754,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -87,9 +87,9 @@\n     use_proprio: bool = True                        # If True, includes robot proprioceptive state in input\n     phase1_path: str = \"None\"\n \n     # Training configuration\n-    batch_size: int = 8                              # Batch size per device (total batch size = batch_size * num GPUs)\n+    batch_size: int = 4                              # Batch size per device (total batch size = batch_size * num GPUs)\n     learning_rate: float = 5e-4                      # Learning rate\n     lr_warmup_steps: int = 0.1                       # Number of steps to warm up learning rate (from 10% to 100%)\n     num_steps_before_decay: int = 200000             # Number of steps before LR decays by 10x\n     grad_accumulation_steps: int = 1                 # Number of gradient accumulation steps\n"
                },
                {
                    "date": 1765097871236,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -106,9 +106,9 @@\n     diffusion_sample_freq: int = 50                  # (When `use_diffusion==True`) Frequency for sampling in steps\n \n     # LoRA\n     use_lora: bool = True                           # If True, uses LoRA fine-tuning\n-    lora_rank: int = 32                              # Rank of LoRA weight matrix\n+    lora_rank: int = 64                              # Rank of LoRA weight matrix\n     lora_dropout: float = 0.0                        # Dropout applied to LoRA weights\n     merge_lora_during_training: bool = True         # If True, merges LoRA weights and saves result during training\n                                                      #   Note: Merging can be very slow on some machines. If so, set to\n                                                      #         False and merge final checkpoint offline!\n"
                },
                {
                    "date": 1765105544622,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -774,9 +774,9 @@\n     # Load processor and VLA\n     AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n     processor = AutoProcessor.from_pretrained(cfg.config_file_path, trust_remote_code=True)\n \n-    if cfg.use_minivlm:#使用旧的vla权重加载进来，改成新的名字，并迁移\n+    if cfg.use_minivlm:#使用旧的vla权重加载进来，改成新的名字，并迁移 vlm->vla\n         hf_token = ''\n         if 'prism-qwen25-extra-dinosiglip-224px-0_5b' in cfg.vlm_path:\n             \n             vlm = load(cfg.vlm_path, hf_token=hf_token, load_for_training=True)\n"
                },
                {
                    "date": 1765106966105,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -398,9 +398,9 @@\n         \n         for item in output.hidden_states[0:]:\n             # last_hidden_states = output.hidden_states[-1]  # (B, seq_len, D)\n             # Get hidden states for text portion of prompt+response (after the vision patches)\n-            text_hidden_states = item[:, num_patches:-1]\n+            text_hidden_states = item[:, num_patches:-1] #text + action\n             # Get hidden states for action portion of response\n             batch_size = batch[\"input_ids\"].shape[0]\n             # actions_hidden_states = text_hidden_states[:, -1, :].reshape(batch_size, 1, -1).to(torch.bfloat16)\n             actions_hidden_states = text_hidden_states[current_action_mask | next_actions_mask].reshape(batch_size, 1,NUM_TOKENS, -1).to(torch.bfloat16)\n"
                },
                {
                    "date": 1765107124366,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -403,8 +403,9 @@\n             # Get hidden states for action portion of response\n             batch_size = batch[\"input_ids\"].shape[0]\n             # actions_hidden_states = text_hidden_states[:, -1, :].reshape(batch_size, 1, -1).to(torch.bfloat16)\n             actions_hidden_states = text_hidden_states[current_action_mask | next_actions_mask].reshape(batch_size, 1,NUM_TOKENS, -1).to(torch.bfloat16)\n+            #(B, T_text, D)->(B, 1, 64, D)\n             task_latten_states = item[:, :num_patches].reshape(batch_size, 1, num_patches , -1)\n             all_hidden_states = torch.cat((task_latten_states, actions_hidden_states),2)\n             multi_layer_hidden_states.append(all_hidden_states)\n         multi_layer_hidden_states = torch.cat(multi_layer_hidden_states, dim = 1)\n"
                },
                {
                    "date": 1765110339369,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -91,9 +91,9 @@\n     batch_size: int = 4                              # Batch size per device (total batch size = batch_size * num GPUs)\n     learning_rate: float = 5e-4                      # Learning rate\n     lr_warmup_steps: int = 0.1                       # Number of steps to warm up learning rate (from 10% to 100%)\n     num_steps_before_decay: int = 200000             # Number of steps before LR decays by 10x\n-    grad_accumulation_steps: int = 1                 # Number of gradient accumulation steps\n+    grad_accumulation_steps: int = 4                 # Number of gradient accumulation steps\n     max_steps: int = 200005                          # Max number of training steps\n     use_val_set: bool = False                        # If True, uses validation set and log validation metrics\n     val_freq: int = 10_000                           # (When `use_val_set==True`) Validation set logging frequency in steps\n     val_time_limit: int = 180                        # (When `use_val_set==True`) Time limit for computing validation metrics\n"
                },
                {
                    "date": 1765112593217,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -769,9 +769,10 @@\n         update_auto_map(cfg.config_file_path)\n         check_model_logic_mismatch(cfg.config_file_path)\n \n     # Wait for model files to be synced\n-    dist.barrier()\n+    if dist.is_available() and dist.is_initialized():\n+        dist.barrier()\n \n     # Load processor and VLA\n     AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n     processor = AutoProcessor.from_pretrained(cfg.config_file_path, trust_remote_code=True)\n"
                },
                {
                    "date": 1765264641531,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -843,9 +843,9 @@\n         vla = get_peft_model(vla, lora_config)\n         for name, param in vla.named_parameters():\n             if \"action_queries\" in name:\n                 param.requires_grad = True\n-        vla.print_trainable_parameters()\n+        vla.print_trainable_parameters() #103M\n \n     else:\n         for name, param in vla.named_parameters():\n             if \"action_queries\" in name:\n"
                },
                {
                    "date": 1765264850903,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -868,9 +868,9 @@\n             vla.model.vision_backbone.load_state_dict(state_dict)\n         vla.model.vision_backbone = vla.model.vision_backbone.to(device_id)\n \n     # Wrap VLA with DDP\n-    vla = wrap_ddp(vla, device_id, find_unused=True)\n+    # vla = wrap_ddp(vla, device_id, find_unused=True)\n \n     # If applicable, instantiate proprio projector\n     if cfg.use_proprio:\n         proprio_projector = init_module(\n"
                },
                {
                    "date": 1765264984248,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -868,9 +868,9 @@\n             vla.model.vision_backbone.load_state_dict(state_dict)\n         vla.model.vision_backbone = vla.model.vision_backbone.to(device_id)\n \n     # Wrap VLA with DDP\n-    # vla = wrap_ddp(vla, device_id, find_unused=True)\n+    vla = wrap_ddp(vla, device_id, find_unused=True)\n \n     # If applicable, instantiate proprio projector\n     if cfg.use_proprio:\n         proprio_projector = init_module(\n"
                },
                {
                    "date": 1765266527545,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,9 +73,9 @@\n \n     # Dataset\n     # data_root_dir: Path = Path(\"datasets/rlds\")      # Directory containing RLDS datasets\n     data_root_dir: Path = Path(\"data/libero\")\n-    dataset_name: str = \"aloha_scoop_x_into_bowl\"    # Name of fine-tuning dataset (e.g., `aloha_scoop_x_into_bowl`)\n+    dataset_name: str = \"libero_spatial_no_noops\"    # Name of fine-tuning dataset (e.g., `aloha_scoop_x_into_bowl`)\n     run_root_dir: Path = Path(\"outputs\")                # Path to directory to store logs & checkpoints\n     shuffle_buffer_size: int = 100_000               # Dataloader shuffle buffer size (can reduce if OOM errors occur)\n \n     # Algorithm and architecture\n"
                },
                {
                    "date": 1765268030227,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,18 +65,19 @@\n \n @dataclass\n class FinetuneConfig:\n     # fmt: off\n+\n     config_file_path: str = \"pretrained_models/configs\"     # Path to necessary config files of LA-Adapter\n     vlm_path: str = \"pretrained_models/prism-qwen25-extra-dinosiglip-224px-0_5b\"             # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n     use_minivlm: bool = True                        # \n     resum_vla_path: str = \"openvla/openvla-7b\"       # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n \n     # Dataset\n     # data_root_dir: Path = Path(\"datasets/rlds\")      # Directory containing RLDS datasets\n     data_root_dir: Path = Path(\"data/libero\")\n+    run_root_dir: Path = Path(\"outputs\")                # Path to directory to store logs & checkpoints\n     dataset_name: str = \"libero_spatial_no_noops\"    # Name of fine-tuning dataset (e.g., `aloha_scoop_x_into_bowl`)\n-    run_root_dir: Path = Path(\"outputs\")                # Path to directory to store logs & checkpoints\n     shuffle_buffer_size: int = 100_000               # Dataloader shuffle buffer size (can reduce if OOM errors occur)\n \n     # Algorithm and architecture\n     use_l1_regression: bool = True                   # If True, trains continuous action head with L1 regression objective\n"
                },
                {
                    "date": 1765268097948,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1101,5 +1101,30 @@\n                     new_state_dict=RAW_STATE_DICT,\n                 )\n \n             # Test model on validation set\n-            if cfg.use_val_set and log_step > 0 and l\n\\ No newline at end of file\n+            if cfg.use_val_set and log_step > 0 and log_step % cfg.val_freq == 0:\n+                run_validation(\n+                    vla=vla,\n+                    action_head=action_head,\n+                    noisy_action_projector=None,\n+                    proprio_projector=proprio_projector if cfg.use_proprio else None,\n+                    val_dataloader=val_dataloader,\n+                    action_tokenizer=action_tokenizer,\n+                    device_id=device_id,\n+                    cfg=cfg,\n+                    num_patches=NUM_PATCHES,\n+                    log_step=log_step,\n+                    distributed_state=distributed_state,\n+                    val_time_limit=cfg.val_time_limit,\n+                )\n+                # Set model back to training mode after validation\n+                vla.train()\n+\n+            # Stop training when max_steps is reached\n+            if log_step == cfg.max_steps:\n+                print(f\"Max step {cfg.max_steps} reached! Stopping training...\")\n+                break\n+\n+\n+if __name__ == \"__main__\":\n+    finetune()\n"
                },
                {
                    "date": 1765268107885,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1101,5 +1101,30 @@\n                     new_state_dict=RAW_STATE_DICT,\n                 )\n \n             # Test model on validation set\n-            if cfg.use_val_set and log_step > 0 and l\n\\ No newline at end of file\n+            if cfg.use_val_set and log_step > 0 and log_step % cfg.val_freq == 0:\n+                run_validation(\n+                    vla=vla,\n+                    action_head=action_head,\n+                    noisy_action_projector=None,\n+                    proprio_projector=proprio_projector if cfg.use_proprio else None,\n+                    val_dataloader=val_dataloader,\n+                    action_tokenizer=action_tokenizer,\n+                    device_id=device_id,\n+                    cfg=cfg,\n+                    num_patches=NUM_PATCHES,\n+                    log_step=log_step,\n+                    distributed_state=distributed_state,\n+                    val_time_limit=cfg.val_time_limit,\n+                )\n+                # Set model back to training mode after validation\n+                vla.train()\n+\n+            # Stop training when max_steps is reached\n+            if log_step == cfg.max_steps:\n+                print(f\"Max step {cfg.max_steps} reached! Stopping training...\")\n+                break\n+\n+\n+if __name__ == \"__main__\":\n+    finetune()\n"
                }
            ],
            "date": 1765092846921,
            "name": "Commit-0",
            "content": "\"\"\"\nfinetune.py\n\nFine-tunes Qwen2.5-0.5B via LoRA.\n\"\"\"\n\nimport os\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, Optional, Tuple, Type\nimport torch.nn.functional as F\nimport draccus\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport tqdm\nfrom accelerate import PartialState\nfrom huggingface_hub import HfApi, snapshot_download\nfrom peft import LoraConfig, PeftModel, get_peft_model\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoConfig, AutoImageProcessor, AutoModelForVision2Seq, AutoProcessor\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nimport wandb\n\nfrom experiments.robot.openvla_utils import (\n    check_model_logic_mismatch,\n    model_is_on_hf_hub,\n    update_auto_map\n)\nfrom prismatic.extern.hf.configuration_prismatic import OpenVLAConfig\nfrom prismatic.extern.hf.modeling_prismatic import OpenVLAForActionPrediction\nfrom prismatic.extern.hf.processing_prismatic import PrismaticImageProcessor, PrismaticProcessor\nfrom prismatic.models.action_heads import L1RegressionActionHead\nfrom prismatic.models.backbones.llm.prompting import PurePromptBuilder\nfrom prismatic.models.film_vit_wrapper import FiLMedPrismaticVisionBackbone\nfrom prismatic.models.projectors import ProprioProjector\nfrom prismatic.training.train_utils import (\n    compute_actions_l1_loss,\n    compute_token_accuracy,\n    get_current_action_mask,\n    get_next_actions_mask\n)\nfrom prismatic.util.data_utils import PaddedCollatorForActionPrediction\nfrom prismatic.vla.action_tokenizer import ActionTokenizer\nfrom prismatic.vla.constants import (\n    ACTION_DIM,\n    ACTION_PROPRIO_NORMALIZATION_TYPE,\n    NUM_ACTIONS_CHUNK,\n    PROPRIO_DIM,\n    NUM_TOKENS\n)\nfrom prismatic.vla.datasets import RLDSDataset, RLDSBatchTransform\nfrom prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics\nfrom prismatic.models import load, load_vla\n\n\n\n# Sane Defaults\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n@dataclass\nclass FinetuneConfig:\n    # fmt: off\n    config_file_path: str = \"openvla/openvla-7b\"     # Path to necessary config files of LA-Adapter\n    vlm_path: str = \"openvla/openvla-7b\"             # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n    use_minivlm: bool = False                        # \n    resum_vla_path: str = \"openvla/openvla-7b\"       # Path to OpenVLA model (on HuggingFace Hub or stored locally)\n\n    # Dataset\n    data_root_dir: Path = Path(\"datasets/rlds\")      # Directory containing RLDS datasets\n    dataset_name: str = \"aloha_scoop_x_into_bowl\"    # Name of fine-tuning dataset (e.g., `aloha_scoop_x_into_bowl`)\n    run_root_dir: Path = Path(\"runs\")                # Path to directory to store logs & checkpoints\n    shuffle_buffer_size: int = 100_000               # Dataloader shuffle buffer size (can reduce if OOM errors occur)\n\n    # Algorithm and architecture\n    use_l1_regression: bool = True                   # If True, trains continuous action head with L1 regression objective\n    use_diffusion: bool = False                      # If True, trains continuous action head with diffusion modeling objective (DDIM)\n    num_diffusion_steps: int = 50                    # (When `diffusion==True`) Number of diffusion steps for training \n    use_film: bool = False                           # If True, uses FiLM to infuse language inputs into visual features\n    num_images_in_input: int = 1                     # Number of images in the VLA input (default: 1)\n    use_proprio: bool = False                        # If True, includes robot proprioceptive state in input\n    phase1_path: str = \"None\"\n\n    # Training configuration\n    batch_size: int = 8                              # Batch size per device (total batch size = batch_size * num GPUs)\n    learning_rate: float = 5e-4                      # Learning rate\n    lr_warmup_steps: int = 0.1                       # Number of steps to warm up learning rate (from 10% to 100%)\n    num_steps_before_decay: int = 100000             # Number of steps before LR decays by 10x\n    grad_accumulation_steps: int = 1                 # Number of gradient accumulation steps\n    max_steps: int = 200000                          # Max number of training steps\n    use_val_set: bool = False                        # If True, uses validation set and log validation metrics\n    val_freq: int = 10_000                           # (When `use_val_set==True`) Validation set logging frequency in steps\n    val_time_limit: int = 180                        # (When `use_val_set==True`) Time limit for computing validation metrics\n    save_freq: int = 10_000                          # Checkpoint saving frequency in steps\n    save_latest_checkpoint_only: bool = False        # If True, saves only 1 checkpoint, overwriting latest checkpoint\n                                                     #   (If False, saves all checkpoints)\n    resume: bool = False                             # If True, resumes from checkpoint\n    resume_step: Optional[int] = None                # (When `resume==True`) Step number that we are resuming from\n    image_aug: bool = True                           # If True, trains with image augmentations (HIGHLY RECOMMENDED)\n    diffusion_sample_freq: int = 50                  # (When `use_diffusion==True`) Frequency for sampling in steps\n\n    # LoRA\n    use_lora: bool = False                           # If True, uses LoRA fine-tuning\n    lora_rank: int = 32                              # Rank of LoRA weight matrix\n    lora_dropout: float = 0.0                        # Dropout applied to LoRA weights\n    merge_lora_during_training: bool = False         # If True, merges LoRA weights and saves result during training\n                                                     #   Note: Merging can be very slow on some machines. If so, set to\n                                                     #         False and merge final checkpoint offline!\n\n    # Full Finetune\n    use_fz: bool = False                             # If True, uses LoRA fine-tuning\n\n    # Logging\n    wandb_entity: str = \"your-wandb-entity\"          # Name of WandB entity\n    wandb_project: str = \"your-wandb-project\"        # Name of WandB project\n    run_id_note: Optional[str] = None                # Extra note to add to end of run ID for logging\n    run_id_override: Optional[str] = None            # Optional string to override the run ID with\n    wandb_log_freq: int = 10                         # WandB logging frequency in steps\n\n    # revision version\n    use_pro_version: bool = True                             # the version number\n    phase: str = \"Training\"\n    # fmt: on\n\n\n\ndef remove_ddp_in_checkpoint(state_dict) -> dict:\n    \"\"\"\n    Removes the 'module.' prefix from parameter names in a PyTorch model state dictionary that was saved using\n    DistributedDataParallel (DDP).\n\n    When a model is trained using PyTorch's DistributedDataParallel, the saved state dictionary contains parameters\n    prefixed with 'module.'. This function removes these prefixes to make the state dictionary compatible when\n    loading into models that are not yet wrapped in DDP.\n\n    Args:\n        state_dict (dict): PyTorch model state dictionary.\n\n    Returns:\n        dict: A new state dictionary with the same contents but with 'module.' prefixes removed from parameter names.\n              Parameters without the 'module.' prefix remain unchanged.\n    \"\"\"\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        if k[:7] == \"module.\":\n            new_state_dict[k[7:]] = v\n        else:\n            new_state_dict[k] = v\n    return new_state_dict\n\n\n\ndef get_run_id(cfg) -> str:\n    \"\"\"\n    Generates or retrieves an identifier string for an experiment run.\n\n    Args:\n        cfg (FinetuneConfig): Training configuration.\n\n    Returns:\n        str: Experiment run ID.\n    \"\"\"\n    if cfg.run_id_override is not None:\n        # Override the run ID with the user-provided ID\n        run_id = cfg.run_id_override\n    elif cfg.resume:\n        # Override run ID with the previous resumed run's ID\n        run_id = cfg.config_file_path.split(\"/\")[-1]\n        # Remove the \"--XXX_chkpt\" suffix from the run ID if it exists\n        if \"chkpt\" in run_id.split(\"--\")[-1]:\n            run_id = \"--\".join(run_id.split(\"--\")[:-1])\n    else:\n        run_id = (\n            f\"{cfg.config_file_path.split('/')[-1]}+{cfg.dataset_name}\"\n            f\"+b{cfg.batch_size * cfg.grad_accumulation_steps}\"\n            f\"+lr-{cfg.learning_rate}\"\n        )\n        if cfg.use_fz:\n            run_id += f\"+frozen+dropout-{cfg.lora_dropout}\"\n        if cfg.use_lora:\n            run_id += f\"+lora-r{cfg.lora_rank}+dropout-{cfg.lora_dropout}\"\n        if cfg.image_aug:\n            run_id += \"--image_aug\"\n        if cfg.run_id_note is not None:\n            run_id += f\"--{cfg.run_id_note}\"\n    return run_id\n\n\n\ndef load_checkpoint(module_name: str, path: str, step: int, device: str = \"cpu\") -> dict:\n    \"\"\"\n    Loads a checkpoint for a given module.\n\n    Args:\n        module_name (str): Name of model component to load checkpoint for.\n        path (str): Path to checkpoint directory.\n        step (int): Gradient step number of saved checkpoint.\n        device (str): String specifying how to remap storage locations (default = \"cpu\").\n\n    Returns:\n        dict: PyTorch model state dictionary.\n    \"\"\"\n    checkpoint_path = os.path.join(path, f\"{module_name}--{step}_checkpoint.pt\")\n    print(f\"Loading checkpoint: {checkpoint_path}\")\n    state_dict = torch.load(checkpoint_path, weights_only=True, map_location=device)\n    return remove_ddp_in_checkpoint(state_dict)\n\n\n\ndef wrap_ddp(module: nn.Module, device_id: int, find_unused: bool = False) -> DDP:\n    \"\"\"\n    Wrap a module with DistributedDataParallel.\n\n    Args:\n        module (nn.Module): PyTorch module.\n        device_id (str): Device ID.\n        find_unused (bool): Whether to detect parameters without gradients in distributed training.\n\n    Returns:\n        DistributedDataParallel: PyTorch module wrapped with DDP.\n    \"\"\"\n    return DDP(module, device_ids=[device_id], find_unused_parameters=find_unused, gradient_as_bucket_view=True)\n\n\n\ndef count_parameters(module: nn.Module, name: str) -> None:\n    \"\"\"\n    Counts and prints the number of trainable parameters in a module.\n\n    Args:\n        module (nn.Module): PyTorch module.\n        module_name (str): Name of model component.\n\n    Returns:\n        None.\n    \"\"\"\n    num_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n    \n    print(f\"# trainable params in {name}: {num_params}\")\n\n\n\ndef init_module(\n    module_class: Type[nn.Module],\n    module_name: str,\n    cfg: FinetuneConfig,\n    device_id: int,\n    module_args: dict,\n    to_bf16: bool = False,\n    find_unused_params: bool = False,\n) -> DDP:\n    \"\"\"\n    Initializes a module, optionally loads checkpoint, moves to device, and wraps with DDP.\n\n    Args:\n        module_class (Type[nn.Module]): Class of PyTorch module to initialize.\n        module_name (str): Name of model component to load checkpoint for.\n        cfg (FinetuneConfig): Training configuration.\n        device_id (str): Device ID.\n        module_args (dict): Args for initializing the module.\n        to_bf16 (bool): Whether to convert to torch.bfloat16 data type.\n        find_unused_params (bool): Whether to detect parameters without gradients in distributed training.\n\n    Returns:\n        DistributedDataParallel: PyTorch module wrapped with DDP.\n    \"\"\"\n    module = module_class(**module_args)\n    count_parameters(module, module_name)\n\n    if cfg.resume:\n        state_dict = load_checkpoint(module_name, cfg.resum_vla_path, cfg.resume_step)\n        module.load_state_dict(state_dict)\n        print('loaded!!!!!!!!!')\n\n    if to_bf16:\n        module = module.to(torch.bfloat16)\n    module = module.to(device_id)\n\n    return wrap_ddp(module, device_id, find_unused_params)\n\n\n\ndef run_forward_pass(\n    vla,\n    action_head,\n    proprio_projector,\n    batch,\n    action_tokenizer,\n    device_id,\n    use_l1_regression,\n    use_proprio,\n    use_film,\n    num_patches,\n    compute_diffusion_l1=False,\n    use_pro_version=True,\n    cfg=None\n) -> Tuple[torch.Tensor, Dict[str, float]]:\n    \"\"\"\n    Compute model forward pass and metrics for both training and validation.\n\n    Args:\n        vla (OpenVLAForActionPrediction): Vision-language-action policy.\n        action_head (nn.Module): Action head module.\n        noisy_action_projector (nn.Module): Noisy action projector module (only used for diffusion).\n        proprio_projector (nn.Module): Proprioceptive state projector module.\n        batch (dict): Input batch.\n        action_tokenizer (ActionTokenizer): Action tokenizer.\n        device_id (str): Device ID.\n        use_l1_regression (bool): Whether to use L1 regression.\n        use_diffusion (bool): Whether to use diffusion.\n        use_proprio (bool): Whether to use proprioceptive state as input.\n        use_film (bool): Whether to use FiLM for better language following.\n        num_patches (int): Number of vision patches.\n        compute_diffusion_l1 (bool): Whether to sample actions and compute L1 loss for diffusion (do this once every\n                                    diffusion_sample_freq steps during training; do it every batch for validation)\n        num_diffusion_steps (int): Number of diffusion steps (only used for diffusion).\n\n    Returns:\n        tuple: (loss, metrics_dict)\n            loss: The loss tensor with gradient for backpropagation.\n            metrics_dict: Dictionary of computed metrics (detached values for logging).\n    \"\"\"\n    metrics = {}\n\n    # Get ground-truth action labels\n    ground_truth_actions = batch[\"actions\"].to(device_id).to(torch.bfloat16)\n    noise, noisy_actions, diffusion_timestep_embeddings = None, None, None\n\n    # VLA forward pass\n    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n        output: CausalLMOutputWithPast = vla(\n            input_ids=batch[\"input_ids\"].to(device_id),\n            attention_mask=batch[\"attention_mask\"].to(device_id),\n            pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n            labels=batch[\"labels\"],\n            output_hidden_states=True,\n            proprio=batch[\"proprio\"] if use_proprio else None,\n            proprio_projector=proprio_projector if use_proprio else None,\n            noisy_actions=None,\n            noisy_action_projector=None,\n            diffusion_timestep_embeddings=None,\n            use_film=use_film,\n            )\n\n    # Get action masks needed for logging\n    ground_truth_token_ids = batch[\"labels\"][:,1:].to(device_id)\n    current_action_mask = get_current_action_mask(ground_truth_token_ids)\n    next_actions_mask = get_next_actions_mask(ground_truth_token_ids)\n\n    # Compute metrics for discrete action representation (next-token prediction)\n    if not (use_l1_regression):\n        loss = output.loss\n        predicted_token_ids = output.logits[:, num_patches:-1].argmax(dim=2)\n\n        curr_action_accuracy = compute_token_accuracy(\n            predicted_token_ids, \n            ground_truth_token_ids, \n            mask=current_action_mask\n            )\n        curr_action_l1_loss = compute_actions_l1_loss(\n            action_tokenizer, \n            predicted_token_ids, \n            ground_truth_token_ids, \n            mask=current_action_mask\n            )\n        next_actions_accuracy = compute_token_accuracy(\n            predicted_token_ids, \n            ground_truth_token_ids, \n            mask=next_actions_mask\n            )\n        next_actions_l1_loss = compute_actions_l1_loss(\n            action_tokenizer, \n            predicted_token_ids, \n            ground_truth_token_ids, \n            mask=next_actions_mask\n            )\n        \n        metrics.update(\n            {\n                \"loss_value\": loss.item(),  # Detached value for logging\n                \"curr_action_accuracy\": curr_action_accuracy.item(),\n                \"curr_action_l1_loss\": curr_action_l1_loss.item(),\n                \"next_actions_accuracy\": next_actions_accuracy.item(),\n                \"next_actions_l1_loss\": next_actions_l1_loss.item(),\n                }\n            )\n        \n    # Compute metrics for continuous action representations (L1 regression)\n    else:\n        # Get last layer hidden states\n        multi_layer_hidden_states = []\n        \n        for item in output.hidden_states[0:]:\n            # last_hidden_states = output.hidden_states[-1]  # (B, seq_len, D)\n            # Get hidden states for text portion of prompt+response (after the vision patches)\n            text_hidden_states = item[:, num_patches:-1]\n            # Get hidden states for action portion of response\n            batch_size = batch[\"input_ids\"].shape[0]\n            # actions_hidden_states = text_hidden_states[:, -1, :].reshape(batch_size, 1, -1).to(torch.bfloat16)\n            actions_hidden_states = text_hidden_states[current_action_mask | next_actions_mask].reshape(batch_size, 1,NUM_TOKENS, -1).to(torch.bfloat16)\n            task_latten_states = item[:, :num_patches].reshape(batch_size, 1, num_patches , -1)\n            all_hidden_states = torch.cat((task_latten_states, actions_hidden_states),2)\n            multi_layer_hidden_states.append(all_hidden_states)\n        multi_layer_hidden_states = torch.cat(multi_layer_hidden_states, dim = 1)\n\n        predicted_actions = action_head.module.predict_action(\n            multi_layer_hidden_states,\n            proprio=batch[\"proprio\"] if use_proprio else None,\n            proprio_projector=proprio_projector if use_proprio else None,\n            phase=cfg.phase,\n            )\n\n        loss = torch.nn.L1Loss()(predicted_actions, ground_truth_actions)\n\n        metrics.update(\n            {\n                \"loss_value\": loss.item(),  # Detached value for logging\n            }\n        )\n\n        # Get detailed L1 losses for logging\n        should_log_l1_loss = use_l1_regression\n        if should_log_l1_loss:\n            ground_truth_curr_action = ground_truth_actions[:, 0]\n            predicted_curr_action = predicted_actions[:, 0]\n            ground_truth_next_actions = ground_truth_actions[:, 1:]\n            predicted_next_actions = predicted_actions[:, 1:]\n            curr_action_l1_loss = torch.nn.L1Loss()(ground_truth_curr_action, predicted_curr_action)\n            next_actions_l1_loss = torch.nn.L1Loss()(ground_truth_next_actions, predicted_next_actions)\n            if compute_diffusion_l1:\n                print('curr: ',curr_action_l1_loss.item())\n                # print('next: ',next_actions_l1_loss.item())\n\n            metrics.update(\n                {\n                    \"curr_action_l1_loss\": curr_action_l1_loss.item(),\n                    \"next_actions_l1_loss\": next_actions_l1_loss.item(),\n                }\n            )\n\n    # Return both the loss tensor (with gradients) and the metrics dictionary (with detached values)\n    return loss, metrics\n\n\n\ndef compute_smoothened_metrics(metrics_deques) -> dict:\n    \"\"\"\n    Compute smoothened metrics from recent deques.\n\n    Args:\n        metrics_deques (dict): Dictionary of deques containing recent metrics.\n\n    Returns:\n        dict: Dictionary of smoothened metrics.\n    \"\"\"\n    smoothened_metrics = {}\n    for name, deque in metrics_deques.items():\n        if deque and len(deque) > 0:\n            smoothened_metrics[name] = sum(deque) / len(deque)\n    return smoothened_metrics\n\n\n\ndef log_metrics_to_wandb(metrics, prefix, step, wandb_entity) -> None:\n    \"\"\"\n    Log metrics to Weights & Biases.\n\n    Args:\n        metrics (dict): Dictionary of metrics to log\n        prefix (str): Prefix for metric names\n        step (int): Training step\n        wandb_entity (str): W&B entity instance\n\n    Returns:\n        None.\n    \"\"\"\n    log_dict = {}\n    for name, value in metrics.items():\n        # Map loss_value to Loss for better readability in W&B\n        if name == \"loss_value\":\n            log_dict[f\"{prefix}/Loss\"] = value\n        # Keep other metrics as is\n        else:\n            log_dict[f\"{prefix}/{name.replace('_', ' ').title()}\"] = value\n    wandb_entity.log(log_dict, step=step)\n\n\n\ndef save_training_checkpoint(\n    cfg,\n    run_dir,\n    log_step,\n    vla,\n    processor,\n    proprio_projector,\n    noisy_action_projector,\n    action_head,\n    train_dataset,\n    distributed_state,\n    new_state_dict,\n    \n) -> None:\n    \"\"\"\n    Save all training checkpoints including model components, LoRA adapter, and dataset statistics.\n\n    Args:\n        cfg (FinetuneConfig): Training configuration.\n        run_dir (Path): Experiment run directory path.\n        log_step (int): Current logging step.\n        vla (OpenVLAForActionPrediction): Vision-language-action policy.\n        processor (PrismaticProcessor): OpenVLA inputs processor.\n        proprio_projector (nn.Module): Proprioceptive state projector module.\n        noisy_action_projector (nn.Module): Noisy action projector module (only used for diffusion).\n        action_head (nn.Module): Action head module.\n        train_dataset (RLDSDataset): Training dataset.\n        distributed_state (PartialState): Distributed training state.\n\n    Returns:\n        None.\n    \"\"\"\n    # Determine checkpoint paths and naming\n    if cfg.save_latest_checkpoint_only:\n        checkpoint_dir = run_dir\n        checkpoint_name_suffix = \"latest_checkpoint.pt\"\n    else:\n        checkpoint_dir = Path(str(run_dir) + f\"--{log_step}_chkpt\")\n        checkpoint_name_suffix = f\"{log_step}_checkpoint.pt\"\n\n    adapter_dir = checkpoint_dir / \"lora_adapter\"\n\n    # Create directories and save dataset statistics (main process only)\n    if distributed_state.is_main_process:\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        os.makedirs(adapter_dir, exist_ok=True)\n        save_dataset_statistics(train_dataset.dataset_statistics, checkpoint_dir)\n        print(f\"Saving Model Checkpoint for Step {log_step}\")\n\n    # Wait for directories to be created\n    dist.barrier()\n\n    # Save model components (main process only)\n    if distributed_state.is_main_process:\n        # Save processor and LoRA adapter\n        processor.save_pretrained(checkpoint_dir)\n\n        if cfg.use_fz:\n            vla.module.save_pretrained(checkpoint_dir) # directly save checkpoint without lora\n        else:\n            vla.module.save_pretrained(adapter_dir)\n\n        # Save other components\n        if cfg.use_proprio and proprio_projector is not None:\n            torch.save(proprio_projector.state_dict(), checkpoint_dir / f\"proprio_projector--{checkpoint_name_suffix}\")\n\n        if cfg.use_diffusion and noisy_action_projector is not None:\n            torch.save(\n                noisy_action_projector.state_dict(), checkpoint_dir / f\"noisy_action_projector--{checkpoint_name_suffix}\"\n            )\n\n        if cfg.use_l1_regression and action_head is not None:\n            torch.save(action_head.state_dict(), checkpoint_dir / f\"action_head--{checkpoint_name_suffix}\")\n\n        if cfg.use_film:\n            # To be safe, just save the entire vision backbone (not just FiLM components)\n            torch.save(\n                vla.module.vision_backbone.state_dict(), checkpoint_dir / f\"vision_backbone--{checkpoint_name_suffix}\"\n            )\n\n    # Wait for model components to be saved\n    dist.barrier()\n\n    # Merge LoRA weights into base model and save resulting model checkpoint\n    # Note: Can be very slow on some devices; if so, we recommend merging offline\n    if cfg.use_lora and cfg.merge_lora_during_training:\n        if cfg.use_minivlm:\n            config = AutoConfig.from_pretrained(\"pretrained_models/configs/config.json\")\n            base_vla = AutoModelForVision2Seq.from_config(config, torch_dtype=torch.bfloat16)  # Create a new model with configuration, the parameters are randomly initialized\n            # print(new_state_dict['action_queries.weight'])\n            new_state_dict['action_queries.weight'] = vla.state_dict()['module.base_model.model.action_queries.weight'].cpu()\n            missing_keys, unexpected_keys = base_vla.load_state_dict(new_state_dict, strict=False)\n            \n        else:\n            base_vla = AutoModelForVision2Seq.from_pretrained(\n            cfg.config_file_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=False, trust_remote_code=False\n        )\n\n\n        merged_vla = PeftModel.from_pretrained(base_vla, adapter_dir)\n        merged_vla = merged_vla.merge_and_unload()\n\n        if distributed_state.is_main_process:\n            merged_vla.save_pretrained(checkpoint_dir)\n            print(f\"Saved merged model for Step {log_step} at: {checkpoint_dir}\")\n        \n        # Wait for merged model to be saved\n        dist.barrier()\n\n\n\ndef run_validation(\n    vla,\n    action_head,\n    noisy_action_projector,\n    proprio_projector,\n    val_dataloader,\n    action_tokenizer,\n    device_id,\n    cfg,\n    num_patches,\n    log_step,\n    distributed_state,\n    val_time_limit,\n) -> None:\n    \"\"\"\n    Compute validation set metrics for logging.\n\n    Args:\n        vla (OpenVLAForActionPrediction): Vision-language-action policy.\n        action_head (nn.Module): Action head module.\n        noisy_action_projector (nn.Module): Noisy action projector module (only used for diffusion).\n        proprio_projector (nn.Module): Proprioceptive state projector module.\n        val_dataloader (DataLoader): Validation data loader.\n        action_tokenizer (ActionTokenizer): Action tokenizer.\n        device_id (str): Device ID.\n        cfg (FinetuneConfig): Training configuration.\n        num_patches (int): Number of vision patches.\n        log_step (int): Current logging step.\n        distributed_state (PartialState): Distributed training state.\n        val_time_limit (int): Time limit for computing validation metrics.\n\n    Returns:\n        None.\n    \"\"\"\n    val_start_time = time.time()\n    vla.eval()\n    val_batches_count = 0\n\n    # List to store validation metrics\n    all_val_metrics = []\n\n    with torch.no_grad():\n        for batch in val_dataloader:\n            # Always compute L1 loss for validation, even for diffusion\n            _, metrics = run_forward_pass(\n                vla=vla,\n                action_head=action_head,\n                proprio_projector=proprio_projector,\n                batch=batch,\n                action_tokenizer=action_tokenizer,\n                device_id=device_id,\n                use_l1_regression=cfg.use_l1_regression,\n                use_proprio=cfg.use_proprio,\n                use_film=cfg.use_film,\n                num_patches=num_patches,\n                compute_diffusion_l1=True,\n                use_pro_version=cfg.use_pro_version\n            )\n\n            # Add the loss value to the metrics\n            metrics[\"loss\"] = metrics[\"loss_value\"]\n            all_val_metrics.append(metrics)\n            val_batches_count += 1\n\n            # Cut testing on validation set short if it exceeds time limit\n            if time.time() - val_start_time > val_time_limit:\n                break\n\n    # Compute average validation metrics\n    avg_val_metrics = {}\n    for metric_name in all_val_metrics[0].keys():\n        values = [metrics[metric_name] for metrics in all_val_metrics if metric_name in metrics]\n        if values:\n            avg_val_metrics[metric_name] = sum(values) / len(values)\n\n    # Add batch count to metrics\n    avg_val_metrics[\"val_batches_count\"] = val_batches_count\n\n    # Log validation metrics to W&B\n    if distributed_state.is_main_process:\n        log_metrics_to_wandb(avg_val_metrics, \"VLA Val\", log_step, wandb)\n\n\n\n@draccus.wrap()\ndef finetune(cfg: FinetuneConfig) -> None:\n    \"\"\"\n    Fine-tunes base VLA on demonstration dataset via LoRA.\n\n    Allows toggling different action representations (discrete vs. continuous), different learning objectives\n    (next-token prediction vs. L1 regression vs. diffusion), FiLM. Also allows for additional model inputs,\n    such as additional camera images and robot proprioceptive state. Assumes parallel action generation with\n    action chunking.\n\n    Args:\n        cfg (FinetuneConfig): Training configuration.\n\n    Returns:\n        None.\n    \"\"\" \n\n    global RAW_STATE_DICT\n\n    assert not (cfg.use_l1_regression and cfg.use_diffusion), (\n        \"Cannot do both L1 regression and diffusion. Please pick one of them!\"\n    )\n\n    # Trim trailing forward slash ('/') in VLA path if it exists\n    cfg.config_file_path = cfg.config_file_path.rstrip(\"/\")\n    print(f\"Fine-tuning OpenVLA Model `{cfg.config_file_path}` on `{cfg.dataset_name}`\")\n\n    # Get experiment run ID\n    run_id = get_run_id(cfg)\n\n    # Create experiment run directory\n    run_dir = cfg.run_root_dir / run_id\n    os.makedirs(run_dir, exist_ok=True)\n\n    # GPU setup\n    distributed_state = PartialState()\n    device_id = distributed_state.local_process_index\n    torch.cuda.set_device(device_id)\n    torch.cuda.empty_cache()\n\n    # Initialize wandb logging\n    if distributed_state.is_main_process:\n        wandb.init(project=cfg.wandb_project, name=f\"ft+{run_id}\", mode=\"offline\")\n\n    # Print detected constants\n    print(\n        \"Detected constants:\\n\"\n        f\"\\tNUM_ACTIONS_CHUNK: {NUM_ACTIONS_CHUNK}\\n\"\n        f\"\\tACTION_DIM: {ACTION_DIM}\\n\"\n        f\"\\tPROPRIO_DIM: {PROPRIO_DIM}\\n\"\n        f\"\\tACTION_PROPRIO_NORMALIZATION_TYPE: {ACTION_PROPRIO_NORMALIZATION_TYPE}\"\n    )\n\n    # Two options:\n    # (1) Base model is on Hugging Face Hub\n    #   - Then download it and record the path to the download directory\n    # (2) Base model is stored locally\n    #   - Then register model config in HF Auto Classes\n    # In both cases, we want to check whether any changes have been made to\n    # the `modeling_prismatic.py` file in this codebase; if so, we will copy\n    # the file to the downloaded or locally stored checkpoint directory so\n    # that the user's changes to the VLA class logic go into effect\n\n    if model_is_on_hf_hub(cfg.config_file_path):\n        # Download model directly from Hugging Face Hub\n        vla_download_path = snapshot_download(repo_id=cfg.config_file_path)\n        # Overwrite VLA path\n        cfg.config_file_path = vla_download_path\n    else:\n        # Register OpenVLA model to HF Auto Classes (not needed if the model is on HF Hub)\n        AutoConfig.register(\"openvla\", OpenVLAConfig)\n        AutoImageProcessor.register(OpenVLAConfig, PrismaticImageProcessor)\n        AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n        AutoModelForVision2Seq.register(OpenVLAConfig, OpenVLAForActionPrediction)\n\n\n    # Update config.json and sync model files\n    if distributed_state.is_main_process:\n        update_auto_map(cfg.config_file_path)\n        check_model_logic_mismatch(cfg.config_file_path)\n\n    # Wait for model files to be synced\n    dist.barrier()\n\n    # Load processor and VLA\n    AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)\n    processor = AutoProcessor.from_pretrained(cfg.config_file_path, trust_remote_code=True)\n\n    if cfg.use_minivlm:\n        hf_token = ''\n        if 'prism-qwen25-extra-dinosiglip-224px-0_5b' in cfg.vlm_path:\n            \n            vlm = load(cfg.vlm_path, hf_token=hf_token, load_for_training=True)\n        else:\n            vlm = load_vla(\n                cfg.vlm_path,\n                hf_token=hf_token,\n                load_for_training=True,\n                )\n        config = AutoConfig.from_pretrained(\"pretrained_models/configs/config.json\")\n        vla = AutoModelForVision2Seq.from_config(config, torch_dtype=torch.bfloat16).to(device_id)  # Create a new model with configuration, the parameters are randomly initialized\n        # for name, param in model.named_parameters():\n        #     print(f\"{name}: {param.shape}\")\n        replace_map = [\n            (\"vision_backbone.dino_featurizer\", \"vision_backbone.featurizer\"),\n            (\"vision_backbone.siglip_featurizer\", \"vision_backbone.fused_featurizer\"),\n            (\"llm_backbone.llm\", \"language_model\"),\n            (\"projector.projector.0\", \"projector.fc1\"),\n            (\"projector.projector.2\", \"projector.fc2\"),\n            (\"projector.projector.4\", \"projector.fc3\"),\n            (\"gamma\", \"scale_factor\"),\n            ]\n\n        def rename_state_dict_keys(state_dict, replace_map):\n            new_state_dict = {}\n            for k, v in state_dict.items():\n                new_k = k\n                for old, new in replace_map:\n                    if old in new_k:\n                        new_k = new_k.replace(old, new)\n                new_state_dict[new_k] = v\n            return new_state_dict\n        \n        old_state_dict = vlm.state_dict()\n        RAW_STATE_DICT = rename_state_dict_keys(old_state_dict, replace_map)\n    \n        missing_keys, unexpected_keys = vla.load_state_dict(RAW_STATE_DICT, strict=False)\n        del old_state_dict\n\n    else:\n        RAW_STATE_DICT ={}\n        vla = AutoModelForVision2Seq.from_pretrained(\n            cfg.config_file_path,\n            torch_dtype=torch.bfloat16,\n            low_cpu_mem_usage=False,\n            trust_remote_code=False,\n            ).to(device_id)\n\n    # Set number of images in VLA input\n    vla.vision_backbone.set_num_images_in_input(cfg.num_images_in_input)\n\n    # vla.set_version(cfg.version)\n\n    if cfg.use_lora:\n        lora_config = LoraConfig(\n            r=cfg.lora_rank,\n            lora_alpha= 2 * cfg.lora_rank,\n            lora_dropout=cfg.lora_dropout,\n            target_modules=\"all-linear\",\n            init_lora_weights=\"gaussian\",\n        )\n        vla = get_peft_model(vla, lora_config)\n        for name, param in vla.named_parameters():\n            if \"action_queries\" in name:\n                param.requires_grad = True\n        vla.print_trainable_parameters()\n\n    else:\n        for name, param in vla.named_parameters():\n            if \"action_queries\" in name:\n                param.requires_grad = True\n\n    # FiLM setup\n    if cfg.use_film:\n        count_parameters(vla.vision_backbone, \"vla.vision_backbone (original)\")\n        # Wrap vision backbone with FiLM wrapper\n        # Important: For this, must specify `vla.model.vision_backbone` instead of just `vla.vision_backbone`, since the\n        # latter would cause the new wrapped backbone to be saved as a new attribute of `vla` instead of overwriting the\n        # original one (due to the LoRA wrapper)\n        vla.model.vision_backbone = FiLMedPrismaticVisionBackbone(\n            vision_backbone=vla.model.vision_backbone,\n            llm_dim=vla.llm_dim,\n        )\n        count_parameters(vla.vision_backbone, \"vla.vision_backbone (post-wrap)\")\n        if cfg.resume:\n            state_dict = load_checkpoint(\"vision_backbone\", cfg.config_file_path, cfg.resume_step)\n            vla.model.vision_backbone.load_state_dict(state_dict)\n        vla.model.vision_backbone = vla.model.vision_backbone.to(device_id)\n\n    # Wrap VLA with DDP\n    vla = wrap_ddp(vla, device_id, find_unused=True)\n\n    # If applicable, instantiate proprio projector\n    if cfg.use_proprio:\n        proprio_projector = init_module(\n            ProprioProjector,\n            \"proprio_projector\",\n            cfg,\n            device_id,\n            {\"llm_dim\": vla.module.llm_dim, \"proprio_dim\": PROPRIO_DIM},\n            to_bf16=True,\n        )\n\n    # If applicable, instantiate continuous action head for L1 regression\n    if cfg.use_l1_regression:\n        action_head = init_module(\n        L1RegressionActionHead,\n        \"action_head\",\n        cfg,\n        device_id,\n        {\n            \"input_dim\": vla.module.llm_dim, \n            \"hidden_dim\": vla.module.llm_dim, \n            \"action_dim\": ACTION_DIM,\n            \"use_pro_version\": cfg.use_pro_version,\n            },\n        to_bf16=True,\n        )\n\n    # Get number of vision patches\n    NUM_PATCHES = vla.module.vision_backbone.get_num_patches() * vla.module.vision_backbone.get_num_images_in_input()\n    # If we have proprio inputs, a single proprio embedding is appended to the end of the vision patch embeddings\n\n    # Instantiate optimizer\n    trainable_params = [param for param in vla.parameters() if param.requires_grad]\n    if cfg.use_l1_regression:\n        trainable_params += [param for param in action_head.parameters() if param.requires_grad]\n\n    if cfg.use_proprio:\n        trainable_params += [param for param in proprio_projector.parameters() if param.requires_grad]\n    print(f\"# total trainable params: {sum(p.numel() for p in trainable_params)}\")\n    optimizer = AdamW(trainable_params, lr=cfg.learning_rate)\n\n    # Record original learning rate\n    original_lr = optimizer.param_groups[0][\"lr\"]\n\n    # Create learning rate scheduler\n    # 1. MultiStepLR\n    scheduler = MultiStepLR(\n        optimizer,\n        milestones=[cfg.num_steps_before_decay],  # Number of steps after which LR will change\n        gamma=0.1,  # Multiplicative factor of learning rate decay\n    )\n    # 2. CosineAnnealingLR\n    # scheduler = CosineAnnealingLR(\n    #         optimizer,\n    #         T_max=cfg.num_steps_before_decay, \n    #         eta_min=0.0001,          \n    #         )\n\n    # Create Action Tokenizer\n    action_tokenizer = ActionTokenizer(processor.tokenizer)\n\n    # Load Fine-tuning Dataset =>> note that we use an RLDS-formatted dataset following Open X-Embodiment by default.\n    #   =>> If you want to use a non-RLDS dataset (e.g., a standard PyTorch Dataset) see the following commented block.\n    #   =>> Note that our training code does not loop over epochs because the RLDS loader does this implicitly; if using\n    #       your own Dataset, make sure to add the appropriate logic to the training loop!\n    #\n    # ---\n    # from prismatic.vla.datasets import DummyDataset\n    #\n    # train_dataset = DummyDataset(\n    #     action_tokenizer,\n    #     processor.tokenizer,\n    #     image_transform=processor.image_processor.apply_transform,\n    #     prompt_builder_fn=PurePromptBuilder,\n    # )\n    # ---\n\n    # We assume that the model takes as input one third-person camera image and 1 or 2 optional wrist camera image(s)\n    use_wrist_image = cfg.num_images_in_input > 1\n\n    # Create training and optional validation datasets\n    batch_transform = RLDSBatchTransform(\n        action_tokenizer,\n        processor.tokenizer,\n        image_transform=processor.image_processor.apply_transform,\n        prompt_builder_fn=PurePromptBuilder,\n        use_wrist_image=use_wrist_image,\n        use_proprio=cfg.use_proprio,\n        use_minivlm=cfg.use_minivlm\n        )\n    train_dataset = RLDSDataset(\n        cfg.data_root_dir,\n        cfg.dataset_name,\n        batch_transform,\n        resize_resolution=tuple(vla.module.config.image_sizes),\n        shuffle_buffer_size=cfg.shuffle_buffer_size,\n        image_aug=cfg.image_aug,\n    )\n    if cfg.use_val_set:\n        val_dataset = RLDSDataset(\n            cfg.data_root_dir,\n            cfg.dataset_name,\n            batch_transform,\n            resize_resolution=tuple(vla.module.config.image_sizes),\n            shuffle_buffer_size=cfg.shuffle_buffer_size // 10,\n            image_aug=cfg.image_aug,\n            train=False,\n        )\n\n    # [Important] Save dataset statistics so that we can unnormalize actions during inference\n    if distributed_state.is_main_process:\n        save_dataset_statistics(train_dataset.dataset_statistics, run_dir)\n\n    # Create collator and dataloader\n    collator = PaddedCollatorForActionPrediction(\n        processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n    )\n    dataloader = DataLoader(\n        train_dataset,\n        batch_size=cfg.batch_size,\n        sampler=None,\n        collate_fn=collator,\n        num_workers=0,  # Important: Set to 0 if using RLDS, which uses its own parallelism\n    )\n    print('Len of dataloader: ', len(dataloader))\n    if cfg.use_val_set:\n        val_batch_size = cfg.batch_size\n        val_dataloader = DataLoader(\n            val_dataset,\n            batch_size=val_batch_size,\n            sampler=None,\n            collate_fn=collator,\n            num_workers=0,  # Important: Set to 0 if using RLDS, which uses its own parallelism\n        )\n\n    # Deque to store recent train metrics (used for computing smoothened metrics for gradient accumulation)\n    recent_metrics = {\n        \"loss_value\": deque(maxlen=cfg.grad_accumulation_steps),\n        \"curr_action_accuracy\": deque(maxlen=cfg.grad_accumulation_steps),\n        \"curr_action_l1_loss\": deque(maxlen=cfg.grad_accumulation_steps),\n        \"next_actions_accuracy\": deque(maxlen=cfg.grad_accumulation_steps),\n        \"next_actions_l1_loss\": deque(maxlen=cfg.grad_accumulation_steps),\n    }\n\n    # Start training\n    with tqdm.tqdm(total=cfg.max_steps, leave=False) as progress:\n        vla.train()\n        optimizer.zero_grad()\n        for batch_idx, batch in enumerate(dataloader):\n            # Compute training metrics and loss\n            compute_diffusion_l1 = (cfg.use_l1_regression and batch_idx % cfg.diffusion_sample_freq == 0) or (cfg.use_diffusion and batch_idx % cfg.diffusion_sample_freq == 0)\n            loss, metrics = run_forward_pass(\n                vla=vla,\n                action_head=action_head,\n                proprio_projector=proprio_projector if cfg.use_proprio else None,\n                batch=batch,\n                action_tokenizer=action_tokenizer,\n                device_id=device_id,\n                use_l1_regression=cfg.use_l1_regression,\n                use_proprio=cfg.use_proprio,\n                use_film=cfg.use_film,\n                num_patches=NUM_PATCHES,\n                compute_diffusion_l1=compute_diffusion_l1,\n                use_pro_version=cfg.use_pro_version,\n                cfg=cfg,\n            )\n\n            # Normalize loss to account for gradient accumulation\n            normalized_loss = loss / cfg.grad_accumulation_steps\n\n            # Backward pass\n            normalized_loss.backward()\n\n            # Store recent train metrics\n            for metric_name, value in metrics.items():\n                if metric_name in recent_metrics:\n                    recent_metrics[metric_name].append(value)\n\n            # Compute gradient step index\n            gradient_step_idx = batch_idx // cfg.grad_accumulation_steps\n\n            # Compute smoothened train metrics\n            smoothened_metrics = compute_smoothened_metrics(recent_metrics)\n\n            # Push Metrics to W&B (every wandb_log_freq gradient steps)\n            log_step = gradient_step_idx if not cfg.resume else cfg.resume_step + gradient_step_idx\n            if distributed_state.is_main_process and log_step % cfg.wandb_log_freq == 0:\n                log_metrics_to_wandb(smoothened_metrics, \"VLA Train\", log_step, wandb)\n\n            # [If applicable] Linearly warm up learning rate from 10% to 100% of original\n            if cfg.lr_warmup_steps > 0:\n                lr_progress = min((gradient_step_idx + 1) / cfg.lr_warmup_steps, 1.0)  # Cap at 1.0\n                current_lr = original_lr * (0.1 + 0.9 * lr_progress)\n                for param_group in optimizer.param_groups:\n                    param_group[\"lr\"] = current_lr\n\n            if distributed_state.is_main_process and gradient_step_idx % cfg.wandb_log_freq == 0:\n                # Log the learning rate\n                # Make sure to do this AFTER any learning rate modifications (e.g., warmup/decay)\n                wandb.log(\n                    {\n                        \"VLA Train/Learning Rate\": scheduler.get_last_lr()[0],\n                    },\n                    step=log_step,\n                )\n\n            # Optimizer and LR scheduler step\n            if (batch_idx + 1) % cfg.grad_accumulation_steps == 0:\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                progress.update()\n\n            # Save model checkpoint: either keep latest checkpoint only or all checkpoints\n            if gradient_step_idx > 0 and log_step % cfg.save_freq == 0:\n                save_training_checkpoint(\n                    cfg=cfg,\n                    run_dir=run_dir,\n                    log_step=log_step,\n                    vla=vla,\n                    processor=processor,\n                    proprio_projector=proprio_projector if cfg.use_proprio else None,\n                    noisy_action_projector=None,\n                    action_head=action_head,\n                    train_dataset=train_dataset,\n                    distributed_state=distributed_state,\n                    new_state_dict=RAW_STATE_DICT,\n                )\n\n            # Test model on validation set\n            if cfg.use_val_set and log_step > 0 and log_step % cfg.val_freq == 0:\n                run_validation(\n                    vla=vla,\n                    action_head=action_head,\n                    noisy_action_projector=None,\n                    proprio_projector=proprio_projector if cfg.use_proprio else None,\n                    val_dataloader=val_dataloader,\n                    action_tokenizer=action_tokenizer,\n                    device_id=device_id,\n                    cfg=cfg,\n                    num_patches=NUM_PATCHES,\n                    log_step=log_step,\n                    distributed_state=distributed_state,\n                    val_time_limit=cfg.val_time_limit,\n                )\n                # Set model back to training mode after validation\n                vla.train()\n\n            # Stop training when max_steps is reached\n            if log_step == cfg.max_steps:\n                print(f\"Max step {cfg.max_steps} reached! Stopping training...\")\n                break\n\n\nif __name__ == \"__main__\":\n    finetune()\n"
        }
    ]
}