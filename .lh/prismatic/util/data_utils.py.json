{
    "sourceFile": "prismatic/util/data_utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1765369904604,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1765370061422,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -154,8 +154,11 @@\n         # Stack all actions\n         actions = [torch.from_numpy(np.copy(instance[\"actions\"])) for instance in instances]\n         actions = torch.stack(actions)\n \n+        discretized_action = [torch.from_numpy(np.copy(instance[\"discretized_action\"])) for instance in instances]\n+        discretized_action = torch.stack(discretized_action)\n+\n         # Stack proprio\n         if \"proprio\" in instances[0]:\n             proprio = [instance[\"proprio\"] for instance in instances]\n             proprio = torch.Tensor(np.squeeze(np.stack(proprio)))\n"
                },
                {
                    "date": 1765370072921,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -171,8 +171,9 @@\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             labels=labels,\n             actions=actions,\n+            discretized_action=discretized_action,\n         )\n         if dataset_names is not None:\n             output[\"dataset_names\"] = dataset_names\n         return output\n"
                }
            ],
            "date": 1765369904604,
            "name": "Commit-0",
            "content": "\"\"\"\ndata_utils.py\n\nGeneral utilities and classes for facilitating data loading and collation.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Callable, Dict, Sequence, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\n# HuggingFace Default / LLaMa-2 IGNORE_INDEX (for labels)\nIGNORE_INDEX = -100\n\n\ndef tree_map(fn: Callable, tree: dict) -> dict:\n    \"\"\"Maps a function over a nested dictionary.\"\"\"\n    return {k: tree_map(fn, v) if isinstance(v, dict) else fn(v) for k, v in tree.items()}\n\n\ndef tree_map_with_key(fn: Callable, tree: dict, keys: Sequence = ()) -> dict:\n    \"\"\"Maps a function over a nested dictionary.\"\"\"\n    return {\n        k: tree_map_with_key(fn, v, (*keys, k)) if isinstance(v, dict) else fn((*keys, k), v) for k, v in tree.items()\n    }\n\n\n@dataclass\nclass PaddedCollatorForLanguageModeling:\n    model_max_length: int\n    pad_token_id: int\n    default_image_resolution: Tuple[int, int, int]\n    padding_side: str = \"right\"\n    pixel_values_dtype: torch.dtype = torch.float32\n\n    def __post_init__(self) -> None:\n        self.dummy_pixel_values = torch.zeros(self.default_image_resolution, dtype=self.pixel_values_dtype)\n\n    def __call__(self, instances: Sequence[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n        pixel_values = [instance[\"pixel_values\"] for instance in instances]\n\n        # For now, we only support Tokenizers with `padding_side = \"right\"` during Training (but plan to extend!)\n        #   => Handle padding via RNN Utils => `pad_sequence`\n        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)\n        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n\n        # Truncate (if necessary)\n        input_ids, labels = input_ids[:, : self.model_max_length], labels[:, : self.model_max_length]\n\n        # Get `attention_mask` by checking for `pad_token_id`\n        attention_mask = input_ids.ne(self.pad_token_id)\n\n        # === Handle \"unimodal\" (language-only) vs. \"multimodal\" ===\n\n        # Some examples are \"language-only\" --> build a Tensor of `multimodal_indices` that we can slice into easily\n        multimodal_indices = torch.tensor(\n            [idx for idx in range(len(pixel_values)) if pixel_values[idx] is not None], dtype=torch.long\n        )\n\n        # Stack all `pixel_values` --> depending on type (torch.Tensor, or Dict[str, torch.Tensor]) & presence of None\n        if len(multimodal_indices) == 0:\n            pixel_values = torch.stack([self.dummy_pixel_values for _ in range(len(input_ids))])\n        elif isinstance(pv_example := pixel_values[multimodal_indices[0]], torch.Tensor):\n            pixel_values = torch.stack(\n                [\n                    pixel_values[idx] if idx in multimodal_indices else self.dummy_pixel_values\n                    for idx in range(len(input_ids))\n                ]\n            )\n        elif isinstance(pv_example, dict):\n            pixel_values = {\n                k: torch.stack(\n                    [\n                        pixel_values[idx][k] if idx in multimodal_indices else self.dummy_pixel_values\n                        for idx in range(len(input_ids))\n                    ]\n                )\n                for k in pv_example\n            }\n        else:\n            raise ValueError(f\"Unsupported `pixel_values` type = {type(pixel_values)}\")\n\n        return dict(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            multimodal_indices=multimodal_indices,\n        )\n\n\n@dataclass\nclass PaddedCollatorForActionPrediction:\n    model_max_length: int\n    pad_token_id: int\n    padding_side: str = \"right\"\n    pixel_values_dtype: torch.dtype = torch.float32\n\n    def __call__(self, instances: Sequence[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n        pixel_values = [instance[\"pixel_values\"] for instance in instances]\n        if \"dataset_name\" in instances[0]:\n            dataset_names = [instance[\"dataset_name\"] for instance in instances]\n        else:\n            dataset_names = None\n\n        # For now, we only support Tokenizers with `padding_side = \"right\"` during training\n        #   => Handle padding via RNN Utils => `pad_sequence`\n        assert self.padding_side == \"right\", f\"Invalid Tokenizer `{self.padding_side = }`\"\n        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)\n        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n\n\n        if self.padding_side == \"left\":\n            def left_pad_sequence(sequences, padding_value):\n                max_len = max(seq.size(0) for seq in sequences)\n                padded = []\n                for seq in sequences:\n                    pad_len = max_len - seq.size(0)\n                    pad = torch.full((pad_len,), padding_value, dtype=seq.dtype)\n                    padded_seq = torch.cat([pad, seq], dim=0)\n                    padded.append(padded_seq)\n                return torch.stack(padded)\n\n            input_ids = left_pad_sequence(input_ids, self.pad_token_id)\n            labels = left_pad_sequence(labels, IGNORE_INDEX)\n        else:\n            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_token_id)\n            labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n\n\n        # Truncate (if necessary)\n        input_ids, labels = input_ids[:, : self.model_max_length], labels[:, : self.model_max_length]\n\n        # Get `attention_mask` by checking for `pad_token_id`\n        attention_mask = input_ids.ne(self.pad_token_id)\n\n        # [Contract] For VLA Training =>> No \"Unimodal\" Data!\n        assert all([pv is not None for pv in pixel_values]), \"Invalid VLA Example with `pixel_values = None`!\"\n\n        # Stack all `pixel_values` --> depending on type is torch.Tensor or Dict[str, torch.Tensor]\n        if isinstance(pixel_values[0], torch.Tensor):\n            if \"pixel_values_wrist\" in instances[0]:\n                pixel_values_wrist = [instance[\"pixel_values_wrist\"] for instance in instances]\n                pixel_values = torch.cat((torch.stack(pixel_values), torch.stack(pixel_values_wrist)), dim=1)\n            else:\n                pixel_values = torch.stack(pixel_values)\n        else:\n            raise ValueError(f\"Unsupported `pixel_values` type = {type(pixel_values)}\")\n\n        # Stack all actions\n        actions = [torch.from_numpy(np.copy(instance[\"actions\"])) for instance in instances]\n        actions = torch.stack(actions)\n\n        # Stack proprio\n        if \"proprio\" in instances[0]:\n            proprio = [instance[\"proprio\"] for instance in instances]\n            proprio = torch.Tensor(np.squeeze(np.stack(proprio)))\n        else:\n            proprio = None\n\n        output = dict(\n            pixel_values=pixel_values,\n            proprio=proprio,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            actions=actions,\n        )\n        if dataset_names is not None:\n            output[\"dataset_names\"] = dataset_names\n        return output\n"
        }
    ]
}