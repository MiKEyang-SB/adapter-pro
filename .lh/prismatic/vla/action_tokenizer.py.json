{
    "sourceFile": "prismatic/vla/action_tokenizer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 7,
            "patches": [
                {
                    "date": 1765367438201,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1765367784380,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n         discretized_action = np.digitize(action, self.bins)\n \n         # import pdb; pdb.set_trace()\n         if use_minivlm:\n-            return (self.tokenizer_len - discretized_action).tolist()\n+            return (self.tokenizer_len - discretized_action).tolist(), discretized_action.tolist()\n \n         else:\n             # Handle single element vs. batch\n             if len(discretized_action.shape) <= 1:\n"
                },
                {
                    "date": 1765367892261,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n         discretized_action = np.digitize(action, self.bins)\n \n         # import pdb; pdb.set_trace()\n         if use_minivlm:\n-            return (self.tokenizer_len - discretized_action).tolist(), discretized_action.tolist()\n+            return (self.tokenizer_len - discretized_action).tolist()\n \n         else:\n             # Handle single element vs. batch\n             if len(discretized_action.shape) <= 1:\n"
                },
                {
                    "date": 1765367947725,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n         discretized_action = np.digitize(action, self.bins)\n \n         # import pdb; pdb.set_trace()\n         if use_minivlm:\n-            return (self.tokenizer_len - discretized_action).tolist()\n+            return (self.tokenizer_len - discretized_action).tolist(), discretized_action.tolist()\n \n         else:\n             # Handle single element vs. batch\n             if len(discretized_action.shape) <= 1:\n"
                },
                {
                    "date": 1765367991802,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n         discretized_action = np.digitize(action, self.bins)\n \n         # import pdb; pdb.set_trace()\n         if use_minivlm:\n-            return (self.tokenizer_len - discretized_action).tolist(), discretized_action.tolist()\n+            return (self.tokenizer_len - discretized_action).tolist()\n \n         else:\n             # Handle single element vs. batch\n             if len(discretized_action.shape) <= 1:\n"
                },
                {
                    "date": 1765369071683,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n         discretized_action = np.digitize(action, self.bins)\n \n         # import pdb; pdb.set_trace()\n         if use_minivlm:\n-            return (self.tokenizer_len - discretized_action).tolist()\n+            return (self.tokenizer_len - discretized_action).tolist(), discretized_action.tolist()\n \n         else:\n             # Handle single element vs. batch\n             if len(discretized_action.shape) <= 1:\n"
                },
                {
                    "date": 1765369120624,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n         discretized_action = np.digitize(action, self.bins)\n \n         # import pdb; pdb.set_trace()\n         if use_minivlm:\n-            return (self.tokenizer_len - discretized_action).tolist(), discretized_action.tolist()\n+            return (self.tokenizer_len - discretized_action).tolist()\n \n         else:\n             # Handle single element vs. batch\n             if len(discretized_action.shape) <= 1:\n"
                },
                {
                    "date": 1765371956988,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -59,9 +59,9 @@\n \n     def __call__(self, action: np.ndarray, use_minivlm) -> Union[str, List[str]]:\n         \"\"\"Clip & bin actions to *the last `n_bins` tokens* of the vocabulary (e.g., tokenizer.vocab[-256:]).\"\"\"\n         action = np.clip(action, a_min=float(self.min_action), a_max=float(self.max_action))\n-        discretized_action = np.digitize(action, self.bins)\n+        discretized_action = np.digitize(action, self.bins) #[1, 255]\n \n         # import pdb; pdb.set_trace()\n         if use_minivlm:\n             return (self.tokenizer_len - discretized_action).tolist()\n"
                }
            ],
            "date": 1765367438201,
            "name": "Commit-0",
            "content": "\"\"\"\naction_tokenizer.py\n\nExtension class; wraps base LLM/VLM tokenizer with logic to discretize and tokenize continuous robot actions.\n\"\"\"\n\nimport json\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import List, Union\n\nimport numpy as np\nimport torch\nfrom transformers import PreTrainedTokenizerBase\nfrom transformers.models.qwen2.tokenization_qwen2_fast import Qwen2TokenizerFast\n\nfrom prismatic.overwatch.overwatch import initialize_overwatch\n\noverwatch = initialize_overwatch(__name__)\n\n\nclass ActionTokenizer:\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizerBase,\n        bins: int = 256,\n        min_action: int = -1,\n        max_action: int = 1,\n        use_extra: bool = False,\n    ) -> None:\n        \"\"\"\n        Discretizes continuous robot actions into N bins per dimension and maps to the least used tokens.\n\n        NOTE =>> by default, assumes a BPE-style tokenizer akin to the LlamaTokenizer, where *the least used tokens*\n                 appear at the end of the vocabulary!\n\n        :param tokenizer: Base LLM/VLM tokenizer to extend.\n        :param bins: Number of bins for each continuous value; we'll adopt a uniform binning strategy.\n        :param min_action: Minimum action value (for clipping, setting lower bound on bin interval).\n        :param max_action: Maximum action value (for clipping, setting upper bound on bin interval).\n        :param use_extra: Use the extra tokens (not just the last ones), only implemented for Qwen2\n        \"\"\"\n        self.tokenizer, self.n_bins, self.min_action, self.max_action = tokenizer, bins, min_action, max_action\n\n        # Create Uniform Bins + Compute Bin Centers\n        self.bins = np.linspace(min_action, max_action, self.n_bins)\n        self.bin_centers = (self.bins[:-1] + self.bins[1:]) / 2.0\n\n        self.tokenizer_len = self.tokenizer.vocab_size\n        if isinstance(tokenizer, Qwen2TokenizerFast) and use_extra:\n            self.tokenizer_len = len(self.tokenizer)\n        elif use_extra:\n            raise NotImplementedError(\"Cannot use extra tokens for this tokenizer!\")\n\n        # [Contract] Set \"action_token_begin_idx\" based on `self.tokenizer.vocab_size - (self.n_bins + 1)`\n        #   =>> Assumes we're always overwriting the final `n_bins` tokens of the vocabulary!\n        self.action_token_begin_idx: int = int(self.tokenizer_len - (self.n_bins + 1))\n        self.action_token_end_idx: int = int(self.tokenizer_len)\n\n    def __call__(self, action: np.ndarray, use_minivlm) -> Union[str, List[str]]:\n        \"\"\"Clip & bin actions to *the last `n_bins` tokens* of the vocabulary (e.g., tokenizer.vocab[-256:]).\"\"\"\n        action = np.clip(action, a_min=float(self.min_action), a_max=float(self.max_action))\n        discretized_action = np.digitize(action, self.bins)\n\n        # import pdb; pdb.set_trace()\n        if use_minivlm:\n            return (self.tokenizer_len - discretized_action).tolist()\n\n        else:\n            # Handle single element vs. batch\n            if len(discretized_action.shape) <= 1:\n                return self.tokenizer.decode(list(self.tokenizer_len - discretized_action))\n            else:\n                return self.tokenizer.batch_decode((self.tokenizer_len - discretized_action).tolist())\n\n    def decode_token_ids_to_actions(self, action_token_ids: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Returns continuous actions for discrete action token IDs.\n\n        NOTE =>> Because of the way the actions are discretized w.r.t. the bins (and not the bin centers), the\n                 digitization returns bin indices between [1, # bins], inclusive, when there are actually only\n                 (# bins - 1) bin intervals.\n\n                 Therefore, if the digitization returns the last possible index, we map this to the last bin interval.\n\n        EXAMPLE =>> Let's say self._bins has 256 values. Then self._bin_centers has 255 values. Digitization returns\n                    indices between [1, 256]. We subtract 1 from all indices so that they are between [0, 255]. There\n                    is still one index (i==255) that would cause an out-of-bounds error if used to index into\n                    self._bin_centers. Therefore, if i==255, we subtract 1 from it so that it just becomes the index of\n                    the last bin center. We implement this simply via clipping between [0, 255 - 1].\n        \"\"\"\n        discretized_actions = self.tokenizer_len - action_token_ids\n        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.bin_centers.shape[0] - 1)\n\n        return self.bin_centers[discretized_actions]\n\n    @property\n    def vocab_size(self) -> int:\n        return self.n_bins\n\n    @property\n    def required_future_horizon(self) -> int:\n        # the number of future action horizon elements\n        return 0\n\n\nclass VQActionTokenizer(ActionTokenizer):\n    \"\"\"Loads a torch model (VqVaE) that turns\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizerBase,\n        vq_vae_path=\"\",\n        device=\"cpu\",\n        use_extra: bool = False,\n    ):\n        self.tokenizer = tokenizer\n        self.device = device\n\n        ### VQ VAE loading ###\n\n        # NOTE: if this errors, you need to install vqvae, source: https://github.com/jayLEE0301/vq_bet_official\n        from vqvae.vqvae import VqVae\n\n        self.vq_path = Path(vq_vae_path)\n        assert self.vq_path.exists(), f\"Missing VQ VAE path: {self.vq_path}\"\n        vq_model_path = self.vq_path / \"checkpoints\" / \"model.pt\"\n        vq_config_path = self.vq_path / \"config.json\"\n        assert vq_model_path.exists(), f\"Missing VQ checkpoint path: {vq_model_path}\"\n        assert vq_config_path.exists(), f\"Missing VQ config path: {vq_config_path}\"\n        with open(vq_config_path, \"r\") as f:\n            vq_config = dict(json.load(f))\n        # set the load checkpoint\n        vq_config[\"load_dir\"] = vq_model_path\n        vq_config[\"eval\"] = True\n        vq_config[\"device\"] = self.device\n        overwatch.info(f\"Loading VQ VAE for Action Tokenization from {vq_config_path}...\")\n        # instantiate the vqvae and load\n        self.vq_vae = VqVae(**vq_config)\n        overwatch.info(f\"Found VQ VAE parameters: \\n{self.vq_vae}\")\n        ### TOKENIZATION arguments ###\n        # number of bins to assign for each \"action\" dimension\n        self.n_bins = self.vq_vae.vqvae_n_embed\n\n        self.tokenizer_len = self.tokenizer.vocab_size\n        if isinstance(tokenizer, Qwen2TokenizerFast) and use_extra:\n            self.tokenizer_len = len(self.tokenizer)\n        elif use_extra:\n            raise NotImplementedError(\"Cannot use extra tokens for this tokenizer!\")\n\n        # [Contract] Set \"action_token_begin_idx\" based on `self.tokenizer.vocab_size - (self.n_bins + 1)`\n        #   =>> Assumes we're always overwriting the final `n_bins` tokens of the vocabulary!\n        self.action_token_begin_idx: int = int(self.tokenizer_len - (self.n_bins + 1))\n        self.action_token_end_idx: int = int(self.tokenizer_len)\n\n    def __call__(self, action: np.ndarray) -> Union[str, List[str]]:\n        # make sure shape matches (1 x T x A)\n        action = torch.from_numpy(action).to(self.device).reshape((1, self.vq_vae.input_dim_h, self.vq_vae.input_dim_w))\n        # action is (1 x T x A), codes will be (1 x GROUPS) each between 0 and BINS-1\n        _, vq_code = self.vq_vae.get_code(action)\n        assert torch.all(vq_code >= 0) and torch.all(vq_code < self.n_bins)\n\n        # vq_codes will be between [0, n_bins-1], so we subtract them from vocab_size - 1\n        # for example, code 0 maps to vocab_size - 1\n        return self.tokenizer.decode(list(self.tokenizer_len - 1 - vq_code[0].numpy()))\n\n    def decode_token_ids_to_actions(self, action_token_ids: np.ndarray) -> np.ndarray:\n        # first convert from tokens to bins (inverse of what happens in __call__)\n        action_token_ids = self.tokenizer_len - 1 - action_token_ids\n        initial_shape = action_token_ids.shape\n        # these directly correspond to the bins\n        action_token_ids = np.clip(action_token_ids, 0, self.n_bins - 1)\n        action_token_ids = torch.from_numpy(action_token_ids).to(self.device).reshape(-1, self.vq_vae.vqvae_groups)\n        assert torch.all(action_token_ids >= 0) and torch.all(action_token_ids < self.n_bins)\n        # (1 x G) --> (1 x Z_DIM)\n        latent = self.vq_vae.draw_code_forward(action_token_ids)\n        # --> (1 x A) --> (A,)\n        ret_action = self.vq_vae.get_action_from_latent(latent)\n\n        # reshape to be a flat array if the input was a single action\n        if action_token_ids.shape[0] == 1 and len(initial_shape) == 1:\n            return ret_action[0, 0]\n\n        # get the first horizon element of the returned actions (VQ might return an action horizon)\n        # TODO parameterize this\n        return ret_action[:, 0]\n\n    @property\n    def required_future_horizon(self) -> int:\n        # the number of future action horizon elements\n        return self.vq_vae.input_dim_h - 1\n\n\nACTION_TOKENIZERS = {\n    \"action_tokenizer\": ActionTokenizer,\n    \"extra_action_tokenizer\": partial(ActionTokenizer, use_extra=True),\n    # libero\n    \"libero_vq_action_tokenizer\": partial(\n        VQActionTokenizer, vq_vae_path=\"vq/pretrain_vq+mx-libero_90+fach-7+ng-7+nemb-128+nlatent-512\"\n    ),\n    \"libero_vq_extra_action_tokenizer\": partial(\n        VQActionTokenizer, vq_vae_path=\"vq/pretrain_vq+mx-libero_90+fach-7+ng-7+nemb-128+nlatent-512\", use_extra=True\n    ),\n    \"libero_vq_h0_extra_action_tokenizer\": partial(\n        VQActionTokenizer, vq_vae_path=\"vq/pretrain_vq+mx-libero_90+fach-0+ng-7+nemb-128+nlatent-512\", use_extra=True\n    ),\n    # bridge\n    \"bridge_vq_extra_action_tokenizer\": partial(\n        VQActionTokenizer,\n        vq_vae_path=\"vq/pretrain_modvq+mx-bridge_dataset+fach-7+ng-7+nemb-256+nlatent-512\",\n        use_extra=True,\n    ),\n}\n"
        }
    ]
}