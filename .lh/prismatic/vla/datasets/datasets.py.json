{
    "sourceFile": "prismatic/vla/datasets/datasets.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 15,
            "patches": [
                {
                    "date": 1765358093881,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1765368245730,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,9 +70,9 @@\n             for turn in conversation:\n                 prompt_builder.add_turn(turn[\"from\"], turn[\"value\"])\n \n             prompt = prompt_builder.get_prompt() #e.g. 'In: What action should the robot take to put both the cream cheese box and the butter in the basket?\\nOut: 希</s>'\n-            input_ids = self.base_tokenizer(prompt_builder.get_prompt(), add_special_tokens=True).input_ids\n+            input_ids = self.base_tokenizer(prompt_builder.get_prompt(), add_special_tokens=True).input_ids #(55,)\n \n             if len(input_ids) >= 3:\n                 del input_ids[-3] \n                 del input_ids[-2] \n"
                },
                {
                    "date": 1765368272955,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -76,9 +76,9 @@\n             if len(input_ids) >= 3:\n                 del input_ids[-3] \n                 del input_ids[-2] \n                 del input_ids[-1] \n-\n+            #len(input_ids) = 52\n             if NUM_TOKENS<len(flattened_action_chunk_string):\n                 input_ids = input_ids + flattened_action_chunk_string[:NUM_TOKENS]\n             else:\n                 remaining_length = NUM_TOKENS - len(flattened_action_chunk_string)\n"
                },
                {
                    "date": 1765368336486,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -84,9 +84,9 @@\n                 remaining_length = NUM_TOKENS - len(flattened_action_chunk_string)\n                 extended_array = random.choices(flattened_action_chunk_string, k=remaining_length)\n                 \n                 input_ids = input_ids + flattened_action_chunk_string + extended_array\n-            labels = list(input_ids)\n+            labels = list(input_ids) #(116)\n             action_chunk_len = NUM_TOKENS\n \n         else:\n             future_actions_string = ''.join(self.action_tokenizer(future_actions, use_minivlm=False))\n"
                },
                {
                    "date": 1765368386517,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,9 +117,9 @@\n \n         # Tensorize =>> Run Image Transform to get `pixel_values` =>> Return\n         #   =>> IMPORTANT :: IF WE'RE USING HF LLM.forward(..., labels=labels), SHIFTING HAPPENS _INSIDE_ MODEL!\n         input_ids, labels = torch.tensor(input_ids), torch.tensor(labels)\n-        pixel_values = self.image_transform(img)\n+        pixel_values = self.image_transform(img) #(6, 224, 224)\n \n         # [CRITICAL] We do not want to take the loss for anything but the predicted action tokens!\n         labels[: -(action_chunk_len + 1)] = IGNORE_INDEX\n         if not self.predict_stop_token:\n"
                },
                {
                    "date": 1765369256787,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -58,8 +58,12 @@\n             future_actions_string = self.action_tokenizer(future_actions,self.use_minivlm)\n             current_action_string = self.action_tokenizer(current_action,self.use_minivlm)\n \n             action_chunk_string = [current_action_string] + future_actions_string\n+\n+            #ADD TO MASKVLA\n+            tokenizer_len = self.action_tokenizer.tokenizer_len\n+            discretized_action = tokenizer_len - action_chunk_string\n             flattened_action_chunk_string = [item for sublist in action_chunk_string for item in sublist]\n             action_chunk_len = len(flattened_action_chunk_string) \n \n             conversation = [\n"
                },
                {
                    "date": 1765369264067,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -62,8 +62,9 @@\n \n             #ADD TO MASKVLA\n             tokenizer_len = self.action_tokenizer.tokenizer_len\n             discretized_action = tokenizer_len - action_chunk_string\n+            \n             flattened_action_chunk_string = [item for sublist in action_chunk_string for item in sublist]\n             action_chunk_len = len(flattened_action_chunk_string) \n \n             conversation = [\n"
                },
                {
                    "date": 1765369537915,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,10 +61,13 @@\n             action_chunk_string = [current_action_string] + future_actions_string\n \n             #ADD TO MASKVLA\n             tokenizer_len = self.action_tokenizer.tokenizer_len\n-            discretized_action = tokenizer_len - action_chunk_string\n-            \n+            # discretized_action = tokenizer_len - action_chunk_string\n+            # discretized_action = [[tokenizer_len - x for x in row] for row in action_chunk_string]\n+            arr = np.array(action_chunk_string)\n+            discretized_action = tokenizer_len - arr\n+ \n             flattened_action_chunk_string = [item for sublist in action_chunk_string for item in sublist]\n             action_chunk_len = len(flattened_action_chunk_string) \n \n             conversation = [\n"
                },
                {
                    "date": 1765369548431,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,9 +64,9 @@\n             tokenizer_len = self.action_tokenizer.tokenizer_len\n             # discretized_action = tokenizer_len - action_chunk_string\n             # discretized_action = [[tokenizer_len - x for x in row] for row in action_chunk_string]\n             arr = np.array(action_chunk_string)\n-            discretized_action = tokenizer_len - arr\n+            discretized_action = tokenizer_len - arr #array\n  \n             flattened_action_chunk_string = [item for sublist in action_chunk_string for item in sublist]\n             action_chunk_len = len(flattened_action_chunk_string) \n \n"
                },
                {
                    "date": 1765369586525,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -146,9 +146,9 @@\n             return_dict[\"pixel_values_wrist\"] = torch.cat(all_wrist_pixels, dim=0)\n         if self.use_proprio and \"proprio\" in rlds_batch[\"observation\"]:\n             proprio = rlds_batch[\"observation\"][\"proprio\"]\n             return_dict[\"proprio\"] = proprio\n-\n+        return_dict[\"discretized_action\"] = discretized_action\n         return return_dict\n     \n     \n \n"
                },
                {
                    "date": 1768656239242,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,8 +28,9 @@\n \n @dataclass\n class RLDSBatchTransform:\n     action_tokenizer: ActionTokenizer\n+    ddaction_tokenizer: DDActionTokenizer\n     base_tokenizer: PreTrainedTokenizerBase\n     image_transform: ImageTransform\n     prompt_builder_fn: Type[PromptBuilder]\n     predict_stop_token: bool = True\n@@ -59,14 +60,14 @@\n             current_action_string = self.action_tokenizer(current_action,self.use_minivlm)\n \n             action_chunk_string = [current_action_string] + future_actions_string\n \n-            #ADD TO MASKVLA\n-            tokenizer_len = self.action_tokenizer.tokenizer_len\n-            # discretized_action = tokenizer_len - action_chunk_string\n-            # discretized_action = [[tokenizer_len - x for x in row] for row in action_chunk_string]\n-            arr = np.array(action_chunk_string)\n-            discretized_action = tokenizer_len - arr #array\n+            # #ADD TO MASKVLA\n+            # tokenizer_len = self.action_tokenizer.tokenizer_len\n+            # # discretized_action = tokenizer_len - action_chunk_string\n+            # # discretized_action = [[tokenizer_len - x for x in row] for row in action_chunk_string]\n+            # arr = np.array(action_chunk_string)\n+            # discretized_action = tokenizer_len - arr #array\n  \n             flattened_action_chunk_string = [item for sublist in action_chunk_string for item in sublist]\n             action_chunk_len = len(flattened_action_chunk_string) \n \n"
                },
                {
                    "date": 1768656327338,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,9 +18,9 @@\n \n from prismatic.models.backbones.llm.prompting import PromptBuilder, QwenPromptBuilder\n from prismatic.models.backbones.vision import ImageTransform\n from prismatic.util.data_utils import tree_map\n-from prismatic.vla.action_tokenizer import ActionTokenizer\n+from prismatic.vla.action_tokenizer import ActionTokenizer, DDActionTokenizer\n from prismatic.vla.constants import ACTION_DIM, ACTION_PROPRIO_NORMALIZATION_TYPE, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX, NUM_TOKENS\n from prismatic.vla.datasets.rlds import make_interleaved_dataset, make_single_dataset\n from prismatic.vla.datasets.rlds.oxe import OXE_NAMED_MIXTURES, get_oxe_dataset_kwargs_and_weights\n \n"
                },
                {
                    "date": 1768657029668,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,9 +66,10 @@\n             # # discretized_action = tokenizer_len - action_chunk_string\n             # # discretized_action = [[tokenizer_len - x for x in row] for row in action_chunk_string]\n             # arr = np.array(action_chunk_string)\n             # discretized_action = tokenizer_len - arr #array\n- \n+            discretized_action = ddaction_tokenizer.encode(actions)\n+\n             flattened_action_chunk_string = [item for sublist in action_chunk_string for item in sublist]\n             action_chunk_len = len(flattened_action_chunk_string) \n \n             conversation = [\n"
                },
                {
                    "date": 1768657044015,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,9 +66,9 @@\n             # # discretized_action = tokenizer_len - action_chunk_string\n             # # discretized_action = [[tokenizer_len - x for x in row] for row in action_chunk_string]\n             # arr = np.array(action_chunk_string)\n             # discretized_action = tokenizer_len - arr #array\n-            discretized_action = ddaction_tokenizer.encode(actions)\n+            discretized_action = self.ddaction_tokenizer.encode(actions)\n \n             flattened_action_chunk_string = [item for sublist in action_chunk_string for item in sublist]\n             action_chunk_len = len(flattened_action_chunk_string) \n \n"
                },
                {
                    "date": 1768657206468,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,9 +66,9 @@\n             # # discretized_action = tokenizer_len - action_chunk_string\n             # # discretized_action = [[tokenizer_len - x for x in row] for row in action_chunk_string]\n             # arr = np.array(action_chunk_string)\n             # discretized_action = tokenizer_len - arr #array\n-            discretized_action = self.ddaction_tokenizer.encode(actions)\n+            \n \n             flattened_action_chunk_string = [item for sublist in action_chunk_string for item in sublist]\n             action_chunk_len = len(flattened_action_chunk_string) \n \n@@ -126,8 +126,9 @@\n             labels = list(input_ids)\n \n         # Tensorize =>> Run Image Transform to get `pixel_values` =>> Return\n         #   =>> IMPORTANT :: IF WE'RE USING HF LLM.forward(..., labels=labels), SHIFTING HAPPENS _INSIDE_ MODEL!\n+        discretized_action = self.ddaction_tokenizer.encode(actions)\n         input_ids, labels = torch.tensor(input_ids), torch.tensor(labels)\n         pixel_values = self.image_transform(img) #(6, 224, 224)\n \n         # [CRITICAL] We do not want to take the loss for anything but the predicted action tokens!\n"
                },
                {
                    "date": 1768657253504,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -47,9 +47,9 @@\n         actions = rlds_batch[\"action\"]\n \n         # Construct Chat-based Prompt =>> Input is default query + language instruction, output are the action tokens\n         prompt_builder = self.prompt_builder_fn(\"openvla\")\n-\n+        discretized_action = self.ddaction_tokenizer.encode(actions)\n         # Get future action chunk\n         future_actions = rlds_batch[\"action\"][1:]\n \n         if self.use_minivlm:\n@@ -126,9 +126,9 @@\n             labels = list(input_ids)\n \n         # Tensorize =>> Run Image Transform to get `pixel_values` =>> Return\n         #   =>> IMPORTANT :: IF WE'RE USING HF LLM.forward(..., labels=labels), SHIFTING HAPPENS _INSIDE_ MODEL!\n-        discretized_action = self.ddaction_tokenizer.encode(actions)\n+       \n         input_ids, labels = torch.tensor(input_ids), torch.tensor(labels)\n         pixel_values = self.image_transform(img) #(6, 224, 224)\n \n         # [CRITICAL] We do not want to take the loss for anything but the predicted action tokens!\n"
                }
            ],
            "date": 1765358093881,
            "name": "Commit-0",
            "content": "\"\"\"\ndatasets.py\n\nLightweight PyTorch Dataset Definition for wrapping RLDS TFDS Pipeline; just defines transform from RLDS default\nformat to OpenVLA, IterableDataset shim.\n\"\"\"\n\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple, Type\nimport numpy as np\nimport random\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset, IterableDataset\nfrom transformers import PreTrainedTokenizerBase\n\nfrom prismatic.models.backbones.llm.prompting import PromptBuilder, QwenPromptBuilder\nfrom prismatic.models.backbones.vision import ImageTransform\nfrom prismatic.util.data_utils import tree_map\nfrom prismatic.vla.action_tokenizer import ActionTokenizer\nfrom prismatic.vla.constants import ACTION_DIM, ACTION_PROPRIO_NORMALIZATION_TYPE, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX, NUM_TOKENS\nfrom prismatic.vla.datasets.rlds import make_interleaved_dataset, make_single_dataset\nfrom prismatic.vla.datasets.rlds.oxe import OXE_NAMED_MIXTURES, get_oxe_dataset_kwargs_and_weights\n\n\n\n@dataclass\nclass RLDSBatchTransform:\n    action_tokenizer: ActionTokenizer\n    base_tokenizer: PreTrainedTokenizerBase\n    image_transform: ImageTransform\n    prompt_builder_fn: Type[PromptBuilder]\n    predict_stop_token: bool = True\n    use_wrist_image: bool = False\n    use_proprio: bool = False\n    use_minivlm: bool = False\n\n\n    def __call__(self, rlds_batch: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Converts a RLDS batch to the format expected by the OpenVLA collator/models.\"\"\"\n        dataset_name, current_action = rlds_batch[\"dataset_name\"], rlds_batch[\"action\"][0]\n        img = Image.fromarray(rlds_batch[\"observation\"][\"image_primary\"][0])\n        lang = rlds_batch[\"task\"][\"language_instruction\"].decode().lower()\n        actions = rlds_batch[\"action\"]\n\n        # Construct Chat-based Prompt =>> Input is default query + language instruction, output are the action tokens\n        prompt_builder = self.prompt_builder_fn(\"openvla\")\n\n        # Get future action chunk\n        future_actions = rlds_batch[\"action\"][1:]\n\n        if self.use_minivlm:\n            self.prompt_builder_fn = QwenPromptBuilder\n            prompt_builder = self.prompt_builder_fn(\"openvla\")\n            # Get action chunk string\n            future_actions_string = self.action_tokenizer(future_actions,self.use_minivlm)\n            current_action_string = self.action_tokenizer(current_action,self.use_minivlm)\n\n            action_chunk_string = [current_action_string] + future_actions_string\n            flattened_action_chunk_string = [item for sublist in action_chunk_string for item in sublist]\n            action_chunk_len = len(flattened_action_chunk_string) \n\n            conversation = [\n                {\"from\": \"human\", \"value\": f\"What action should the robot take to {lang}?\"},\n                {\"from\": \"gpt\", \"value\": ''},\n            ]\n\n            for turn in conversation:\n                prompt_builder.add_turn(turn[\"from\"], turn[\"value\"])\n\n            prompt = prompt_builder.get_prompt() #e.g. 'In: What action should the robot take to put both the cream cheese box and the butter in the basket?\\nOut: 希</s>'\n            input_ids = self.base_tokenizer(prompt_builder.get_prompt(), add_special_tokens=True).input_ids\n\n            if len(input_ids) >= 3:\n                del input_ids[-3] \n                del input_ids[-2] \n                del input_ids[-1] \n\n            if NUM_TOKENS<len(flattened_action_chunk_string):\n                input_ids = input_ids + flattened_action_chunk_string[:NUM_TOKENS]\n            else:\n                remaining_length = NUM_TOKENS - len(flattened_action_chunk_string)\n                extended_array = random.choices(flattened_action_chunk_string, k=remaining_length)\n                \n                input_ids = input_ids + flattened_action_chunk_string + extended_array\n            labels = list(input_ids)\n            action_chunk_len = NUM_TOKENS\n\n        else:\n            future_actions_string = ''.join(self.action_tokenizer(future_actions, use_minivlm=False))\n\n            # Get action chunk string\n            current_action_string = self.action_tokenizer(current_action, use_minivlm=False)\n            action_chunk_string = current_action_string + future_actions_string\n            action_chunk_len = len(action_chunk_string)\n\n            conversation = [\n                {\"from\": \"human\", \"value\": f\"What action should the robot take to {lang}?\"},\n                {\"from\": \"gpt\", \"value\": action_chunk_string[0]},\n            ]\n            # remove action token\n            # conversation = [\n            #     {\"from\": \"human\", \"value\": f\"What action should the robot take to {lang}?\"},\n            #     {\"from\": \"gpt\", \"value\": \"\"},\n            # ]\n            action_chunk_len = 1\n\n\n            for turn in conversation:\n                prompt_builder.add_turn(turn[\"from\"], turn[\"value\"])\n            prompt = prompt_builder.get_prompt() #e.g. 'In: What action should the robot take to put both the cream cheese box and the butter in the basket?\\nOut: 希</s>'\n            # Tokenize (w/ `base_tokenizer`)\n            input_ids = self.base_tokenizer(prompt, add_special_tokens=True).input_ids\n            labels = list(input_ids)\n\n        # Tensorize =>> Run Image Transform to get `pixel_values` =>> Return\n        #   =>> IMPORTANT :: IF WE'RE USING HF LLM.forward(..., labels=labels), SHIFTING HAPPENS _INSIDE_ MODEL!\n        input_ids, labels = torch.tensor(input_ids), torch.tensor(labels)\n        pixel_values = self.image_transform(img)\n\n        # [CRITICAL] We do not want to take the loss for anything but the predicted action tokens!\n        labels[: -(action_chunk_len + 1)] = IGNORE_INDEX\n        if not self.predict_stop_token:\n            labels[-1] = IGNORE_INDEX\n\n        return_dict = dict(pixel_values=pixel_values, input_ids=input_ids, labels=labels, dataset_name=dataset_name, actions=actions)\n\n        # Add additional inputs\n        if self.use_wrist_image:\n            all_wrist_pixels = []\n            for k in rlds_batch[\"observation\"].keys():\n                if \"wrist\" in k:\n                    img_wrist = Image.fromarray(rlds_batch[\"observation\"][k][0])\n                    pixel_values_wrist = self.image_transform(img_wrist)\n                    all_wrist_pixels.append(pixel_values_wrist)\n            return_dict[\"pixel_values_wrist\"] = torch.cat(all_wrist_pixels, dim=0)\n        if self.use_proprio and \"proprio\" in rlds_batch[\"observation\"]:\n            proprio = rlds_batch[\"observation\"][\"proprio\"]\n            return_dict[\"proprio\"] = proprio\n\n        return return_dict\n    \n    \n\nclass RLDSDataset(IterableDataset):\n    def __init__(\n        self,\n        data_root_dir: Path,\n        data_mix: str,\n        batch_transform: RLDSBatchTransform,\n        resize_resolution: Tuple[int, int],\n        shuffle_buffer_size: int = 256_000,\n        train: bool = True,\n        image_aug: bool = False,\n    ) -> None:\n        \"\"\"Lightweight wrapper around RLDS TFDS Pipeline for use with PyTorch/OpenVLA Data Loaders.\"\"\"\n        self.data_root_dir, self.data_mix, self.batch_transform = data_root_dir, data_mix, batch_transform\n\n        # Configure RLDS Dataset(s)\n        if self.data_mix in OXE_NAMED_MIXTURES:\n            mixture_spec = OXE_NAMED_MIXTURES[self.data_mix]\n        else:\n            # Assume that passed \"mixture\" name is actually a single dataset -- create single-dataset \"mix\"\n            mixture_spec = [(self.data_mix, 1.0)]\n\n        # fmt: off\n        if \"aloha\" in self.data_mix:\n            load_camera_views = (\"primary\", \"left_wrist\", \"right_wrist\")\n        else:\n            load_camera_views = (\"primary\", \"wrist\")\n\n        per_dataset_kwargs, weights = get_oxe_dataset_kwargs_and_weights(\n            self.data_root_dir,\n            mixture_spec,\n            load_camera_views=load_camera_views,\n            load_depth=False,\n            load_proprio=True,\n            load_language=True,\n            action_proprio_normalization_type=ACTION_PROPRIO_NORMALIZATION_TYPE,\n        )\n        rlds_config = dict(\n            traj_transform_kwargs=dict(\n                window_size=1,                                      # If we wanted to feed / predict more than one step\n                future_action_window_size=NUM_ACTIONS_CHUNK-1,      # For action chunking\n                skip_unlabeled=True,                                # Skip trajectories without language labels\n                goal_relabeling_strategy=\"uniform\",                 # Goals are currently unused\n            ),\n            frame_transform_kwargs=dict(\n                resize_size=resize_resolution,\n                num_parallel_calls=16,                          # For CPU-intensive ops (decoding, resizing, etc.)\n            ),\n            dataset_kwargs_list=per_dataset_kwargs,\n            shuffle_buffer_size=shuffle_buffer_size,\n            sample_weights=weights,\n            balance_weights=True,\n            traj_transform_threads=len(mixture_spec),\n            traj_read_threads=len(mixture_spec),\n            train=train,\n        )\n\n        # If applicable, enable image augmentations\n        if image_aug:\n            rlds_config[\"frame_transform_kwargs\"].update({\"image_augment_kwargs\" : dict(\n                random_resized_crop=dict(scale=[0.9, 0.9], ratio=[1.0, 1.0]),\n                random_brightness=[0.2],\n                random_contrast=[0.8, 1.2],\n                random_saturation=[0.8, 1.2],\n                random_hue=[0.05],\n                augment_order=[\n                    \"random_resized_crop\",\n                    \"random_brightness\",\n                    \"random_contrast\",\n                    \"random_saturation\",\n                    \"random_hue\",\n                ],\n            )}),\n        # fmt: on\n\n        # Initialize RLDS Dataset\n        self.dataset, self.dataset_length, self.dataset_statistics = self.make_dataset(rlds_config)\n\n    def make_dataset(self, rlds_config):\n        return make_interleaved_dataset(**rlds_config)\n\n    def __iter__(self) -> Dict[str, Any]:\n        for rlds_batch in self.dataset.as_numpy_iterator():\n            yield self.batch_transform(rlds_batch)\n\n    def __len__(self) -> int:\n        return self.dataset_length\n\n    # === Explicitly Unused ===\n    def __getitem__(self, idx: int) -> None:\n        raise NotImplementedError(\"IterableDataset does not implement map-style __getitem__; see __iter__ instead!\")\n\n\nclass EpisodicRLDSDataset(RLDSDataset):\n    \"\"\"Returns full episodes as list of steps instead of individual transitions (useful for visualizations).\"\"\"\n\n    def make_dataset(self, rlds_config):\n        per_dataset_kwargs = rlds_config[\"dataset_kwargs_list\"]\n        assert len(per_dataset_kwargs) == 1, \"Only support single-dataset `mixes` for episodic datasets.\"\n\n        return make_single_dataset(\n            per_dataset_kwargs[0],\n            train=rlds_config[\"train\"],\n            traj_transform_kwargs=rlds_config[\"traj_transform_kwargs\"],\n            frame_transform_kwargs=rlds_config[\"frame_transform_kwargs\"],\n        )\n\n    def __iter__(self) -> Dict[str, Any]:\n        for rlds_batch in self.dataset.as_numpy_iterator():\n            out = [\n                self.batch_transform(tree_map(lambda x: x[i], rlds_batch))  # noqa: B023\n                for i in range(rlds_batch[\"action\"].shape[0])\n            ]\n            yield out\n\n\nclass DummyDataset(Dataset):\n    def __init__(\n        self,\n        action_tokenizer: ActionTokenizer,\n        base_tokenizer: PreTrainedTokenizerBase,\n        image_transform: ImageTransform,\n        prompt_builder_fn: Type[PromptBuilder],\n    ) -> None:\n        self.action_tokenizer = action_tokenizer\n        self.base_tokenizer = base_tokenizer\n        self.image_transform = image_transform\n        self.prompt_builder_fn = prompt_builder_fn\n\n        # Note =>> We expect the dataset to store statistics for action de-normalization. Specifically, we store the\n        # per-dimension 1st and 99th action quantile. The values below correspond to \"no normalization\" for simplicity.\n        self.dataset_statistics = {\n            \"dummy_dataset\": {\n                \"action\": {\"q01\": np.zeros((7,), dtype=np.float32), \"q99\": np.ones((7,), dtype=np.float32)}\n            }\n        }\n\n    def __len__(self):\n        # TODO =>> Replace with number of elements in your dataset!\n        return 10000\n\n    def __getitem__(self, idx):\n        # TODO =>> Load image, action and instruction from disk -- we use dummy values\n        image = Image.fromarray(np.asarray(np.random.rand(224, 224, 3) * 255.0, dtype=np.uint8))\n        action = np.asarray(np.random.rand(7), dtype=np.float32)\n        instruction = \"do something spectacular\"\n\n        # Add instruction to VLA prompt\n        prompt_builder = self.prompt_builder_fn(\"openvla\")\n        conversation = [\n            {\"from\": \"human\", \"value\": f\"What action should the robot take to {instruction}?\"},\n            {\"from\": \"gpt\", \"value\": self.action_tokenizer(action)},\n        ]\n        for turn in conversation:\n            prompt_builder.add_turn(turn[\"from\"], turn[\"value\"])\n\n        # Tokenize (w/ `base_tokenizer`)\n        input_ids = self.base_tokenizer(prompt_builder.get_prompt(), add_special_tokens=True).input_ids\n        labels = list(input_ids)\n\n        # Tensorize =>> Run Image Transform to get `pixel_values` =>> Return\n        #   =>> IMPORTANT :: IF WE'RE USING HF .forward(..., labels=labels), SHIFTING HAPPENS _INSIDE_ MODEL!\n        input_ids, labels = torch.tensor(input_ids), torch.tensor(labels)\n        pixel_values = self.image_transform(image)\n\n        # [CRITICAL] We do not want to take the loss for anything but the predicted action tokens!\n        labels[: -(len(action) + 1)] = IGNORE_INDEX\n\n        return dict(pixel_values=pixel_values, input_ids=input_ids, labels=labels)\n"
        }
    ]
}