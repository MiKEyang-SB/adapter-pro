{
    "sourceFile": "prismatic/models/action_heads.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 113,
            "patches": [
                {
                    "date": 1765107839596,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1765108455526,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,9 +22,9 @@\n     \"\"\"Simple MLP-based action head that generates continuous actions via L1 regression.\"\"\"\n     def __init__(\n         self,\n         input_dim=4096, #vla.module.llm_dim\n-        hidden_dim=4096,\n+        hidden_dim=4096, #vla.module.llm_dim\n         action_dim=7,\n         num_task_tokens=512,\n         use_pro_version=False,\n     ):\n@@ -59,10 +59,11 @@\n \n         cond_actions_hidden_states = torch.zeros(\n             (batch_size, self.action_dim * NUM_ACTIONS_CHUNK, self.hidden_dim),\n             device=device, dtype=actions_hidden_states.dtype\n-        ).detach()  \n+        ).detach()  #(B, action_dim * NUM_ACTIONS_CHUNK, hidden_dim)\n \n+\n         rearranged_actions_hidden_states = cond_actions_hidden_states.reshape(\n             batch_size, NUM_ACTIONS_CHUNK, -1\n         )  # (batch, chunk_len, action_dim * hidden_dim)\n \n"
                },
                {
                    "date": 1765108633880,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,9 +61,8 @@\n             (batch_size, self.action_dim * NUM_ACTIONS_CHUNK, self.hidden_dim),\n             device=device, dtype=actions_hidden_states.dtype\n         ).detach()  #(B, action_dim * NUM_ACTIONS_CHUNK, hidden_dim)\n \n-\n         rearranged_actions_hidden_states = cond_actions_hidden_states.reshape(\n             batch_size, NUM_ACTIONS_CHUNK, -1\n         )  # (batch, chunk_len, action_dim * hidden_dim)\n \n@@ -73,11 +72,11 @@\n             rearranged_actions_hidden_states = (rearranged_actions_hidden_states + random_perturbations) # (1, seq_len, dim)\n \n         action = self.model(\n             rearranged_actions_hidden_states,\n-            h_a=actions_hidden_states,\n+            h_a=actions_hidden_states, #transformer中和的动作hidden_state\n             p=proprio_features,\n-            h_t=task_hidden_states\n+            h_t=task_hidden_states #视觉语言\n             )\n \n         return action\n     \n"
                },
                {
                    "date": 1765109691288,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -388,9 +388,9 @@\n         # attention scores\n         attn_scores = [torch.matmul(q_1, k_tokens.transpose(-2, -1))]\n         attn_scores.append(torch.matmul(q_1, k_adapter.transpose(-2, -1)))\n         attn_scores.append(torch.matmul(q_1, k_task.transpose(-2, -1)) * ratio_g)\n-        attn_scores = torch.cat(attn_scores, dim=-1) / math.sqrt(self.head_dim)\n+        attn_scores = torch.cat(attn_scores, dim=-1) / math.sqrt(self.head_dim)#(B, H, T, T + K_a + K_t)\n         attn_weights = torch.softmax(attn_scores, dim=-1)\n \n         # combine V\n         v_list = [v_tokens,v_adapter,v_task]\n"
                },
                {
                    "date": 1765272019599,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,9 +41,9 @@\n             )\n \n     def predict_action(\n             self, \n-            actions_hidden_states, \n+            actions_hidden_states, #(b, 25, 576, 896)\n             proprio=None, \n             proprio_projector=None,\n             phase=\"Inference\"\n             ):\n"
                },
                {
                    "date": 1765272029733,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,9 +42,9 @@\n \n     def predict_action(\n             self, \n             actions_hidden_states, #(b, 25, 576, 896)\n-            proprio=None, \n+            proprio=None, #(b, 8)\n             proprio_projector=None,\n             phase=\"Inference\"\n             ):\n         batch_size = actions_hidden_states.shape[0]\n"
                },
                {
                    "date": 1765272266773,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,10 +53,10 @@\n         proprio = proprio.reshape(batch_size, -1).to(torch.bfloat16)  # (bsz, proprio_dim)\n         proprio_features = proprio_projector(proprio)  # (bsz, llm_dim)\n         proprio_features = proprio_features.unsqueeze(dim=1)  # (bsz, 1, llm_dim)\n \n-        task_hidden_states = actions_hidden_states[:, :, :self.num_task_tokens, :]\n-        actions_hidden_states = actions_hidden_states[:, :, self.num_task_tokens:, :]\n+        task_hidden_states = actions_hidden_states[:, :, :self.num_task_tokens, :] #(b, 25, 512, 896)\n+        actions_hidden_states = actions_hidden_states[:, :, self.num_task_tokens:, :] #(b, 25, 64, 896)\n \n         cond_actions_hidden_states = torch.zeros(\n             (batch_size, self.action_dim * NUM_ACTIONS_CHUNK, self.hidden_dim),\n             device=device, dtype=actions_hidden_states.dtype\n"
                },
                {
                    "date": 1765272617819,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,9 +71,9 @@\n             random_perturbations = learnable_random_perturbations(seq_len, dim, device=rearranged_actions_hidden_states.device, dtype=rearranged_actions_hidden_states.dtype) \n             rearranged_actions_hidden_states = (rearranged_actions_hidden_states + random_perturbations) # (1, seq_len, dim)\n \n         action = self.model(\n-            rearranged_actions_hidden_states,\n+            rearranged_actions_hidden_states, # (batch, chunk_len, action_dim * hidden_dim)\n             h_a=actions_hidden_states, #transformer中和的动作hidden_state\n             p=proprio_features,\n             h_t=task_hidden_states #视觉语言\n             )\n"
                },
                {
                    "date": 1765272657584,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,9 +72,9 @@\n             rearranged_actions_hidden_states = (rearranged_actions_hidden_states + random_perturbations) # (1, seq_len, dim)\n \n         action = self.model(\n             rearranged_actions_hidden_states, # (batch, chunk_len, action_dim * hidden_dim)\n-            h_a=actions_hidden_states, #transformer中和的动作hidden_state\n+            h_a=actions_hidden_states, #transformer中和的动作hidden_state #(b, 25, 64, 896)\n             p=proprio_features,\n             h_t=task_hidden_states #视觉语言\n             )\n \n"
                },
                {
                    "date": 1765272664292,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,9 +73,9 @@\n \n         action = self.model(\n             rearranged_actions_hidden_states, # (batch, chunk_len, action_dim * hidden_dim)\n             h_a=actions_hidden_states, #transformer中和的动作hidden_state #(b, 25, 64, 896)\n-            p=proprio_features,\n+            p=proprio_features,  # (bsz, 1, llm_dim)\n             h_t=task_hidden_states #视觉语言\n             )\n \n         return action\n"
                },
                {
                    "date": 1765272716496,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,12 +71,12 @@\n             random_perturbations = learnable_random_perturbations(seq_len, dim, device=rearranged_actions_hidden_states.device, dtype=rearranged_actions_hidden_states.dtype) \n             rearranged_actions_hidden_states = (rearranged_actions_hidden_states + random_perturbations) # (1, seq_len, dim)\n \n         action = self.model(\n-            rearranged_actions_hidden_states, # (batch, chunk_len, action_dim * hidden_dim)\n+            rearranged_actions_hidden_states, # (batch, chunk_len, action_dim * hidden_dim) (b, 7, 8*896)\n             h_a=actions_hidden_states, #transformer中和的动作hidden_state #(b, 25, 64, 896)\n             p=proprio_features,  # (bsz, 1, llm_dim)\n-            h_t=task_hidden_states #视觉语言\n+            h_t=task_hidden_states #视觉语言 torch.Size([4, 25, 512, 896])\n             )\n \n         return action\n     \n"
                },
                {
                    "date": 1765277570378,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -335,8 +335,9 @@\n \n \n     def forward(self, x, h_a=None, h_t=None, p=None):\n         \"\"\"\n+        x: (B, 8, 496)\n         h_a: adapter tokens\n         h_t: task tokens\n         p:   possible conditioning vector (for FiLM)\n         \"\"\"\n"
                },
                {
                    "date": 1765277576528,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -335,9 +335,9 @@\n \n \n     def forward(self, x, h_a=None, h_t=None, p=None):\n         \"\"\"\n-        x: (B, 8, 496)\n+        x: (B, 8, 896)\n         h_a: adapter tokens\n         h_t: task tokens\n         p:   possible conditioning vector (for FiLM)\n         \"\"\"\n"
                },
                {
                    "date": 1765277598912,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -336,9 +336,9 @@\n \n     def forward(self, x, h_a=None, h_t=None, p=None):\n         \"\"\"\n         x: (B, 8, 896)\n-        h_a: adapter tokens\n+        h_a: adapter tokens #(b, 64, 896)\n         h_t: task tokens\n         p:   possible conditioning vector (for FiLM)\n         \"\"\"\n         g = self.gating_factor\n"
                },
                {
                    "date": 1765278538337,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -337,10 +337,10 @@\n     def forward(self, x, h_a=None, h_t=None, p=None):\n         \"\"\"\n         x: (B, 8, 896)\n         h_a: adapter tokens #(b, 64, 896)\n-        h_t: task tokens\n-        p:   possible conditioning vector (for FiLM)\n+        h_t: task tokens #(b, 512, 896)\n+        p:   possible conditioning vector (for FiLM) #(b, 1, 896)\n         \"\"\"\n         g = self.gating_factor\n         ratio_g = torch.tanh(g)\n \n"
                },
                {
                    "date": 1765278675179,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,9 +108,13 @@\n         self.fc2 = nn.Linear(hidden_dim, output_dim)\n \n \n     def forward(self, x, h_a=None, h_t=None, p= None):\n- \n+        #  rearranged_actions_hidden_states, # (batch, chunk_len, action_dim * hidden_dim) (b, 7, 8*896)\n+        # h_a=actions_hidden_states, #transformer中和的动作hidden_state #(b, 25, 64, 896)\n+        # p=proprio_features,  # (bsz, 1, llm_dim)\n+        # h_t=task_hidden_states #视觉语言 torch.Size([4, 25, 512, 896])\n+\n         # x: (batch_size, input_dim)\n         x = self.layer_norm1(x)  # shape: (batch_size, input_dim)\n         x = self.fc1(x)  # shape: (batch_size, hidden_dim)\n         x = self.relu(x)  # shape: (batch_size, hidden_dim)\n"
                },
                {
                    "date": 1765285914901,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,13 +108,9 @@\n         self.fc2 = nn.Linear(hidden_dim, output_dim)\n \n \n     def forward(self, x, h_a=None, h_t=None, p= None):\n-        #  rearranged_actions_hidden_states, # (batch, chunk_len, action_dim * hidden_dim) (b, 7, 8*896)\n-        # h_a=actions_hidden_states, #transformer中和的动作hidden_state #(b, 25, 64, 896)\n-        # p=proprio_features,  # (bsz, 1, llm_dim)\n-        # h_t=task_hidden_states #视觉语言 torch.Size([4, 25, 512, 896])\n-\n+ \n         # x: (batch_size, input_dim)\n         x = self.layer_norm1(x)  # shape: (batch_size, input_dim)\n         x = self.fc1(x)  # shape: (batch_size, hidden_dim)\n         x = self.relu(x)  # shape: (batch_size, hidden_dim)\n@@ -342,9 +338,9 @@\n         \"\"\"\n         x: (B, 8, 896)\n         h_a: adapter tokens #(b, 64, 896)\n         h_t: task tokens #(b, 512, 896)\n-        p:   possible conditioning vector (for FiLM) #(b, 1, 896)\n+        p:   possible conditioning vector (for FiLM) #(b, 1, )\n         \"\"\"\n         g = self.gating_factor\n         ratio_g = torch.tanh(g)\n \n"
                },
                {
                    "date": 1765286126349,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,10 +119,12 @@\n         x = self.layer_norm2(x)  # shape: (batch_size, hidden_dim)\n         x = self.fc2(x)  # shape: (batch_size, output_dim)\n         return x   \n \n+class MaskTransformer(nn.Module):\n+    def __init__(self, ) -> None:\n+        super().__init__(*args, **kwargs)\n \n-\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765286131935,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -120,10 +120,10 @@\n         x = self.fc2(x)  # shape: (batch_size, output_dim)\n         return x   \n \n class MaskTransformer(nn.Module):\n-    def __init__(self, ) -> None:\n-        super().__init__(*args, **kwargs)\n+    def __init__(self, ):\n+        super().__init__()\n \n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n"
                },
                {
                    "date": 1765286687207,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,8 +4,9 @@\n Implementations of various action heads, which serve as alternatives to VLM sequential token prediction.\n \"\"\"\n \n import math\n+from turtle import forward\n import torch\n import torch.nn as nn\n from prismatic.vla.constants import ACTION_DIM, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX, NUM_TOKENS\n \n@@ -122,9 +123,19 @@\n \n class MaskTransformer(nn.Module):\n     def __init__(self, ):\n         super().__init__()\n+    \n+    def forward(self, x): \n+        '''\n+        task_description:(b, 512, 896)\n+        action_query: (b, 64, 896)\n+        '''\n \n+\n+\n+\n+\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765286764714,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -126,8 +126,9 @@\n         super().__init__()\n     \n     def forward(self, x): \n         '''\n+        x: \n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n \n"
                },
                {
                    "date": 1765286836801,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -127,8 +127,9 @@\n     \n     def forward(self, x): \n         '''\n         x: \n+        action: (b, w, 7)\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n \n"
                },
                {
                    "date": 1765286883441,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,9 +121,11 @@\n         x = self.fc2(x)  # shape: (batch_size, output_dim)\n         return x   \n \n class MaskTransformer(nn.Module):\n-    def __init__(self, ):\n+    def __init__(self, \n+                 mask_type = '1D',\n+                 ):\n         super().__init__()\n     \n     def forward(self, x): \n         '''\n"
                },
                {
                    "date": 1765286907398,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,21 +125,21 @@\n     def __init__(self, \n                  mask_type = '1D',\n                  ):\n         super().__init__()\n-    \n+        self.mask_type = mask_type\n     def forward(self, x): \n         '''\n         x: \n         action: (b, w, 7)\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n+        if self.mask_type == '1D':\n \n \n \n \n-\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765287032817,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,13 +133,14 @@\n         action: (b, w, 7)\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n+        bs = x.shape[0]\n         if self.mask_type == '1D':\n+            \n \n \n \n-\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765287126268,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,9 +119,9 @@\n             x = block(x, h_t = h_t[:,i+1,:], h_a = h_a[:,i+1,:], p=p)  # shape: (batch_size, hidden_dim)\n         x = self.layer_norm2(x)  # shape: (batch_size, hidden_dim)\n         x = self.fc2(x)  # shape: (batch_size, output_dim)\n         return x   \n-\n+from .tools import *\n class MaskTransformer(nn.Module):\n     def __init__(self, \n                  mask_type = '1D',\n                  ):\n@@ -135,12 +135,13 @@\n         action_query: (b, 64, 896)\n         '''\n         bs = x.shape[0]\n         if self.mask_type == '1D':\n-            \n+            rand_time = uniform((bs,), device=self.device)#(bs,)\n \n \n \n+\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765287144102,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,12 +136,12 @@\n         '''\n         bs = x.shape[0]\n         if self.mask_type == '1D':\n             rand_time = uniform((bs,), device=self.device)#(bs,)\n+            rand_mask_probs = self.noise_schedule(rand_time)\n \n \n \n-\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765287155202,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -126,8 +126,10 @@\n                  mask_type = '1D',\n                  ):\n         super().__init__()\n         self.mask_type = mask_type\n+        self.noise_schedule = cosine_schedule\n+        self.noise_schedule_backward = cosine_schedule_backward\n     def forward(self, x): \n         '''\n         x: \n         action: (b, w, 7)\n@@ -140,8 +142,9 @@\n             rand_mask_probs = self.noise_schedule(rand_time)\n \n \n \n+\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765287209197,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,16 +128,18 @@\n         super().__init__()\n         self.mask_type = mask_type\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n+\n     def forward(self, x): \n         '''\n         x: \n         action: (b, w, 7)\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n-        bs = x.shape[0]\n+        bs, xtokens, ytokens = x.shape\n+        ntokens = \n         if self.mask_type == '1D':\n             rand_time = uniform((bs,), device=self.device)#(bs,)\n             rand_mask_probs = self.noise_schedule(rand_time)\n \n"
                },
                {
                    "date": 1765287217470,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n         bs, xtokens, ytokens = x.shape\n-        ntokens = \n+        ntokens = xtokens * ytokenss\n         if self.mask_type == '1D':\n             rand_time = uniform((bs,), device=self.device)#(bs,)\n             rand_mask_probs = self.noise_schedule(rand_time)\n \n"
                },
                {
                    "date": 1765287223514,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,9 +137,9 @@\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n         bs, xtokens, ytokens = x.shape\n-        ntokens = xtokens * ytokenss\n+        ntokens = xtokens * ytokens\n         if self.mask_type == '1D':\n             rand_time = uniform((bs,), device=self.device)#(bs,)\n             rand_mask_probs = self.noise_schedule(rand_time)\n \n"
                },
                {
                    "date": 1765287285151,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -141,12 +141,14 @@\n         ntokens = xtokens * ytokens\n         if self.mask_type == '1D':\n             rand_time = uniform((bs,), device=self.device)#(bs,)\n             rand_mask_probs = self.noise_schedule(rand_time)\n+            num_token_masked = (ntokens * rand_mask_probs).round().clamp(min=1)\n+            batch_randperm = torch.rand((bs, ntokens), device=self.device).argsort(dim=-1)\n+            mask = batch_randperm < num_token_masked.unsqueeze(-1) #(bs, ntokens)\n+            labels = torch.where(mask, discretized_action_ids, self.mask_id)\n \n \n-\n-\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765287340396,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -132,8 +132,9 @@\n \n     def forward(self, x): \n         '''\n         x: \n+        注意这里的x是原始tokenizer之后的action_id\n         action: (b, w, 7)\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n@@ -144,9 +145,9 @@\n             rand_mask_probs = self.noise_schedule(rand_time)\n             num_token_masked = (ntokens * rand_mask_probs).round().clamp(min=1)\n             batch_randperm = torch.rand((bs, ntokens), device=self.device).argsort(dim=-1)\n             mask = batch_randperm < num_token_masked.unsqueeze(-1) #(bs, ntokens)\n-            labels = torch.where(mask, discretized_action_ids, self.mask_id)\n+            labels = torch.where(mask, x, self.mask_id)\n \n \n def apply_rope(q, k, cos, sin):\n     \"\"\"\n"
                },
                {
                    "date": 1765287673933,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -146,8 +146,15 @@\n             num_token_masked = (ntokens * rand_mask_probs).round().clamp(min=1)\n             batch_randperm = torch.rand((bs, ntokens), device=self.device).argsort(dim=-1)\n             mask = batch_randperm < num_token_masked.unsqueeze(-1) #(bs, ntokens)\n             labels = torch.where(mask, x, self.mask_id)\n+            x_id = x.clone()\n+            mask_rid = get_mask_subset_prob(mask, 0.1)\n+            rand_id = torch.randint_like(x_ids, high=self.mask_id)\n+            x_ids = torch.where(mask_rid, rand_id, x_ids)#10%替换为随机的值\n+            mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n+            x_ids = torch.where(mask_mid, self.mask_id, x_ids)#(b, x*y)\n+            \n \n \n def apply_rope(q, k, cos, sin):\n     \"\"\"\n"
                },
                {
                    "date": 1765287701760,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -152,11 +152,59 @@\n             rand_id = torch.randint_like(x_ids, high=self.mask_id)\n             x_ids = torch.where(mask_rid, rand_id, x_ids)#10%替换为随机的值\n             mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n             x_ids = torch.where(mask_mid, self.mask_id, x_ids)#(b, x*y)\n-            \n+        elif self.mask_type == '2D':\n+            rand_time = uniform((bs,), device=self.device)\n+            rand_mask_probs = self.noise_schedule(rand_time)\n \n+            # ========== temporal mask\n+            #因为太少了，min就设置为0吧\n+            num_token_masked = (xtokens * rand_mask_probs).round().clamp(min=0)\n+            batch_randperm = torch.rand((bs, xtokens), device=self.device).argsort(dim=-1)\n+            # Positions to be MASKED are ALL TRUE\n+            mask = batch_randperm < num_token_masked.unsqueeze(-1)\n+            # Positions to be MASKED must also be NON-PADDED\n+            # mask = mask & non_pad_mask[..., 0]\n+            # Note this is our training target, not input\n+            labels = torch.where(mask[..., None].repeat(1, 1, ytokens), discretized_action, self.mask_id)\n+            x_ids = x.clone()\n+            # Further Apply Bert Masking Scheme\n+            # Step 1: 10% replace with an incorrect token\n+            mask_rid = get_mask_subset_prob(mask, 0.1)\n+            rand_id = torch.randint_like(x_ids, high=self.mask_id)\n+            x_ids = torch.where(mask_rid[..., None].repeat(1, 1, ytokens), rand_id, x_ids)\n+            # Step 2: 90% x 10% replace with correct token, and 90% x 88% replace with mask token\n+            mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n+            x_ids = torch.where(mask_mid[..., None].repeat(1, 1, ytokens), self.mask_id, x_ids)\n+            mask_time = mask\n+            mask_time = mask_time[..., None].repeat(1, 1, ytokens)       # keep temperal mask still masked\n+            # print((x_ids==512).sum(), mask_time.sum(), mask.sum(), (labels!=512).sum(), mask_rid.sum())\n \n+            # ========== spatial mask\n+            num_token_masked = (ntokens * rand_mask_probs).round().clamp(min=0)\n+            batch_randperm = torch.rand((bs, ntokens), device=self.device).argsort(dim=-1)\n+            # Positions to be MASKED are ALL TRUE\n+            mask = batch_randperm < num_token_masked.unsqueeze(-1)\n+            # Positions to be MASKED must also be NON-PADDED\n+            # mask = mask & non_pad_mask.reshape(bs, -1)\n+            mask = mask & ~mask_time.reshape(bs, -1)\n+            # Note this is our training target, not input\n+            labels = torch.where(mask, x_ids.reshape(bs, -1), labels.reshape(bs, -1))\n+            x_ids = x_ids.reshape(bs, -1)\n+            # Further Apply Bert Masking Scheme\n+            # Step 1: 10% replace with an incorrect token\n+            mask_rid = get_mask_subset_prob(mask, 0.1)\n+\n+            rand_id = torch.randint_like(x_ids, high=self.mask_id)\n+            x_ids = torch.where(mask_rid, rand_id, x_ids)\n+            # Step 2: 90% x 10% replace with correct token, and 90% x 88% replace with mask token\n+            mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n+            # mask_mid = mask\n+            x_ids = torch.where(mask_mid, self.mask_id, x_ids)#(b, x*y)\n+\n+\n+\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765287710792,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -158,16 +158,16 @@\n             rand_mask_probs = self.noise_schedule(rand_time)\n \n             # ========== temporal mask\n             #因为太少了，min就设置为0吧\n-            num_token_masked = (xtokens * rand_mask_probs).round().clamp(min=0)\n+            num_token_masked = (xtokens * rand_mask_probs).round().clamp(min=1)\n             batch_randperm = torch.rand((bs, xtokens), device=self.device).argsort(dim=-1)\n             # Positions to be MASKED are ALL TRUE\n             mask = batch_randperm < num_token_masked.unsqueeze(-1)\n             # Positions to be MASKED must also be NON-PADDED\n             # mask = mask & non_pad_mask[..., 0]\n             # Note this is our training target, not input\n-            labels = torch.where(mask[..., None].repeat(1, 1, ytokens), discretized_action, self.mask_id)\n+            labels = torch.where(mask[..., None].repeat(1, 1, ytokens), x, self.mask_id)\n             x_ids = x.clone()\n             # Further Apply Bert Masking Scheme\n             # Step 1: 10% replace with an incorrect token\n             mask_rid = get_mask_subset_prob(mask, 0.1)\n"
                },
                {
                    "date": 1765287721313,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -146,9 +146,9 @@\n             num_token_masked = (ntokens * rand_mask_probs).round().clamp(min=1)\n             batch_randperm = torch.rand((bs, ntokens), device=self.device).argsort(dim=-1)\n             mask = batch_randperm < num_token_masked.unsqueeze(-1) #(bs, ntokens)\n             labels = torch.where(mask, x, self.mask_id)\n-            x_id = x.clone()\n+            x_ids = x.clone()\n             mask_rid = get_mask_subset_prob(mask, 0.1)\n             rand_id = torch.randint_like(x_ids, high=self.mask_id)\n             x_ids = torch.where(mask_rid, rand_id, x_ids)#10%替换为随机的值\n             mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n"
                },
                {
                    "date": 1765287745917,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -200,11 +200,11 @@\n             # Step 2: 90% x 10% replace with correct token, and 90% x 88% replace with mask token\n             mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n             # mask_mid = mask\n             x_ids = torch.where(mask_mid, self.mask_id, x_ids)#(b, x*y)\n+        logits = self.trans_forward(x_ids, cond, force_mask)\n \n \n-\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765287796129,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -129,8 +129,19 @@\n         self.mask_type = mask_type\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n \n+    def mask_cond(self, cond, force_mask=False):#条件掩码\n+        bs, d =  cond.shape\n+        if force_mask:\n+            return torch.zeros_like(cond)\n+        elif self.training and self.cond_drop_prob > 0.:#1\n+            mask = torch.bernoulli(torch.ones(bs, device=cond.device) * self.cond_drop_prob).view(bs, 1)\n+            return cond * (1. - mask)\n+        else:\n+            return cond\n+    def trans_forward(self, x_ids, cond, force_mask):\n+\n     def forward(self, x): \n         '''\n         x: \n         注意这里的x是原始tokenizer之后的action_id\n"
                },
                {
                    "date": 1765287817776,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -139,8 +139,10 @@\n             return cond * (1. - mask)\n         else:\n             return cond\n     def trans_forward(self, x_ids, cond, force_mask):\n+        cond = self.mask_cond(cond, force_mask=force_mask)\n+        x = self.token_emb(x_ids) \n \n     def forward(self, x): \n         '''\n         x: \n"
                },
                {
                    "date": 1765287839031,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -141,8 +141,9 @@\n             return cond\n     def trans_forward(self, x_ids, cond, force_mask):\n         cond = self.mask_cond(cond, force_mask=force_mask)\n         x = self.token_emb(x_ids) \n+        #这里看一下位置编码的细节\n \n     def forward(self, x): \n         '''\n         x: \n"
                },
                {
                    "date": 1765287845580,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,8 +142,9 @@\n     def trans_forward(self, x_ids, cond, force_mask):\n         cond = self.mask_cond(cond, force_mask=force_mask)\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n+        return logits\n \n     def forward(self, x): \n         '''\n         x: \n"
                },
                {
                    "date": 1765345712645,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,8 +142,9 @@\n     def trans_forward(self, x_ids, cond, force_mask):\n         cond = self.mask_cond(cond, force_mask=force_mask)\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n+        \n         return logits\n \n     def forward(self, x): \n         '''\n"
                },
                {
                    "date": 1765346297585,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -120,16 +120,31 @@\n         x = self.layer_norm2(x)  # shape: (batch_size, hidden_dim)\n         x = self.fc2(x)  # shape: (batch_size, output_dim)\n         return x   \n from .tools import *\n+class InputProcess(nn.Module):\n+    def __init__(self, input_feats, latent_dim):\n+        super().__init__()\n+        self.input_feats = input_feats\n+        self.latent_dim = latent_dim\n+        self.poseEmbedding = nn.Linear(self.input_feats, self.latent_dim)\n+\n+    def forward(self, x):\n+        # [bs, ntokens, input_feats]\n+        x = x.permute((1, 0, 2)) # [seqen, bs, input_feats]\n+        # print(x.shape)\n+        x = self.poseEmbedding(x)  # [seqlen, bs, d]\n+        return x\n+    \n class MaskTransformer(nn.Module):\n     def __init__(self, \n                  mask_type = '1D',\n                  ):\n         super().__init__()\n         self.mask_type = mask_type\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n+        self.input_process = \n \n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n         if force_mask:\n@@ -142,9 +157,9 @@\n     def trans_forward(self, x_ids, cond, force_mask):\n         cond = self.mask_cond(cond, force_mask=force_mask)\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n-        \n+\n         return logits\n \n     def forward(self, x): \n         '''\n"
                },
                {
                    "date": 1765346386328,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,9 +142,9 @@\n         super().__init__()\n         self.mask_type = mask_type\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n-        self.input_process = \n+        self.input_process = InputProcess(self.code_dim, self.latent_dim)\n \n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n         if force_mask:\n"
                },
                {
                    "date": 1765346441779,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -137,15 +137,18 @@\n     \n class MaskTransformer(nn.Module):\n     def __init__(self, \n                  mask_type = '1D',\n+                 code_dim = 512,\n+                 latent_dim = 512\n                  ):\n         super().__init__()\n         self.mask_type = mask_type\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n         self.input_process = InputProcess(self.code_dim, self.latent_dim)\n \n+\n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n         if force_mask:\n             return torch.zeros_like(cond)\n"
                },
                {
                    "date": 1765346461595,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,8 +142,10 @@\n                  latent_dim = 512\n                  ):\n         super().__init__()\n         self.mask_type = mask_type\n+        self.code_dim = code_dim\n+        self.latent_dim = latent_dim\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n         self.input_process = InputProcess(self.code_dim, self.latent_dim)\n \n"
                },
                {
                    "date": 1765346487640,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -163,8 +163,10 @@\n         cond = self.mask_cond(cond, force_mask=force_mask)\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n \n+        x = self.input_process(x)\n+        \n         return logits\n \n     def forward(self, x): \n         '''\n"
                },
                {
                    "date": 1765346506852,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -162,9 +162,8 @@\n     def trans_forward(self, x_ids, cond, force_mask):\n         cond = self.mask_cond(cond, force_mask=force_mask)\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n-\n         x = self.input_process(x)\n         \n         return logits\n \n"
                },
                {
                    "date": 1765349579241,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -163,9 +163,9 @@\n         cond = self.mask_cond(cond, force_mask=force_mask)\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n         x = self.input_process(x)\n-        \n+        x = self.position_enc(x)\n         return logits\n \n     def forward(self, x): \n         '''\n"
                },
                {
                    "date": 1765349593587,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,10 +147,10 @@\n         self.latent_dim = latent_dim\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n         self.input_process = InputProcess(self.code_dim, self.latent_dim)\n+        self.position_enc = SpaTempPositionalEncoding(self.latent_dim, self.dropout)\n \n-\n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n         if force_mask:\n             return torch.zeros_like(cond)\n"
                },
                {
                    "date": 1765349602986,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,9 +133,35 @@\n         x = x.permute((1, 0, 2)) # [seqen, bs, input_feats]\n         # print(x.shape)\n         x = self.poseEmbedding(x)  # [seqlen, bs, d]\n         return x\n+class SpaTempPositionalEncoding(nn.Module):\n+    def __init__(self, d_model, dropout=0.1):\n+        super(SpaTempPositionalEncoding, self).__init__()\n+        self.dropout = nn.Dropout(p=dropout)\n+        self.positional_encoding = PositionalEncoding2D(d_model)\n     \n+    def forward(self, x, window_size=10):\n+        seqlen, bs, input_feats = x.shape\n+        t = window_size//5\n+        # x1, sep, x2  = x.split([seqlen//2, 1, seqlen//2])\n+        #这里的时空注意力出问题了，到底该怎么办啊\n+        def add_positional_encoding(x):\n+            x = x.permute(1,0,2) # [seqen, bs, input_feats] -> [bs, seqen, input_feats]\n+            x = x.reshape(x.shape[0], t, x.shape[1]//t, x.shape[2])\n+\n+            x = x + self.positional_encoding(x)\n+\n+            x = x.reshape(x.shape[0], x.shape[1] * x.shape[2], x.shape[3])\n+            x = x.permute(1,0,2)\n+\n+            return x\n+\n+        x = add_positional_encoding(x)\n+        # x2 = add_positional_encoding(x2)\n+        \n+        # x = torch.cat([x1, sep, x2], dim=0)\n+        return self.dropout(x)\n class MaskTransformer(nn.Module):\n     def __init__(self, \n                  mask_type = '1D',\n                  code_dim = 512,\n"
                },
                {
                    "date": 1765349631325,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,11 +8,11 @@\n from turtle import forward\n import torch\n import torch.nn as nn\n from prismatic.vla.constants import ACTION_DIM, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX, NUM_TOKENS\n+from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D\n \n \n-\n def learnable_random_perturbations(seq_len, dim, device, dtype):\n     random_perturbations = nn.Parameter(torch.zeros(seq_len, dim, device=device, dtype=dtype))\n     nn.init.normal_(random_perturbations, mean=0.0, std=0.02)\n     return random_perturbations\n@@ -160,8 +160,9 @@\n         # x2 = add_positional_encoding(x2)\n         \n         # x = torch.cat([x1, sep, x2], dim=0)\n         return self.dropout(x)\n+    \n class MaskTransformer(nn.Module):\n     def __init__(self, \n                  mask_type = '1D',\n                  code_dim = 512,\n"
                },
                {
                    "date": 1765349924944,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -191,8 +191,9 @@\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n         x = self.input_process(x)\n         x = self.position_enc(x)\n+        output = self.Transformer(x, cond, )\n         return logits\n \n     def forward(self, x): \n         '''\n"
                },
                {
                    "date": 1765349962474,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -191,9 +191,10 @@\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n         x = self.input_process(x)\n         x = self.position_enc(x)\n-        output = self.Transformer(x, cond, )\n+        output = self.Transformer(x, cond)\n+        logits = self.output_process(output, cond)\n         return logits\n \n     def forward(self, x): \n         '''\n"
                },
                {
                    "date": 1765350098685,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -175,9 +175,14 @@\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n         self.input_process = InputProcess(self.code_dim, self.latent_dim)\n         self.position_enc = SpaTempPositionalEncoding(self.latent_dim, self.dropout)\n-\n+       self.Transformer = VLATransformer(d_model=self.latent_dim,\n+                                        nhead=num_heads,\n+                                        dim_feedforward=ff_size,\n+                                        dropout=dropout,\n+                                        num_layers=num_decoder_layers,\n+                                        nbp=self.nbp)\n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n         if force_mask:\n             return torch.zeros_like(cond)\n"
                },
                {
                    "date": 1765350233441,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,9 +160,39 @@\n         # x2 = add_positional_encoding(x2)\n         \n         # x = torch.cat([x1, sep, x2], dim=0)\n         return self.dropout(x)\n-    \n+\n+class VLATransformer(nn.Module):\n+    def __init__(self, num_layers, d_model, nhead, dim_feedforward, dropout, nbp):\n+        super().__init__()\n+        block = VLATransformerBlock(d_model=d_model,\n+                                nhead=nhead,\n+                                dim_feedforward=dim_feedforward,\n+                                dropout=dropout,\n+                                nbp=nbp)\n+        #方案a,使用adaln把条件信息注入\n+        #方案b,使用cross-attention注入条件信息\n+        module_list = []\n+        \n+        for _ in range(num_layers):\n+            module_list.append(copy.deepcopy(block))\n+\n+        self.blocks =  ModuleList(module_list)\n+\n+    def forward(self, src, cond, src_key_padding_mask=None):\n+        \"\"\"\n+        Args:\n+            src: Tensor, shape (751, B, d_model)\n+            src_key_padding_mask: Tensor, shape (B, 751) \n+\n+        Additional token is for the condition\n+        \"\"\"\n+\n+        for block in self.blocks:\n+            src = block(src, cond, src_key_padding_mask=src_key_padding_mask)\n+        return src\n+\n class MaskTransformer(nn.Module):\n     def __init__(self, \n                  mask_type = '1D',\n                  code_dim = 512,\n@@ -175,14 +205,15 @@\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n         self.input_process = InputProcess(self.code_dim, self.latent_dim)\n         self.position_enc = SpaTempPositionalEncoding(self.latent_dim, self.dropout)\n-       self.Transformer = VLATransformer(d_model=self.latent_dim,\n-                                        nhead=num_heads,\n-                                        dim_feedforward=ff_size,\n-                                        dropout=dropout,\n-                                        num_layers=num_decoder_layers,\n-                                        nbp=self.nbp)\n+        self.Transformer = VLATransformer(d_model=self.latent_dim,\n+                                            nhead=num_heads,\n+                                            dim_feedforward=ff_size,\n+                                            dropout=dropout,\n+                                            num_layers=num_decoder_layers,\n+                                            nbp=self.nbp)\n+    \n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n         if force_mask:\n             return torch.zeros_like(cond)\n"
                },
                {
                    "date": 1765350255564,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,10 +9,10 @@\n import torch\n import torch.nn as nn\n from prismatic.vla.constants import ACTION_DIM, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX, NUM_TOKENS\n from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D\n+from torch.nn import ModuleList\n \n-\n def learnable_random_perturbations(seq_len, dim, device, dtype):\n     random_perturbations = nn.Parameter(torch.zeros(seq_len, dim, device=device, dtype=dtype))\n     nn.init.normal_(random_perturbations, mean=0.0, std=0.02)\n     return random_perturbations\n"
                },
                {
                    "date": 1765350287226,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,9 +160,44 @@\n         # x2 = add_positional_encoding(x2)\n         \n         # x = torch.cat([x1, sep, x2], dim=0)\n         return self.dropout(x)\n+class VLATransformerBlock(nn.Module):\n+    def __init__(self, d_model, nhead, dim_feedforward, dropout, nbp):\n+        super().__init__()\n+        self.spa_dim = nbp #时间步\n+        self.adaLN_mod_combined = AdaLNModulation(d_model, 6)\n+        self.self_attn = SelfAttnLayer(d_model, nhead, dropout) #self-attention\n+        self.ffn_combined = FFN(d_model, dim_feedforward, dropout) #feed-forward\n+        self.adaLN_mod_split = AdaLNModulation(d_model, 9)\n+        self.spa_temp_attn = SpaTempAttnLayer(d_model, nhead, dropout, spa_dim=self.spa_dim) #spatio-temporal-attn\n+        # self.local_inter_attn = LocalInteractionAttnLayer(d_model, nhead, dropout, spa_dim=self.spa_dim) #cross-attn\n+        self.ffn_spa = FFN(d_model, dim_feedforward, dropout) #feed-forward\n+    \n+    def forward(self, src, cond, src_key_padding_mask=None):\n+        N, B, d = src.shape\n+        # AdaLN modulation\n+        shift_self, scale_self, gate_self, \\\n+            shift_ffn_c, scale_ffn_c, gate_ffn_c = self.adaLN_mod_combined(cond) #adal-n all (b, latent_dim)\n+        \n+        # Self-Attention\n+        src = self.self_attn(src, \n+                            shift_self, scale_self, gate_self,\n+                            src_key_padding_mask=src_key_padding_mask)\n+        \n+        # FFN\n+        src = self.ffn_combined(src, shift_ffn_c, scale_ffn_c, gate_ffn_c)\n \n+        # AdaLN modulation\n+        shift_spa, scale_spa, gate_spa, shift_temp, scale_temp, gate_temp, \\\n+                shift_ffn_s, scale_ffn_s, gate_ffn_s = self.adaLN_mod_split(cond) #(b, latent_dim)\n+        # shift_cross, scale_cross, shift_cross2, scale_cross2, gate_cross, \\\n+        src_spa_temp = self.spa_temp_attn(src, \n+                            shift_spa, scale_spa, gate_spa, \n+                            shift_temp, scale_temp, gate_temp, \n+                            src_key_padding_mask=src_key_padding_mask)\n+        src_after = self.ffn_spa(src_spa_temp, shift_ffn_s, scale_ffn_s, gate_ffn_s)\n+        return src_after\n class VLATransformer(nn.Module):\n     def __init__(self, num_layers, d_model, nhead, dim_feedforward, dropout, nbp):\n         super().__init__()\n         block = VLATransformerBlock(d_model=d_model,\n"
                },
                {
                    "date": 1765350300017,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,9 +160,10 @@\n         # x2 = add_positional_encoding(x2)\n         \n         # x = torch.cat([x1, sep, x2], dim=0)\n         return self.dropout(x)\n-class VLATransformerBlock(nn.Module):\n+    \n+class VLATransformerCrossBlock(nn.Module):\n     def __init__(self, d_model, nhead, dim_feedforward, dropout, nbp):\n         super().__init__()\n         self.spa_dim = nbp #时间步\n         self.adaLN_mod_combined = AdaLNModulation(d_model, 6)\n@@ -196,8 +197,9 @@\n                             shift_temp, scale_temp, gate_temp, \n                             src_key_padding_mask=src_key_padding_mask)\n         src_after = self.ffn_spa(src_spa_temp, shift_ffn_s, scale_ffn_s, gate_ffn_s)\n         return src_after\n+    \n class VLATransformer(nn.Module):\n     def __init__(self, num_layers, d_model, nhead, dim_feedforward, dropout, nbp):\n         super().__init__()\n         block = VLATransformerBlock(d_model=d_model,\n"
                },
                {
                    "date": 1765350375030,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,9 +160,9 @@\n         # x2 = add_positional_encoding(x2)\n         \n         # x = torch.cat([x1, sep, x2], dim=0)\n         return self.dropout(x)\n-    \n+from .transformer_modules import AdaLNModulation, SelfAttnLayer, SpaTempAttnLayer, FFN\n class VLATransformerCrossBlock(nn.Module):\n     def __init__(self, d_model, nhead, dim_feedforward, dropout, nbp):\n         super().__init__()\n         self.spa_dim = nbp #时间步\n"
                },
                {
                    "date": 1765350382270,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,9 @@\n import torch.nn as nn\n from prismatic.vla.constants import ACTION_DIM, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX, NUM_TOKENS\n from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D\n from torch.nn import ModuleList\n-\n+from .transformer_modules import AdaLNModulation, SelfAttnLayer, SpaTempAttnLayer, FFN\n def learnable_random_perturbations(seq_len, dim, device, dtype):\n     random_perturbations = nn.Parameter(torch.zeros(seq_len, dim, device=device, dtype=dtype))\n     nn.init.normal_(random_perturbations, mean=0.0, std=0.02)\n     return random_perturbations\n@@ -160,9 +160,9 @@\n         # x2 = add_positional_encoding(x2)\n         \n         # x = torch.cat([x1, sep, x2], dim=0)\n         return self.dropout(x)\n-from .transformer_modules import AdaLNModulation, SelfAttnLayer, SpaTempAttnLayer, FFN\n+\n class VLATransformerCrossBlock(nn.Module):\n     def __init__(self, d_model, nhead, dim_feedforward, dropout, nbp):\n         super().__init__()\n         self.spa_dim = nbp #时间步\n"
                },
                {
                    "date": 1765350703938,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -176,10 +176,10 @@\n     \n     def forward(self, src, cond, src_key_padding_mask=None):\n         N, B, d = src.shape\n         # AdaLN modulation\n-        shift_self, scale_self, gate_self, \\\n-            shift_ffn_c, scale_ffn_c, gate_ffn_c = self.adaLN_mod_combined(cond) #adal-n all (b, latent_dim)\n+        # shift_self, scale_self, gate_self, \\\n+        #     shift_ffn_c, scale_ffn_c, gate_ffn_c = self.adaLN_mod_combined(cond) #adal-n all (b, latent_dim)\n         \n         # Self-Attention\n         src = self.self_attn(src, \n                             shift_self, scale_self, gate_self,\n"
                },
                {
                    "date": 1765350721934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -176,10 +176,10 @@\n     \n     def forward(self, src, cond, src_key_padding_mask=None):\n         N, B, d = src.shape\n         # AdaLN modulation\n-        # shift_self, scale_self, gate_self, \\\n-        #     shift_ffn_c, scale_ffn_c, gate_ffn_c = self.adaLN_mod_combined(cond) #adal-n all (b, latent_dim)\n+        shift_self, scale_self, gate_self, \\\n+            shift_ffn_c, scale_ffn_c, gate_ffn_c = self.adaLN_mod_combined(cond) #adal-n all (b, latent_dim)\n         \n         # Self-Attention\n         src = self.self_attn(src, \n                             shift_self, scale_self, gate_self,\n"
                },
                {
                    "date": 1765350735510,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -161,9 +161,9 @@\n         \n         # x = torch.cat([x1, sep, x2], dim=0)\n         return self.dropout(x)\n \n-class VLATransformerCrossBlock(nn.Module):\n+class VLATransformerAdaLNBlock(nn.Module):\n     def __init__(self, d_model, nhead, dim_feedforward, dropout, nbp):\n         super().__init__()\n         self.spa_dim = nbp #时间步\n         self.adaLN_mod_combined = AdaLNModulation(d_model, 6)\n"
                },
                {
                    "date": 1765350748130,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -201,9 +201,9 @@\n     \n class VLATransformer(nn.Module):\n     def __init__(self, num_layers, d_model, nhead, dim_feedforward, dropout, nbp):\n         super().__init__()\n-        block = VLATransformerBlock(d_model=d_model,\n+        block = VLATransformerAdaLNBlock(d_model=d_model,\n                                 nhead=nhead,\n                                 dim_feedforward=dim_feedforward,\n                                 dropout=dropout,\n                                 nbp=nbp)\n"
                },
                {
                    "date": 1765350763733,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -197,9 +197,9 @@\n                             shift_temp, scale_temp, gate_temp, \n                             src_key_padding_mask=src_key_padding_mask)\n         src_after = self.ffn_spa(src_spa_temp, shift_ffn_s, scale_ffn_s, gate_ffn_s)\n         return src_after\n-    \n+import copy\n class VLATransformer(nn.Module):\n     def __init__(self, num_layers, d_model, nhead, dim_feedforward, dropout, nbp):\n         super().__init__()\n         block = VLATransformerAdaLNBlock(d_model=d_model,\n"
                },
                {
                    "date": 1765350771103,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,8 +11,9 @@\n from prismatic.vla.constants import ACTION_DIM, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX, NUM_TOKENS\n from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D\n from torch.nn import ModuleList\n from .transformer_modules import AdaLNModulation, SelfAttnLayer, SpaTempAttnLayer, FFN\n+import copy\n def learnable_random_perturbations(seq_len, dim, device, dtype):\n     random_perturbations = nn.Parameter(torch.zeros(seq_len, dim, device=device, dtype=dtype))\n     nn.init.normal_(random_perturbations, mean=0.0, std=0.02)\n     return random_perturbations\n@@ -197,9 +198,9 @@\n                             shift_temp, scale_temp, gate_temp, \n                             src_key_padding_mask=src_key_padding_mask)\n         src_after = self.ffn_spa(src_spa_temp, shift_ffn_s, scale_ffn_s, gate_ffn_s)\n         return src_after\n-import copy\n+\n class VLATransformer(nn.Module):\n     def __init__(self, num_layers, d_model, nhead, dim_feedforward, dropout, nbp):\n         super().__init__()\n         block = VLATransformerAdaLNBlock(d_model=d_model,\n"
                },
                {
                    "date": 1765351592279,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -233,9 +233,10 @@\n class MaskTransformer(nn.Module):\n     def __init__(self, \n                  mask_type = '1D',\n                  code_dim = 512,\n-                 latent_dim = 512\n+                 latent_dim = 512,\n+                 num_heads = 8,\n                  ):\n         super().__init__()\n         self.mask_type = mask_type\n         self.code_dim = code_dim\n"
                },
                {
                    "date": 1765351598789,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -240,8 +240,9 @@\n         super().__init__()\n         self.mask_type = mask_type\n         self.code_dim = code_dim\n         self.latent_dim = latent_dim\n+        self.num_heads = num_heads\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n         self.input_process = InputProcess(self.code_dim, self.latent_dim)\n         self.position_enc = SpaTempPositionalEncoding(self.latent_dim, self.dropout)\n"
                },
                {
                    "date": 1765351635133,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -235,8 +235,9 @@\n                  mask_type = '1D',\n                  code_dim = 512,\n                  latent_dim = 512,\n                  num_heads = 8,\n+                 dropout = 0.1,\n                  ):\n         super().__init__()\n         self.mask_type = mask_type\n         self.code_dim = code_dim\n"
                },
                {
                    "date": 1765351707883,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -234,10 +234,12 @@\n     def __init__(self, \n                  mask_type = '1D',\n                  code_dim = 512,\n                  latent_dim = 512,\n+                 num_decoder_layers = 25\n                  num_heads = 8,\n                  dropout = 0.1,\n+\n                  ):\n         super().__init__()\n         self.mask_type = mask_type\n         self.code_dim = code_dim\n"
                },
                {
                    "date": 1765351714692,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -234,9 +234,9 @@\n     def __init__(self, \n                  mask_type = '1D',\n                  code_dim = 512,\n                  latent_dim = 512,\n-                 num_decoder_layers = 25\n+                 num_decoder_layers = 24,\n                  num_heads = 8,\n                  dropout = 0.1,\n \n                  ):\n"
                },
                {
                    "date": 1765351726074,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -237,15 +237,15 @@\n                  latent_dim = 512,\n                  num_decoder_layers = 24,\n                  num_heads = 8,\n                  dropout = 0.1,\n-\n                  ):\n         super().__init__()\n         self.mask_type = mask_type\n         self.code_dim = code_dim\n         self.latent_dim = latent_dim\n         self.num_heads = num_heads\n+        self.num_decoder_layers = num_decoder_layers\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n         self.input_process = InputProcess(self.code_dim, self.latent_dim)\n         self.position_enc = SpaTempPositionalEncoding(self.latent_dim, self.dropout)\n"
                },
                {
                    "date": 1765351734869,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -243,9 +243,8 @@\n         self.mask_type = mask_type\n         self.code_dim = code_dim\n         self.latent_dim = latent_dim\n         self.num_heads = num_heads\n-        self.num_decoder_layers = num_decoder_layers\n         self.noise_schedule = cosine_schedule\n         self.noise_schedule_backward = cosine_schedule_backward\n         self.input_process = InputProcess(self.code_dim, self.latent_dim)\n         self.position_enc = SpaTempPositionalEncoding(self.latent_dim, self.dropout)\n"
                },
                {
                    "date": 1765351804039,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -249,9 +249,9 @@\n         self.input_process = InputProcess(self.code_dim, self.latent_dim)\n         self.position_enc = SpaTempPositionalEncoding(self.latent_dim, self.dropout)\n         self.Transformer = VLATransformer(d_model=self.latent_dim,\n                                             nhead=num_heads,\n-                                            dim_feedforward=ff_size,\n+                                            dim_feedforward=4 * latent_dim,\n                                             dropout=dropout,\n                                             num_layers=num_decoder_layers,\n                                             nbp=self.nbp)\n     \n"
                },
                {
                    "date": 1765351820385,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -269,9 +269,9 @@\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n         x = self.input_process(x)\n         x = self.position_enc(x)\n-        output = self.Transformer(x, cond)\n+        output = self.Transformer(x, cond)#特定层\n         logits = self.output_process(output, cond)\n         return logits\n \n     def forward(self, x): \n"
                },
                {
                    "date": 1765351831728,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -270,9 +270,9 @@\n         #这里看一下位置编码的细节\n         x = self.input_process(x)\n         x = self.position_enc(x)\n         output = self.Transformer(x, cond)#特定层\n-        logits = self.output_process(output, cond)\n+        logits = self.output_process(output, cond) #final\n         return logits\n \n     def forward(self, x): \n         '''\n"
                },
                {
                    "date": 1765351938348,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -268,10 +268,10 @@\n         cond = self.mask_cond(cond, force_mask=force_mask)\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n         x = self.input_process(x)\n-        x = self.position_enc(x)\n-        output = self.Transformer(x, cond)#特定层\n+        # x = self.position_enc(x)\n+        output = self.Transformer(x, cond)#特定层,没一层都有位置编码\n         logits = self.output_process(output, cond) #final\n         return logits\n \n     def forward(self, x): \n"
                },
                {
                    "date": 1765353386760,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -273,9 +273,9 @@\n         output = self.Transformer(x, cond)#特定层,没一层都有位置编码\n         logits = self.output_process(output, cond) #final\n         return logits\n \n-    def forward(self, x): \n+    def forward(self, x, cond): \n         '''\n         x: \n         注意这里的x是原始tokenizer之后的action_id\n         action: (b, w, 7)\n"
                },
                {
                    "date": 1765353560138,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -269,9 +269,9 @@\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n         x = self.input_process(x)\n         # x = self.position_enc(x)\n-        output = self.Transformer(x, cond)#特定层,没一层都有位置编码\n+        output = self.Transformer(x, cond)#特定层,每一层都有位置编码\n         logits = self.output_process(output, cond) #final\n         return logits\n \n     def forward(self, x, cond): \n"
                },
                {
                    "date": 1765353623410,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -161,9 +161,29 @@\n         # x2 = add_positional_encoding(x2)\n         \n         # x = torch.cat([x1, sep, x2], dim=0)\n         return self.dropout(x)\n+class OutputProcess_adaLN(nn.Module):\n+    def __init__(self, out_feats, latent_dim):\n+        super().__init__()\n+        self.dense = nn.Linear(latent_dim, latent_dim)\n+        self.transform_act_fn = F.gelu\n+        \n+        self.LayerNorm = nn.LayerNorm(latent_dim, elementwise_affine=False, eps=1e-6)\n+        self.adaLN_mod = AdaLNModulation(latent_dim, nchunks=2)\n+        \n+        self.poseFinal = nn.Linear(latent_dim, out_feats) #Bias!\n \n+    def forward(self, hidden_states: torch.Tensor, cond:torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.transform_act_fn(hidden_states)\n+        \n+        shift, scale = self.adaLN_mod(cond)\n+        hidden_states = modulate(self.LayerNorm(hidden_states), shift, scale)\n+\n+        output = self.poseFinal(hidden_states)  # [seqlen, bs, out_feats]\n+        output = output.permute(1, 2, 0)  # [bs, e, seqlen]\n+        return output\n class VLATransformerAdaLNBlock(nn.Module):\n     def __init__(self, d_model, nhead, dim_feedforward, dropout, nbp):\n         super().__init__()\n         self.spa_dim = nbp #时间步\n"
                },
                {
                    "date": 1765353639963,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,9 @@\n import torch.nn as nn\n from prismatic.vla.constants import ACTION_DIM, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX, NUM_TOKENS\n from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D\n from torch.nn import ModuleList\n-from .transformer_modules import AdaLNModulation, SelfAttnLayer, SpaTempAttnLayer, FFN\n+from .transformer_modules import AdaLNModulation, SelfAttnLayer, SpaTempAttnLayer, FFN, modulate\n import copy\n def learnable_random_perturbations(seq_len, dim, device, dtype):\n     random_perturbations = nn.Parameter(torch.zeros(seq_len, dim, device=device, dtype=dtype))\n     nn.init.normal_(random_perturbations, mean=0.0, std=0.02)\n@@ -182,8 +182,9 @@\n \n         output = self.poseFinal(hidden_states)  # [seqlen, bs, out_feats]\n         output = output.permute(1, 2, 0)  # [bs, e, seqlen]\n         return output\n+    \n class VLATransformerAdaLNBlock(nn.Module):\n     def __init__(self, d_model, nhead, dim_feedforward, dropout, nbp):\n         super().__init__()\n         self.spa_dim = nbp #时间步\n"
                },
                {
                    "date": 1765353665782,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,8 +12,9 @@\n from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D\n from torch.nn import ModuleList\n from .transformer_modules import AdaLNModulation, SelfAttnLayer, SpaTempAttnLayer, FFN, modulate\n import copy\n+from .tools import *\n def learnable_random_perturbations(seq_len, dim, device, dtype):\n     random_perturbations = nn.Parameter(torch.zeros(seq_len, dim, device=device, dtype=dtype))\n     nn.init.normal_(random_perturbations, mean=0.0, std=0.02)\n     return random_perturbations\n@@ -120,9 +121,9 @@\n             x = block(x, h_t = h_t[:,i+1,:], h_a = h_a[:,i+1,:], p=p)  # shape: (batch_size, hidden_dim)\n         x = self.layer_norm2(x)  # shape: (batch_size, hidden_dim)\n         x = self.fc2(x)  # shape: (batch_size, output_dim)\n         return x   \n-from .tools import *\n+\n class InputProcess(nn.Module):\n     def __init__(self, input_feats, latent_dim):\n         super().__init__()\n         self.input_feats = input_feats\n"
                },
                {
                    "date": 1765354245331,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -257,8 +257,9 @@\n                  mask_type = '1D',\n                  code_dim = 512,\n                  latent_dim = 512,\n                  num_decoder_layers = 24,\n+                 num_tokens = 256，\n                  num_heads = 8,\n                  dropout = 0.1,\n                  ):\n         super().__init__()\n@@ -275,8 +276,9 @@\n                                             dim_feedforward=4 * latent_dim,\n                                             dropout=dropout,\n                                             num_layers=num_decoder_layers,\n                                             nbp=self.nbp)\n+        self.output_process = OutputProcess_adaLN(out_feats=num_tokens, latent_dim=latent_dim)\n     \n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n         if force_mask:\n"
                },
                {
                    "date": 1765354264003,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -257,9 +257,9 @@\n                  mask_type = '1D',\n                  code_dim = 512,\n                  latent_dim = 512,\n                  num_decoder_layers = 24,\n-                 num_tokens = 256，\n+                 num_tokens = 256, #dicret num\n                  num_heads = 8,\n                  dropout = 0.1,\n                  ):\n         super().__init__()\n"
                },
                {
                    "date": 1765354269777,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -257,9 +257,9 @@\n                  mask_type = '1D',\n                  code_dim = 512,\n                  latent_dim = 512,\n                  num_decoder_layers = 24,\n-                 num_tokens = 256, #dicret num\n+                 num_tokens = 256, #discret num\n                  num_heads = 8,\n                  dropout = 0.1,\n                  ):\n         super().__init__()\n"
                },
                {
                    "date": 1765354320805,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -287,9 +287,9 @@\n             mask = torch.bernoulli(torch.ones(bs, device=cond.device) * self.cond_drop_prob).view(bs, 1)\n             return cond * (1. - mask)\n         else:\n             return cond\n-    def trans_forward(self, x_ids, cond, force_mask):\n+    def trans_forward(self, x_ids, cond, force_mask=False):\n         cond = self.mask_cond(cond, force_mask=force_mask)\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n         x = self.input_process(x)\n"
                },
                {
                    "date": 1765354341037,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -305,8 +305,9 @@\n         action: (b, w, 7)\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n+        force_mask = False\n         bs, xtokens, ytokens = x.shape\n         ntokens = xtokens * ytokens\n         if self.mask_type == '1D':\n             rand_time = uniform((bs,), device=self.device)#(bs,)\n"
                },
                {
                    "date": 1765354673099,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -370,10 +370,10 @@\n             mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n             # mask_mid = mask\n             x_ids = torch.where(mask_mid, self.mask_id, x_ids)#(b, x*y)\n         logits = self.trans_forward(x_ids, cond, force_mask)\n+        \n \n-\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765370132050,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -297,9 +297,9 @@\n         output = self.Transformer(x, cond)#特定层,每一层都有位置编码\n         logits = self.output_process(output, cond) #final\n         return logits\n \n-    def forward(self, x, cond): \n+    def forward(self, discretized_action, cond): \n         '''\n         x: \n         注意这里的x是原始tokenizer之后的action_id\n         action: (b, w, 7)\n@@ -370,10 +370,10 @@\n             mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n             # mask_mid = mask\n             x_ids = torch.where(mask_mid, self.mask_id, x_ids)#(b, x*y)\n         logits = self.trans_forward(x_ids, cond, force_mask)\n-        \n \n+\n def apply_rope(q, k, cos, sin):\n     \"\"\"\n     RoPE:\n     q, k: (B, H, T, D)   # D must be an even number\n"
                },
                {
                    "date": 1765370139130,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -306,18 +306,18 @@\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n         force_mask = False\n-        bs, xtokens, ytokens = x.shape\n+        bs, xtokens, ytokens = discretized_action.shape\n         ntokens = xtokens * ytokens\n         if self.mask_type == '1D':\n             rand_time = uniform((bs,), device=self.device)#(bs,)\n             rand_mask_probs = self.noise_schedule(rand_time)\n             num_token_masked = (ntokens * rand_mask_probs).round().clamp(min=1)\n             batch_randperm = torch.rand((bs, ntokens), device=self.device).argsort(dim=-1)\n             mask = batch_randperm < num_token_masked.unsqueeze(-1) #(bs, ntokens)\n-            labels = torch.where(mask, x, self.mask_id)\n-            x_ids = x.clone()\n+            labels = torch.where(mask, discretized_action, self.mask_id)\n+            x_ids = discretized_action.clone()\n             mask_rid = get_mask_subset_prob(mask, 0.1)\n             rand_id = torch.randint_like(x_ids, high=self.mask_id)\n             x_ids = torch.where(mask_rid, rand_id, x_ids)#10%替换为随机的值\n             mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n"
                },
                {
                    "date": 1765370145991,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -301,9 +301,9 @@\n     def forward(self, discretized_action, cond): \n         '''\n         x: \n         注意这里的x是原始tokenizer之后的action_id\n-        action: (b, w, 7)\n+        discretized_action: (b, w, 7)\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n         force_mask = False\n"
                },
                {
                    "date": 1765370160797,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -301,9 +301,9 @@\n     def forward(self, discretized_action, cond): \n         '''\n         x: \n         注意这里的x是原始tokenizer之后的action_id\n-        discretized_action: (b, w, 7)\n+        discretized_action: (b, w, 7) all discretized\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n         force_mask = False\n"
                },
                {
                    "date": 1765370166994,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -301,9 +301,9 @@\n     def forward(self, discretized_action, cond): \n         '''\n         x: \n         注意这里的x是原始tokenizer之后的action_id\n-        discretized_action: (b, w, 7) all discretized\n+        discretized_action: (b, w, 7) all by discretized\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n         force_mask = False\n"
                },
                {
                    "date": 1765370371517,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -335,9 +335,9 @@\n             # Positions to be MASKED must also be NON-PADDED\n             # mask = mask & non_pad_mask[..., 0]\n             # Note this is our training target, not input\n             labels = torch.where(mask[..., None].repeat(1, 1, ytokens), x, self.mask_id)\n-            x_ids = x.clone()\n+            x_ids = discretized_action.clone()\n             # Further Apply Bert Masking Scheme\n             # Step 1: 10% replace with an incorrect token\n             mask_rid = get_mask_subset_prob(mask, 0.1)\n             rand_id = torch.randint_like(x_ids, high=self.mask_id)\n"
                },
                {
                    "date": 1765370523907,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -77,9 +77,9 @@\n         action = self.model(\n             rearranged_actions_hidden_states, # (batch, chunk_len, action_dim * hidden_dim) (b, 7, 8*896)\n             h_a=actions_hidden_states, #transformer中和的动作hidden_state #(b, 25, 64, 896)\n             p=proprio_features,  # (bsz, 1, llm_dim)\n-            h_t=task_hidden_states #视觉语言 torch.Size([4, 25, 512, 896])\n+            h_t=task_hidden_states #视觉语言 torch.Size([b, 25, 512, 896])\n             )\n \n         return action\n     \n"
                },
                {
                    "date": 1765370919997,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -277,9 +277,10 @@\n                                             dropout=dropout,\n                                             num_layers=num_decoder_layers,\n                                             nbp=self.nbp)\n         self.output_process = OutputProcess_adaLN(out_feats=num_tokens, latent_dim=latent_dim)\n-    \n+        self.token_emb = nn.Embedding(_num_tokens, self.code_dim)\n+\n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n         if force_mask:\n             return torch.zeros_like(cond)\n"
                },
                {
                    "date": 1765371075054,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -260,8 +260,9 @@\n                  num_decoder_layers = 24,\n                  num_tokens = 256, #discret num\n                  num_heads = 8,\n                  dropout = 0.1,\n+                 bins = 256，\n                  ):\n         super().__init__()\n         self.mask_type = mask_type\n         self.code_dim = code_dim\n"
                },
                {
                    "date": 1765371087105,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -260,9 +260,9 @@\n                  num_decoder_layers = 24,\n                  num_tokens = 256, #discret num\n                  num_heads = 8,\n                  dropout = 0.1,\n-                 bins = 256，\n+                 bins = 256,\n                  ):\n         super().__init__()\n         self.mask_type = mask_type\n         self.code_dim = code_dim\n@@ -278,9 +278,10 @@\n                                             dropout=dropout,\n                                             num_layers=num_decoder_layers,\n                                             nbp=self.nbp)\n         self.output_process = OutputProcess_adaLN(out_feats=num_tokens, latent_dim=latent_dim)\n-        self.token_emb = nn.Embedding(_num_tokens, self.code_dim)\n+        bins\n+        self.token_emb = nn.Embedding(bins, self.code_dim)\n \n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n         if force_mask:\n"
                },
                {
                    "date": 1765371501994,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -278,10 +278,10 @@\n                                             dropout=dropout,\n                                             num_layers=num_decoder_layers,\n                                             nbp=self.nbp)\n         self.output_process = OutputProcess_adaLN(out_feats=num_tokens, latent_dim=latent_dim)\n-        bins\n-        self.token_emb = nn.Embedding(bins, self.code_dim)\n+        _num_tokens = bins\n+        self.token_emb = nn.Embedding(_num_tokens, self.code_dim)\n \n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n         if force_mask:\n"
                },
                {
                    "date": 1765371513530,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -278,9 +278,9 @@\n                                             dropout=dropout,\n                                             num_layers=num_decoder_layers,\n                                             nbp=self.nbp)\n         self.output_process = OutputProcess_adaLN(out_feats=num_tokens, latent_dim=latent_dim)\n-        _num_tokens = bins\n+        _num_tokens = bins + 1 #看一下这里\n         self.token_emb = nn.Embedding(_num_tokens, self.code_dim)\n \n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n"
                },
                {
                    "date": 1765371536453,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -257,9 +257,9 @@\n                  mask_type = '1D',\n                  code_dim = 512,\n                  latent_dim = 512,\n                  num_decoder_layers = 24,\n-                 num_tokens = 256, #discret num\n+                #  num_tokens = 256, #discret num\n                  num_heads = 8,\n                  dropout = 0.1,\n                  bins = 256,\n                  ):\n"
                },
                {
                    "date": 1765371542294,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -277,10 +277,11 @@\n                                             dim_feedforward=4 * latent_dim,\n                                             dropout=dropout,\n                                             num_layers=num_decoder_layers,\n                                             nbp=self.nbp)\n+        _num_tokens = bins + 1 #看一下这里\n         self.output_process = OutputProcess_adaLN(out_feats=num_tokens, latent_dim=latent_dim)\n-        _num_tokens = bins + 1 #看一下这里\n+        \n         self.token_emb = nn.Embedding(_num_tokens, self.code_dim)\n \n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n"
                },
                {
                    "date": 1765371621969,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -278,10 +278,9 @@\n                                             dropout=dropout,\n                                             num_layers=num_decoder_layers,\n                                             nbp=self.nbp)\n         _num_tokens = bins + 1 #看一下这里\n-        self.output_process = OutputProcess_adaLN(out_feats=num_tokens, latent_dim=latent_dim)\n-        \n+        self.output_process = OutputProcess_adaLN(out_feats=_num_tokens, latent_dim=latent_dim)\n         self.token_emb = nn.Embedding(_num_tokens, self.code_dim)\n \n     def mask_cond(self, cond, force_mask=False):#条件掩码\n         bs, d =  cond.shape\n"
                },
                {
                    "date": 1765371829692,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -278,8 +278,9 @@\n                                             dropout=dropout,\n                                             num_layers=num_decoder_layers,\n                                             nbp=self.nbp)\n         _num_tokens = bins + 1 #看一下这里\n+        self.mask_id = bins\n         self.output_process = OutputProcess_adaLN(out_feats=_num_tokens, latent_dim=latent_dim)\n         self.token_emb = nn.Embedding(_num_tokens, self.code_dim)\n \n     def mask_cond(self, cond, force_mask=False):#条件掩码\n"
                },
                {
                    "date": 1765371976840,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -305,9 +305,9 @@\n     def forward(self, discretized_action, cond): \n         '''\n         x: \n         注意这里的x是原始tokenizer之后的action_id\n-        discretized_action: (b, w, 7) all by discretized\n+        discretized_action: (b, w, 7) all by discretized [1, 25]\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n         force_mask = False\n"
                },
                {
                    "date": 1765372110450,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -305,9 +305,9 @@\n     def forward(self, discretized_action, cond): \n         '''\n         x: \n         注意这里的x是原始tokenizer之后的action_id\n-        discretized_action: (b, w, 7) all by discretized [1, 25]\n+        discretized_action: (b, w, 7) all by discretized [1, 256]\n         task_description:(b, 512, 896)\n         action_query: (b, 64, 896)\n         '''\n         force_mask = False\n"
                },
                {
                    "date": 1765373417447,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -255,9 +255,9 @@\n class MaskTransformer(nn.Module):\n     def __init__(self, \n                  mask_type = '1D',\n                  code_dim = 512,\n-                 latent_dim = 512,\n+                 latent_dim = 896,\n                  num_decoder_layers = 24,\n                 #  num_tokens = 256, #discret num\n                  num_heads = 8,\n                  dropout = 0.1,\n"
                },
                {
                    "date": 1765373627344,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -295,9 +295,9 @@\n     def trans_forward(self, x_ids, cond, force_mask=False):\n         cond = self.mask_cond(cond, force_mask=force_mask)\n         x = self.token_emb(x_ids) \n         #这里看一下位置编码的细节\n-        x = self.input_process(x)\n+        x = self.input_process(x) #(8, b, 896)\n         # x = self.position_enc(x)\n         output = self.Transformer(x, cond)#特定层,每一层都有位置编码\n         logits = self.output_process(output, cond) #final\n         return logits\n"
                },
                {
                    "date": 1765373918687,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -301,9 +301,9 @@\n         output = self.Transformer(x, cond)#特定层,每一层都有位置编码\n         logits = self.output_process(output, cond) #final\n         return logits\n \n-    def forward(self, discretized_action, cond): \n+    def forward(self, discretized_action, actions_hidden_states, task_hidden_states): \n         '''\n         x: \n         注意这里的x是原始tokenizer之后的action_id\n         discretized_action: (b, w, 7) all by discretized [1, 256]\n@@ -373,8 +373,9 @@\n             # Step 2: 90% x 10% replace with correct token, and 90% x 88% replace with mask token\n             mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n             # mask_mid = mask\n             x_ids = torch.where(mask_mid, self.mask_id, x_ids)#(b, x*y)\n+        cond = torch.cat()\n         logits = self.trans_forward(x_ids, cond, force_mask)\n \n \n def apply_rope(q, k, cos, sin):\n"
                },
                {
                    "date": 1765373932837,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -373,9 +373,9 @@\n             # Step 2: 90% x 10% replace with correct token, and 90% x 88% replace with mask token\n             mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n             # mask_mid = mask\n             x_ids = torch.where(mask_mid, self.mask_id, x_ids)#(b, x*y)\n-        cond = torch.cat()\n+        cond = torch.cat([actions_hidden_states, task_hidden_states], dim=2)\n         logits = self.trans_forward(x_ids, cond, force_mask)\n \n \n def apply_rope(q, k, cos, sin):\n"
                },
                {
                    "date": 1765373944808,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -373,9 +373,9 @@\n             # Step 2: 90% x 10% replace with correct token, and 90% x 88% replace with mask token\n             mask_mid = get_mask_subset_prob(mask & ~mask_rid, 0.88)\n             # mask_mid = mask\n             x_ids = torch.where(mask_mid, self.mask_id, x_ids)#(b, x*y)\n-        cond = torch.cat([actions_hidden_states, task_hidden_states], dim=2)\n+        cond = torch.cat([actions_hidden_states, task_hidden_states], dim=2) #(b, 25, 64+512, 896)\n         logits = self.trans_forward(x_ids, cond, force_mask)\n \n \n def apply_rope(q, k, cos, sin):\n"
                },
                {
                    "date": 1765374230762,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -298,8 +298,12 @@\n         #这里看一下位置编码的细节\n         x = self.input_process(x) #(8, b, 896)\n         # x = self.position_enc(x)\n         output = self.Transformer(x, cond)#特定层,每一层都有位置编码\n+        #明天把维度都对对齐\n+        #解决离散动作没有0的问题\n+        #解决位置编码的问题\n+        #初步完成训练\n         logits = self.output_process(output, cond) #final\n         return logits\n \n     def forward(self, discretized_action, actions_hidden_states, task_hidden_states): \n"
                }
            ],
            "date": 1765107839596,
            "name": "Commit-0",
            "content": "\"\"\"\naction_heads.py\n\nImplementations of various action heads, which serve as alternatives to VLM sequential token prediction.\n\"\"\"\n\nimport math\nimport torch\nimport torch.nn as nn\nfrom prismatic.vla.constants import ACTION_DIM, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX, NUM_TOKENS\n\n\n\ndef learnable_random_perturbations(seq_len, dim, device, dtype):\n    random_perturbations = nn.Parameter(torch.zeros(seq_len, dim, device=device, dtype=dtype))\n    nn.init.normal_(random_perturbations, mean=0.0, std=0.02)\n    return random_perturbations\n\n\n\nclass L1RegressionActionHead(nn.Module):\n    \"\"\"Simple MLP-based action head that generates continuous actions via L1 regression.\"\"\"\n    def __init__(\n        self,\n        input_dim=4096, #vla.module.llm_dim\n        hidden_dim=4096,\n        action_dim=7,\n        num_task_tokens=512,\n        use_pro_version=False,\n    ):\n        super().__init__()\n        self.num_task_tokens = num_task_tokens\n        self.action_dim = action_dim\n        self.hidden_dim = hidden_dim\n        self.model = MLPResNet(\n            num_blocks=24, \n            input_dim=input_dim*ACTION_DIM, \n            hidden_dim=hidden_dim, \n            output_dim=action_dim,\n            use_pro_version=use_pro_version\n            )\n\n    def predict_action(\n            self, \n            actions_hidden_states, \n            proprio=None, \n            proprio_projector=None,\n            phase=\"Inference\"\n            ):\n        batch_size = actions_hidden_states.shape[0]\n        device = actions_hidden_states.device\n\n        proprio = proprio.reshape(batch_size, -1).to(torch.bfloat16)  # (bsz, proprio_dim)\n        proprio_features = proprio_projector(proprio)  # (bsz, llm_dim)\n        proprio_features = proprio_features.unsqueeze(dim=1)  # (bsz, 1, llm_dim)\n\n        task_hidden_states = actions_hidden_states[:, :, :self.num_task_tokens, :]\n        actions_hidden_states = actions_hidden_states[:, :, self.num_task_tokens:, :]\n\n        cond_actions_hidden_states = torch.zeros(\n            (batch_size, self.action_dim * NUM_ACTIONS_CHUNK, self.hidden_dim),\n            device=device, dtype=actions_hidden_states.dtype\n        ).detach()  \n\n        rearranged_actions_hidden_states = cond_actions_hidden_states.reshape(\n            batch_size, NUM_ACTIONS_CHUNK, -1\n        )  # (batch, chunk_len, action_dim * hidden_dim)\n\n        if phase == \"Training\":\n            batch_size, seq_len, dim = rearranged_actions_hidden_states.shape\n            random_perturbations = learnable_random_perturbations(seq_len, dim, device=rearranged_actions_hidden_states.device, dtype=rearranged_actions_hidden_states.dtype) \n            rearranged_actions_hidden_states = (rearranged_actions_hidden_states + random_perturbations) # (1, seq_len, dim)\n\n        action = self.model(\n            rearranged_actions_hidden_states,\n            h_a=actions_hidden_states,\n            p=proprio_features,\n            h_t=task_hidden_states\n            )\n\n        return action\n    \n\nclass MLPResNet(nn.Module):\n    \"\"\"MLP with residual connection blocks.\"\"\"\n    def __init__(\n            self, \n            num_blocks, \n            input_dim, \n            hidden_dim, \n            output_dim,\n            use_pro_version=False\n            ):\n        \n        super().__init__()\n        self.layer_norm1 = nn.LayerNorm(input_dim)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.mlp_resnet_blocks = nn.ModuleList()\n\n        for _ in range(num_blocks):\n            if use_pro_version:\n                self.mlp_resnet_blocks.append(MLPResNetBlock_Pro(dim=hidden_dim))\n            else:\n                self.mlp_resnet_blocks.append(MLPResNetBlock(dim=hidden_dim))\n                \n        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n\n    def forward(self, x, h_a=None, h_t=None, p= None):\n \n        # x: (batch_size, input_dim)\n        x = self.layer_norm1(x)  # shape: (batch_size, input_dim)\n        x = self.fc1(x)  # shape: (batch_size, hidden_dim)\n        x = self.relu(x)  # shape: (batch_size, hidden_dim)\n        for i, block in enumerate(self.mlp_resnet_blocks):\n            x = block(x, h_t = h_t[:,i+1,:], h_a = h_a[:,i+1,:], p=p)  # shape: (batch_size, hidden_dim)\n        x = self.layer_norm2(x)  # shape: (batch_size, hidden_dim)\n        x = self.fc2(x)  # shape: (batch_size, output_dim)\n        return x   \n\n\n\ndef apply_rope(q, k, cos, sin):\n    \"\"\"\n    RoPE:\n    q, k: (B, H, T, D)   # D must be an even number\n    cos/sin: (T, D)\n    \"\"\"\n    cos = cos.unsqueeze(0).unsqueeze(0)  # (1, 1, T, D)\n    sin = sin.unsqueeze(0).unsqueeze(0)\n\n\n    def rotate_half(x):\n        # Swap even and odd dimensions and flip the signs\n        x1 = x[..., ::2]   # Even subdimension\n        x2 = x[..., 1::2]  # odd subdimension\n\n        return torch.stack((-x2, x1), dim=-1).reshape_as(x)\n\n\n    q_rot = (q * cos) + (rotate_half(q) * sin)\n    k_rot = (k * cos) + (rotate_half(k) * sin)\n\n    return q_rot, k_rot\n\n\n\nclass RotaryPositionEmbedding(nn.Module):\n    def __init__(self, dim, base=10000):\n        \"\"\"\n        dim = head_dim\n        \"\"\"\n        super().__init__()\n        assert dim % 2 == 0, \"RoPE head_dim must be an even number\"\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n    def forward(self, seq_len, device, dtype):\n        t = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype)\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, dim/2)\n        emb = torch.cat([freqs, freqs], dim=-1)            # (T, dim)\n        return emb.cos().to(dtype), emb.sin().to(dtype)\n\n\n\nclass MLPResNetBlock(nn.Module):\n    \"\"\"\n    One residual MLP block with cross-attention conditioning.\n\n    This block applies multi-head attention over:\n      - token features (self-attention),\n      - task-related hidden states (h_t),\n      - action/proprioception-related hidden states (h_a, p).\n    The outputs are combined via a gating mechanism, projected back to the\n    hidden dimension, and passed through a small feedforward sub-network with\n    residual connection.\n\n    Args:\n        dim (int): Dimensionality of the hidden features. Must be divisible by num_heads.\n\n    Inputs:\n        x (torch.Tensor): Input tensor of shape (batch_size, seq_len, hidden_dim).\n        h_t (torch.Tensor, optional): Task-related hidden states of shape\n                                      (batch_size, K, hidden_dim).\n        h_a (torch.Tensor, optional): Action-related hidden states of shape\n                                      (batch_size, 1, hidden_dim).\n        p (torch.Tensor, optional): Additional conditioning features\n                                    (e.g., proprioception), shape (batch_size, 1, hidden_dim).\n\n    Returns:\n        torch.Tensor: Output tensor of shape (batch_size, seq_len, hidden_dim).\n    \"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n        \n        # Main feedforward network\n        self.ffn = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, dim),\n            nn.ReLU(),\n        )\n\n        self.num_heads = 8\n        self.head_dim = dim // self.num_heads\n\n        self.q_proj = nn.Linear(dim, dim)\n        self.k_proj = nn.Linear(dim, dim)\n        self.v_proj = nn.Linear(dim, dim)\n        self.o_proj = nn.Linear(dim, dim)\n\n        self.gating_factor = nn.Parameter(torch.zeros(1))\n\n\n\n    def forward(self, x, h_t=None, h_a=None, p=None):\n        \"\"\"\n        x: (batch_size, seq_len, hidden_dim)\n        h, t, p: (batch_size, 1, hidden_dim) or None\n        \"\"\"\n\n        g = self.gating_factor\n        ratio_g = nn.Tanh()(g)\n\n        conditions = []\n        if h_a is not None:\n            conditions.append(h_a)\n        if p is not None:\n            conditions.append(p)\n\n        h = torch.cat(conditions, dim=1)  # (batch_size, cond_len, hidden_dim)\n\n        B = x.size(0)\n        T = x.size(1)\n        C = x.size(2)\n        K_t = h.size(1)\n        K = h_t.size(1)\n\n        task_k = h\n        task_v = h\n\n        adapter_k = h_t\n        adapter_v = h_t\n\n        q_1 = self.q_proj(x) # (B, T, C)\n        k_tokens = self.k_proj(x)             # (B, T, C)\n        v_tokens = self.v_proj(x)             # (B, T, C)\n        k_task = self.k_proj(task_k)    # (B, K, C)\n        v_task = self.v_proj(task_v)    # (B, K, C)\n\n        k_adapter = self.k_proj(adapter_k)    # (B, K, C)\n        v_adapter = self.v_proj(adapter_v)    # (B, K, C)\n\n        # (B, seq_len, C) -> (B, num_heads, seq_len, head_dim)\n        q_1 = q_1.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        k_tokens = k_tokens.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        v_tokens = v_tokens.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        k_task = k_task.view(B, K_t, self.num_heads, self.head_dim).transpose(1, 2)\n        v_task = v_task.view(B, K_t, self.num_heads, self.head_dim).transpose(1, 2)\n\n        k_adapter = k_adapter.view(B, K, self.num_heads, self.head_dim).transpose(1, 2)\n        v_adapter = v_adapter.view(B, K, self.num_heads, self.head_dim).transpose(1, 2)\n\n        attn_scores_tokens = torch.matmul(q_1, k_tokens.transpose(-2, -1)) # (B, H, T, T)\n        attn_scores_task = torch.matmul(q_1, k_task.transpose(-2, -1)) * 1 # (B, H, T, K)\n        attn_scores_adapter = torch.matmul(q_1, k_adapter.transpose(-2, -1)) * ratio_g # (B, H, T, K)\n\n        attn_scores = torch.cat([attn_scores_tokens, attn_scores_task, attn_scores_adapter], dim=-1) # (B, H, T, T+K)\n        attn_scores = attn_scores / math.sqrt(self.head_dim)\n        attn_weights = torch.softmax(attn_scores, dim=-1) # (B, H, T, T+K)\n\n        v_combined = torch.cat([v_tokens, v_task, v_adapter], dim=2) # (B, H, T+K, head_dim)\n        output = torch.matmul(attn_weights, v_combined) # (B, H, T, head_dim)\n\n        output = output.transpose(1, 2).contiguous().view(B, T, C)\n        output = self.o_proj(output)\n\n        x = self.ffn(output + x) \n\n        return x\n\n\n\nclass MLPResNetBlock_Pro(nn.Module):\n    \"\"\"One MLP ResNet block with separate projections for self, adapter, task + RoPE, now with FiLM modulation.\"\"\"\n\n    def __init__(self, dim, num_heads=8):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n\n        self.ffn = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, dim),\n            nn.ReLU(),\n            )\n\n        # Q (from x only)\n        self.q_proj = nn.Linear(dim, dim)\n\n        # Self-Attention: K, V\n        self.k_self = nn.Linear(dim, dim)\n        self.v_self = nn.Linear(dim, dim)\n\n        # Adapter cross-attention: K, V\n        self.k_adapter = nn.Linear(dim, dim)\n        self.v_adapter = nn.Linear(dim, dim)\n\n        # Task cross-attention: K, V\n        self.k_task = nn.Linear(dim, dim)\n        self.v_task = nn.Linear(dim, dim)\n\n        self.o_proj = nn.Linear(dim, dim)\n\n        # gating\n        self.gating_factor = nn.Parameter(torch.zeros(1))\n\n        # RoPE\n        self.rope = RotaryPositionEmbedding(self.head_dim)\n\n        # ---- FiLM ----\n        # FiLM is useless; to avoid conflict with chkpt, it can be kept as is for now.\n        self.film_gen = nn.Sequential(\n            nn.Linear(dim, dim * 2),  # output γ and β\n            )\n\n\n    def apply_film(self, x, gamma, beta):\n        \"\"\"FiLM: per-channel modulation\"\"\"\n        return gamma.unsqueeze(1) * x + beta.unsqueeze(1)\n\n\n    def forward(self, x, h_a=None, h_t=None, p=None):\n        \"\"\"\n        h_a: adapter tokens\n        h_t: task tokens\n        p:   possible conditioning vector (for FiLM)\n        \"\"\"\n        g = self.gating_factor\n        ratio_g = torch.tanh(g)\n\n        # concat h_a and p\n        h_adapter = torch.cat((h_a, p),dim=1)\n\n        h_task = h_t\n        B, T, C = x.shape\n        K_a = h_adapter.size(1) if h_a is not None else 0\n        K_t = h_task.size(1) if h_task is not None else 0\n\n        # Q\n        q_1 = self.q_proj(x)\n\n        # self tokens\n        k_tokens = self.k_self(x)\n        v_tokens = self.v_self(x)\n\n        # adapter tokens\n        k_adapter = self.k_adapter(h_adapter)\n        v_adapter = self.v_adapter(h_adapter)\n\n        # task tokens\n        k_task = self.k_task(h_task)\n        v_task = self.v_task(h_task)\n\n\n        # reshape -> multi-head\n        def reshape_heads(t, B, L):\n            return t.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n\n\n        q_1 = reshape_heads(q_1, B, T)\n        k_tokens, v_tokens = reshape_heads(k_tokens, B, T), reshape_heads(v_tokens, B, T)\n        k_adapter, v_adapter = reshape_heads(k_adapter, B, K_a), reshape_heads(v_adapter, B, K_a)\n        k_task, v_task = reshape_heads(k_task, B, K_t), reshape_heads(v_task, B, K_t)\n\n        # RoPE\n        cos_main, sin_main = self.rope(seq_len=T, device=x.device, dtype=x.dtype)\n        q_1, k_tokens = apply_rope(q_1, k_tokens, cos_main, sin_main)\n        cos_a, sin_a = self.rope(seq_len=K_a, device=x.device, dtype=x.dtype)\n        _, k_adapter = apply_rope(k_adapter, k_adapter, cos_a, sin_a)     \n        cos_t, sin_t = self.rope(seq_len=K_t, device=x.device, dtype=x.dtype)\n        _, k_task = apply_rope(k_task, k_task, cos_t, sin_t)\n\n        # attention scores\n        attn_scores = [torch.matmul(q_1, k_tokens.transpose(-2, -1))]\n        attn_scores.append(torch.matmul(q_1, k_adapter.transpose(-2, -1)))\n        attn_scores.append(torch.matmul(q_1, k_task.transpose(-2, -1)) * ratio_g)\n        attn_scores = torch.cat(attn_scores, dim=-1) / math.sqrt(self.head_dim)\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n\n        # combine V\n        v_list = [v_tokens,v_adapter,v_task]\n        v_combined = torch.cat(v_list, dim=2)\n\n        output = torch.matmul(attn_weights, v_combined)\n        output = output.transpose(1, 2).contiguous().view(B, T, C)\n        output = self.o_proj(output)\n\n        # # ---- FiLM ---- \n        # gamma_beta = self.film_gen(p)  # [B, 2C]\n        # gamma, beta = gamma_beta.chunk(2, dim=-1)  # [B, C], [B, C]\n        # output = self.apply_film(output, gamma, beta)\n\n        # residual + FFN\n        x = self.ffn(output + x)\n        return x\n"
        }
    ]
}