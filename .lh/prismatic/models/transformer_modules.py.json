{
    "sourceFile": "prismatic/models/transformer_modules.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1765350340777,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1765350340777,
            "name": "Commit-0",
            "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import ModuleList\nimport numpy as np\nimport copy\n\ndef modulate(x, shift, scale):\n    return x * (1 + scale.unsqueeze(0)) + shift.unsqueeze(0)\n    \nclass AdaLNModulation(nn.Module):\n    def __init__(self, d_model, nchunks=6):\n        super(AdaLNModulation, self).__init__()\n        self.nchunks = nchunks\n\n        self.model = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(d_model, nchunks*d_model, bias = True),\n        )\n    \n    def forward(self, cond):\n        return self.model(cond).chunk(self.nchunks, dim=1)\n\nclass SpaTempAttnLayer(nn.Module):\n    def __init__(self, d_model, nhead, dropout,\n                 spa_dim=5):\n        super(SpaTempAttnLayer, self).__init__()\n\n        self.spa_dim = spa_dim\n\n        self.spa_norm = nn.LayerNorm(d_model, elementwise_affine=False, eps=1e-6)\n        self.spatial_attention = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.spa_dropout = nn.Dropout(dropout)\n\n        self.temp_norm = nn.LayerNorm(d_model, elementwise_affine=False, eps=1e-6)\n        self.temporal_attention = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.temp_dropout = nn.Dropout(dropout)\n\n    def forward(self, src, \n                shift_spa, scale_spa, gate_spa, \n                shift_temp, scale_temp, gate_temp, \n                src_key_padding_mask=None, src2=None):#src2是cross-attention\n        \n        # Reshape input for spatial and temporal attention\n        spa_src, temp_src, spa_src2, temp_src2, pad, \\\n            shift_spa, scale_spa, gate_spa, shift_temp, scale_temp, gate_temp = self.create_spa_temp_attn_inputs(\n                                                                                    src, src2, src_key_padding_mask,\n                                                                                    shift_spa, scale_spa, gate_spa,\n                                                                                    shift_temp, scale_temp, gate_temp\n                                                                                    )\n        \n        # Spatial multihead self-attention block\n        spa_src_mod = modulate(self.spa_norm(spa_src), shift_spa, scale_spa) #(5, B*75, 384)\n        if src2 is None:\n            spa_src = spa_src + gate_spa.unsqueeze(0) * self.spa_dropout(self.spatial_attention(spa_src_mod, spa_src_mod, spa_src_mod,\n                                                                                                key_padding_mask=None,\n                                                                                                need_weights=False)[0]) #(5, B*75, 384)\n        else:\n            spa_src = spa_src + gate_spa.unsqueeze(0) * self.spa_dropout(self.spatial_attention(spa_src_mod, spa_src2, spa_src2,\n                                                                                                key_padding_mask=None,\n                                                                                                need_weights=False)[0])\n        \n\n        # Temporal multihead self-attention block\n        temp_src_mod = modulate(self.temp_norm(temp_src), shift_temp, scale_temp)#(75, 5*b, 384)\n        if src2 is None:\n            temp_src = temp_src + gate_temp.unsqueeze(0) * self.temp_dropout(self.temporal_attention(temp_src_mod, temp_src_mod, temp_src_mod,\n                                                                                                    key_padding_mask=pad,\n                                                                                                    need_weights=False)[0])#(75, 5*b, 384)\n        else:\n            temp_src = temp_src + gate_temp.unsqueeze(0) * self.temp_dropout(self.temporal_attention(temp_src_mod, temp_src2, temp_src2,\n                                                                                                    key_padding_mask=pad,\n                                                                                                    need_weights=False)[0])\n\n        spa_src, temp_src = self.reshape_spa_temp_attn_outputs(spa_src, temp_src) \n        spa_temp_src = spa_src + temp_src #时空注意力相加\n\n        return spa_temp_src\n    \n    def create_spa_temp_attn_inputs(self, src, src2, padding_mask, \n                                    shift_spa, scale_spa, gate_spa,\n                                    shift_temp, scale_temp, gate_temp):\n        n_tokens, B, d = src.shape #维度怎么变了\n        self.B, self.d = B, d\n\n        src_wo_text_3d = src.reshape(self.spa_dim, -1, B, d) #spa_dim = 5,这个可能是时间维度的 (5, 75, b, 384)\n        spa_src = src_wo_text_3d.reshape(self.spa_dim, -1, d) #(5, 75*b, 384)\n        temp_src = src_wo_text_3d.permute(1,0,2,3) #(75, 5, b, 384)\n        temp_src = temp_src.reshape(-1, self.spa_dim*B, d) #(75, 5*b, 384)\n        \n        if src2 is not None:\n            src2_wo_text_3d = src2.reshape(self.spa_dim, -1, B, d)\n            spa_src2 = src2_wo_text_3d.reshape(self.spa_dim, -1, d)\n            temp_src2 = src2_wo_text_3d.permute(1,0,2,3)\n            temp_src2 = temp_src2.reshape(-1, self.spa_dim*B, d)\n        else:\n            spa_src2 = None\n            temp_src2 = None\n            \n        shift_spa = shift_spa.repeat(spa_src.shape[1]//B, 1) #(75*b, 384)\n        scale_spa = scale_spa.repeat(spa_src.shape[1]//B, 1)#(75*b, 384)\n        gate_spa = gate_spa.repeat(spa_src.shape[1]//B, 1)#(75*b, 384)\n\n\n        # temp_src = src_wo_text_3d.permute(1,0,2,3)\n        # temp_src = temp_src.reshape(-1, self.spa_dim*B, d)\n        \n        # temp_src = src_wo_text_3d.permute(1,0,2,3)\n        # temp_src = temp_src.reshape(-1, self.spa_dim*B, d)\n        shift_temp = shift_temp.repeat(self.spa_dim, 1)#(5*b, 384)\n        scale_temp = scale_temp.repeat(self.spa_dim, 1)#(5*b, 384)\n        gate_temp = gate_temp.repeat(self.spa_dim, 1)#(5*b, 384)\n\n        # pad = padding_mask.permute(1,0) #(375, 8)\n        # pad = pad.reshape(self.spa_dim, -1, B)\n        # pad = pad.permute(1,0,2) #(75, 5, 8)\n        # pad = pad.reshape(-1, self.spa_dim*B)\n        # pad = pad.permute(1,0) #(5*b,75)\n        pad = None #变长掩码，实际我们处理的是定长的内容\n\n        return spa_src, temp_src, spa_src2, temp_src2, pad, shift_spa, scale_spa, gate_spa, shift_temp, scale_temp, gate_temp\n    \n    def reshape_spa_temp_attn_outputs(self, spa_out, temp_out):\n        temp_out = temp_out.reshape(-1, self.spa_dim, self.B, self.d)\n        temp_out = temp_out.permute(1,0,2,3)\n        temp_out = temp_out.reshape(-1, self.B, self.d)\n\n        spa_out = spa_out.reshape(self.spa_dim, -1, self.B,  self.d)\n        spa_out = spa_out.reshape(-1, self.B, self.d)\n\n        return spa_out, temp_out\n    \nclass LocalInteractionAttnLayer(nn.Module):\n    def __init__(self, d_model, nhead, dropout, spa_dim=5, window_factor=3):\n        super(LocalInteractionAttnLayer, self).__init__()\n\n        self.inter_norm = nn.LayerNorm(d_model, elementwise_affine=False, eps=1e-6)\n        self.inter_norm2 = nn.LayerNorm(d_model, elementwise_affine=False, eps=1e-6)\n        self.interaction_attention = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.inter_dropout = nn.Dropout(dropout)\n\n        self.spa_dim = spa_dim\n        self.window_factor = window_factor\n    \n    def create_local_attn_mask(self, src):\n        timesteps = src.shape[0] // self.spa_dim\n        window = timesteps // self.window_factor\n        \n        ones = torch.ones((timesteps,timesteps), dtype=bool, device=src.device)\n        upper_tri = torch.triu(ones, diagonal = window//2+1)\n        lower_tri = torch.tril(ones, diagonal = -window//2)\n        mask = (upper_tri + lower_tri)\n\n        mask = mask.repeat(self.spa_dim, self.spa_dim)\n\n        return mask\n\n    def forward(self, src, src2, shift, scale, shift2, scale2, gate, src_key_padding_mask=None):\n        # local_attn_mask = self.create_local_attn_mask(src)\n        \n        src_mod = modulate(self.inter_norm(src), shift, scale)\n        src_mod2 = modulate(self.inter_norm2(src2), shift2, scale2)\n        src = src + gate.unsqueeze(0) * self.inter_dropout(self.interaction_attention(src_mod, src_mod2, src_mod2,\n                                                                                attn_mask = None,\n                                                                                key_padding_mask=src_key_padding_mask,\n                                                                                need_weights=False)[0])\n\n        return src\n    \nclass SelfAttnLayer(nn.Module):\n    def __init__(self, d_model, nhead, dropout):\n        super(SelfAttnLayer, self).__init__()\n\n        self.norm = nn.LayerNorm(d_model, elementwise_affine=False, eps=1e-6)\n        self.attention = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src, shift, scale, gate, src_key_padding_mask=None):\n        \n        src_mod = modulate(self.norm(src), shift, scale)\n        src = src + gate.unsqueeze(0) * self.dropout(self.attention(src_mod, src_mod, src_mod,\n                                                            key_padding_mask=src_key_padding_mask,\n                                                            need_weights=False)[0])\n\n        return src\n\nclass FFN(nn.Module):\n    def __init__(self, d_model, dim_feedforward, dropout):\n        super(FFN, self).__init__()\n\n        self.ffn_norm = nn.LayerNorm(d_model, elementwise_affine=False, eps=1e-6)\n        \n        \n        self.model = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim_feedforward, d_model),\n        )\n\n        self.ffn_dropout = nn.Dropout(dropout)\n\n    def forward(self, src, shift, scale, gate):\n        src = src + gate.unsqueeze(0) * self.ffn_dropout(self.model(modulate(self.ffn_norm(src), shift, scale)))\n        return src\n    \nclass InterMTransformerBlock(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward, dropout, nbp):\n        super(InterMTransformerBlock, self).__init__()\n        self.spa_dim = nbp\n        \n        self.adaLN_mod_combined = AdaLNModulation(d_model, 6)\n        self.self_attn = SelfAttnLayer(d_model, nhead, dropout) #self-attention\n        self.ffn_combined = FFN(d_model, dim_feedforward, dropout) #feed-forward\n\n        self.adaLN_mod_split = AdaLNModulation(d_model, 14)\n        self.spa_temp_attn = SpaTempAttnLayer(d_model, nhead, dropout, spa_dim=self.spa_dim) #spatio-temporal-attn\n        # self.spa_temp_attn_cross = SpaTempAttnLayer(d_model, nhead, dropout, spa_dim=self.spa_dim)\n        self.local_inter_attn = LocalInteractionAttnLayer(d_model, nhead, dropout, spa_dim=self.spa_dim) #cross-attn\n        self.ffn_split = FFN(d_model, dim_feedforward, dropout) #feed-forward\n\n        \n        \n        \n\n    def forward(self, src, cond, src_key_padding_mask=None):\n        N, B, d = src.shape\n        \n        # AdaLN modulation\n        shift_self, scale_self, gate_self, \\\n            shift_ffn_c, scale_ffn_c, gate_ffn_c = self.adaLN_mod_combined(cond) #adal-n all (8, 384)\n\n        # Self-Attention\n        src = self.self_attn(src, \n                            shift_self, scale_self, gate_self,\n                            src_key_padding_mask=src_key_padding_mask)\n\n        # FFN\n        src = self.ffn_combined(src, shift_ffn_c, scale_ffn_c, gate_ffn_c)\n\n        ################### Split ###################\n        src1, sep, src2  = src.split([N//2, 1, N//2]) #第一个人、分隔符、第二个人 (375, b, 384)\n        pad,_,_ = src_key_padding_mask.split([N//2, 1, N//2], dim=-1) #pad分割\n\n        # AdaLN modulation\n        # shift_cross_spa, scale_cross_spa, gate_cross_spa, shift_cross_temp, scale_cross_temp, gate_cross_temp, \\\n        shift_spa, scale_spa, gate_spa, shift_temp, scale_temp, gate_temp, \\\n            shift_cross, scale_cross, shift_cross2, scale_cross2, gate_cross, \\\n                shift_ffn_s, scale_ffn_s, gate_ffn_s = self.adaLN_mod_split(cond) #(8, 384)\n\n        # Spatial-Temporal Attention\n        # src1 = self.spa_temp_attn(src1, \n        src1_spa_temp = self.spa_temp_attn(src1, \n                                    shift_spa, scale_spa, gate_spa, \n                                    shift_temp, scale_temp, gate_temp, \n                                    src_key_padding_mask=pad)\n        # src2 = self.spa_temp_attn(src2,\n        src2_spa_temp = self.spa_temp_attn(src2,\n                                    shift_spa, scale_spa, gate_spa, \n                                    shift_temp, scale_temp, gate_temp, \n                                    src_key_padding_mask=pad)\n\n        # Spatial-Temporal Attention Cross\n        # src1_cross = self.spa_temp_attn_cross(src1_spa_temp,\n        #                             shift_cross_spa, scale_cross_spa, gate_cross_spa,\n        #                             shift_cross_temp, scale_cross_temp, gate_cross_temp,\n        #                             src_key_padding_mask=pad, src2=src2)\n        # src2_cross = self.spa_temp_attn_cross(src2_spa_temp,\n        #                             shift_cross_spa, scale_cross_spa, gate_cross_spa,\n        #                             shift_cross_temp, scale_cross_temp, gate_cross_temp,\n        #                             src_key_padding_mask=pad, src2=src1)\n\n        # Local Interaction Attention\n        src1_cross = self.local_inter_attn(src1_spa_temp, src2,\n                                    shift_cross, scale_cross, shift_cross2, scale_cross2, gate_cross,\n                                    src_key_padding_mask=pad)\n        # src2 = self.local_inter_attn(src2, src1,\n        src2_cross = self.local_inter_attn(src2_spa_temp, src1,\n                                    shift_cross, scale_cross, shift_cross2, scale_cross2, gate_cross,\n                                    src_key_padding_mask=pad)\n\n        # FFN\n        src1 = self.ffn_split(src1_cross, shift_ffn_s, scale_ffn_s, gate_ffn_s)\n        src2 = self.ffn_split(src2_cross, shift_ffn_s, scale_ffn_s, gate_ffn_s)\n\n        ################# Concat #################\n        src = torch.cat([src1, sep, src2], dim=0)\n\n        return src\n    \n\n    \nclass InterMTransformer(nn.Module):\n    def __init__(self, num_layers, d_model, nhead, dim_feedforward, dropout, nbp):\n        super(InterMTransformer, self).__init__()\n\n        block = InterMTransformerBlock(d_model=d_model,\n                                nhead=nhead,\n                                dim_feedforward=dim_feedforward,\n                                dropout=dropout,\n                                nbp=nbp)\n             \n        module_list = []\n        \n        for _ in range(num_layers):\n            module_list.append(copy.deepcopy(block))\n\n        self.blocks =  ModuleList(module_list)\n        \n    def forward(self, src, cond, src_key_padding_mask=None):\n        \"\"\"\n        Args:\n            src: Tensor, shape (751, B, d_model)\n            src_key_padding_mask: Tensor, shape (B, 751) \n\n        Additional token is for the condition\n        \"\"\"\n\n        for block in self.blocks:\n            src = block(src, cond, src_key_padding_mask=src_key_padding_mask)\n\n        src = torch.cat([src[:src.shape[0]//2, : ,:],\n                        src[(src.shape[0]//2)+1:, : ,:]])\n                \n        return src\n    \nclass VLATransformerBlock(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward, dropout, nbp):\n        super().__init__()\n        self.spa_dim = nbp #时间步\n        self.adaLN_mod_combined = AdaLNModulation(d_model, 6)\n        self.self_attn = SelfAttnLayer(d_model, nhead, dropout) #self-attention\n        self.ffn_combined = FFN(d_model, dim_feedforward, dropout) #feed-forward\n        self.adaLN_mod_split = AdaLNModulation(d_model, 9)\n        self.spa_temp_attn = SpaTempAttnLayer(d_model, nhead, dropout, spa_dim=self.spa_dim) #spatio-temporal-attn\n        # self.local_inter_attn = LocalInteractionAttnLayer(d_model, nhead, dropout, spa_dim=self.spa_dim) #cross-attn\n        self.ffn_spa = FFN(d_model, dim_feedforward, dropout) #feed-forward\n    \n    def forward(self, src, cond, src_key_padding_mask=None):\n        N, B, d = src.shape\n        # AdaLN modulation\n        shift_self, scale_self, gate_self, \\\n            shift_ffn_c, scale_ffn_c, gate_ffn_c = self.adaLN_mod_combined(cond) #adal-n all (b, latent_dim)\n        \n        # Self-Attention\n        src = self.self_attn(src, \n                            shift_self, scale_self, gate_self,\n                            src_key_padding_mask=src_key_padding_mask)\n        \n        # FFN\n        src = self.ffn_combined(src, shift_ffn_c, scale_ffn_c, gate_ffn_c)\n\n        # AdaLN modulation\n        shift_spa, scale_spa, gate_spa, shift_temp, scale_temp, gate_temp, \\\n                shift_ffn_s, scale_ffn_s, gate_ffn_s = self.adaLN_mod_split(cond) #(b, latent_dim)\n        # shift_cross, scale_cross, shift_cross2, scale_cross2, gate_cross, \\\n        src_spa_temp = self.spa_temp_attn(src, \n                            shift_spa, scale_spa, gate_spa, \n                            shift_temp, scale_temp, gate_temp, \n                            src_key_padding_mask=src_key_padding_mask)\n        src_after = self.ffn_spa(src_spa_temp, shift_ffn_s, scale_ffn_s, gate_ffn_s)\n        return src_after\n\n\nclass VLATransformer(nn.Module):\n    def __init__(self, num_layers, d_model, nhead, dim_feedforward, dropout, nbp):\n        super().__init__()\n        block = VLATransformerBlock(d_model=d_model,\n                                nhead=nhead,\n                                dim_feedforward=dim_feedforward,\n                                dropout=dropout,\n                                nbp=nbp)\n        #方案a,使用adaln把条件信息注入\n        #方案b,使用cross-attention注入条件信息\n        module_list = []\n        \n        for _ in range(num_layers):\n            module_list.append(copy.deepcopy(block))\n\n        self.blocks =  ModuleList(module_list)\n\n    def forward(self, src, cond, src_key_padding_mask=None):\n        \"\"\"\n        Args:\n            src: Tensor, shape (751, B, d_model)\n            src_key_padding_mask: Tensor, shape (B, 751) \n\n        Additional token is for the condition\n        \"\"\"\n\n        for block in self.blocks:\n            src = block(src, cond, src_key_padding_mask=src_key_padding_mask)\n        return src"
        }
    ]
}