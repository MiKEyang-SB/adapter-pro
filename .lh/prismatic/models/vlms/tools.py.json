{
    "sourceFile": "prismatic/models/vlms/tools.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1765287092895,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1765287092895,
            "name": "Commit-0",
            "content": "import torch\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange\n\n# return mask where padding is FALSE\ndef lengths_to_mask(lengths, max_len):\n    # max_len = max(lengths)\n    mask = torch.arange(max_len, device=lengths.device).repeat(len(lengths), 2) < lengths.unsqueeze(1)\n    # mask = torch.arange(max_len, device=lengths.device).expand(len(lengths), max_len) < lengths.unsqueeze(1)\n    return mask #(b, len)\n\n# return mask where padding is ALL FALSE\ndef get_pad_mask_idx(seq, pad_idx):\n    return (seq != pad_idx).unsqueeze(1)\n\n# Given seq: (b, s)\n# Return mat: (1, s, s)\n# Example Output:\n#        [[[ True, False, False],\n#          [ True,  True, False],\n#          [ True,  True,  True]]]\n# For causal attention\ndef get_subsequent_mask(seq):\n    sz_b, seq_len = seq.shape\n    subsequent_mask = (1 - torch.triu(\n        torch.ones((1, seq_len, seq_len)), diagonal=1)).bool()\n    return subsequent_mask.to(seq.device)\n\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n\ndef l2norm(t):\n    return F.normalize(t, dim = -1)\n\n# tensor helpers\n\n# Get a random subset of TRUE mask, with prob\ndef get_mask_subset_prob(mask, prob):\n    subset_mask = torch.bernoulli(mask, p=prob) & mask\n    return subset_mask\n\n\n# Get mask of special_tokens in ids\ndef get_mask_special_tokens(ids, special_ids):\n    mask = torch.zeros_like(ids).bool()\n    for special_id in special_ids:\n        mask |= (ids==special_id)\n    return mask\n\n# network builder helpers\ndef _get_activation_fn(activation):\n    if activation == \"relu\":\n        return F.relu\n    elif activation == \"gelu\":\n        return F.gelu\n\n    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n\n# classifier free guidance functions\n\ndef uniform(shape, device=None, min=0, max=1):\n    return torch.zeros(shape, device=device).float().uniform_(min, max)\n\ndef prob_mask_like(shape, prob, device=None):\n    if prob == 1:\n        return torch.ones(shape, device=device, dtype=torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device=device, dtype=torch.bool)\n    else:\n        return uniform(shape, device=device) < prob\n\n# sampling helpers\n\ndef log(t, eps = 1e-20):\n    return torch.log(t.clamp(min = eps))\n\ndef gumbel_noise(t):\n    noise = torch.zeros_like(t).uniform_(0, 1)\n    return -log(-log(noise))\n\ndef gumbel_sample(t, temperature = 1., dim = 1):\n    return ((t / max(temperature, 1e-10)) + gumbel_noise(t)).argmax(dim=dim)\n\n\n# Example input:\n#        [[ 0.3596,  0.0862,  0.9771, -1.0000, -1.0000, -1.0000],\n#         [ 0.4141,  0.1781,  0.6628,  0.5721, -1.0000, -1.0000],\n#         [ 0.9428,  0.3586,  0.1659,  0.8172,  0.9273, -1.0000]]\n# Example output:\n#        [[  -inf,   -inf, 0.9771,   -inf,   -inf,   -inf],\n#         [  -inf,   -inf, 0.6628,   -inf,   -inf,   -inf],\n#         [0.9428,   -inf,   -inf,   -inf,   -inf,   -inf]]\ndef top_k(logits, thres = 0.9, dim = 1):\n    k = math.ceil((1 - thres) * logits.shape[dim]) #保留前0.1*257\n    val, ind = logits.topk(k, dim = dim)\n    probs = torch.full_like(logits, float('-inf'))\n    probs.scatter_(dim, ind, val)\n    # func verified\n    # print(probs)\n    # print(logits)\n    # raise\n    return probs\n\n# noise schedules\n\n# More on large value, less on small\ndef cosine_schedule(t):\n    return torch.cos(t * math.pi * 0.5)\ndef cosine_schedule_backward(x):\n    return torch.acos(x) / (math.pi * 0.5)\n\ndef scale_cosine_schedule(t, scale):\n    return torch.clip(scale*torch.cos(t * math.pi * 0.5) + 1 - scale, min=0., max=1.)\n\n# More on small value, less on large\ndef q_schedule(bs, low, high, device):\n    noise = uniform((bs,), device=device)\n    schedule = 1 - cosine_schedule(noise)\n    return torch.round(schedule * (high - low - 1)).long() + low\n\ndef cal_performance(pred, labels, ignore_index=None, smoothing=0., tk=1):\n    loss = cal_loss(pred, labels, ignore_index, smoothing=smoothing)\n    # pred_id = torch.argmax(pred, dim=1)\n    # mask = labels.ne(ignore_index)\n    # n_correct = pred_id.eq(labels).masked_select(mask)\n    # acc = torch.mean(n_correct.float()).item()\n    pred_id_k = torch.topk(pred, k=tk, dim=1).indices #(b, 1, 750)\n    pred_id = pred_id_k[:, 0] #(b, 750)  top1预测值的索引\n    mask = labels.ne(ignore_index) #1024是mask\n    n_correct = (pred_id_k == labels.unsqueeze(1)).any(dim=1).masked_select(mask)\n    acc = torch.mean(n_correct.float()).item() #准确率\n\n    return loss, pred_id, acc\n\n\ndef cal_loss(pred, labels, ignore_index=None, smoothing=0.):\n    '''Calculate cross entropy loss, apply label smoothing if needed.'''\n    # print(pred.shape, labels.shape) #torch.Size([64, 1028, 55]) torch.Size([64, 55])\n    # print(pred.shape, labels.shape) #torch.Size([64, 1027, 55]) torch.Size([64, 55])\n    if smoothing:\n        space = 2\n        n_class = pred.size(1)\n        mask = labels.ne(ignore_index)\n        one_hot = rearrange(F.one_hot(labels, n_class + space), 'a ... b -> a b ...')[:, :n_class]\n        # one_hot = torch.zeros_like(pred).scatter(1, labels.unsqueeze(1), 1)\n        sm_one_hot = one_hot * (1 - smoothing) + (1 - one_hot) * smoothing / (n_class - 1)\n        neg_log_prb = -F.log_softmax(pred, dim=1)\n        loss = (sm_one_hot * neg_log_prb).sum(dim=1)\n        # loss = F.cross_entropy(pred, sm_one_hot, reduction='none')\n        loss = torch.mean(loss.masked_select(mask))\n    else:\n        loss = F.cross_entropy(pred, labels, ignore_index=ignore_index)\n\n    return loss"
        }
    ]
}